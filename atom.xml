<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>吾辈之人，自当自强不息！</title>
  
  <subtitle>博客</subtitle>
  <link href="https://merlynr.github.io/atom.xml" rel="self"/>
  
  <link href="https://merlynr.github.io/"/>
  <updated>2021-07-13T06:46:43.362Z</updated>
  <id>https://merlynr.github.io/</id>
  
  <author>
    <name>Merlynr</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="https://merlynr.github.io/2021/07/13/attachments/1626158803254.table/"/>
    <id>https://merlynr.github.io/2021/07/13/attachments/1626158803254.table/</id>
    <published>2021-07-13T06:46:43.362Z</published>
    <updated>2021-07-13T06:46:43.362Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>强化学习</title>
    <link href="https://merlynr.github.io/2021/07/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <id>https://merlynr.github.io/2021/07/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</id>
    <published>2021-07-12T16:00:00.000Z</published>
    <updated>2021-07-12T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>监督学习在机器学习中取得了重大的成功，然而在<strong>顺序决策制定</strong>和<strong>控制问题</strong>中，比如无人直升机、无人汽车等，难以给出显式的监督信息，因此这类问题中<strong>监督模型无法学习</strong>。<br>强化学习就是为了解决这类问题而产生的。在强化学习框架中，学习算法被称为一个agent，假设这个agent处于一个环境中，两者之间存在交互。<font color="#9400D3">agent通过与环境交互不断增强对环境的适应力，故得名强化学习。</font></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021713/1626145210759.png" alt="强化学习"></p><p>在每个时间步 $t$ ，agent：</p><ul><li>接受状态 $s _ { t }$</li><li>接受标量回报 $r _ { t }$</li><li>执行行动 $a _ { t }$</li></ul><p>环境：</p><ul><li>接受动作 $a _ { t }$</li><li>产生状态 $s _ { t }$</li><li>产生标量回报 $r _ { t }$</li></ul><h2 id="MDP-马尔科夫决策过程"><a href="#MDP-马尔科夫决策过程" class="headerlink" title="MDP(马尔科夫决策过程)"></a>MDP(马尔科夫决策过程)</h2><p>通常我们都是从MDP（马尔科夫决策过程）来了解强化学习的。MDP问题中，我们有一个五元组： $( S , A , P , \gamma , P )$</p><ul><li>$S$ :状态集，由agent所有可能的状态组成</li><li>$A$ :动作集，由agent所有可能的行动构成</li><li>$P ( s , a , s ^ { \prime } )$ :转移概率分布，表示状态s下执行动作a后下个时刻状态的概率分布</li><li>$\gamma$ :折扣因子，0≤ $\gamma$ ≤1，表示未来回报相对于当前回报的重要程度。如果 $\gamma$ =0，表示只重视当前立即回报； $\gamma$ =1表示将未来回报视为与当前回报同等重要。【<font color="#FF8C00">这块不懂，可以看后面下围棋的栗子</font>】</li><li>$R ( s , a , s ^ { \prime } )$ :标量立即回报函数。执行动作a，导致状态s转移到s′产生的回报。可以是关于状态-动作的函数 $S \times A \rightarrow R$ ，也可以是只关于状态的函数 $S \rightarrow R$ 。记t时刻的回报为 $r _ { t }$ ，为了后续表述方便，假设我们感兴趣的问题中回报函数只取决于状态，而状态-动作函数可以很容易地推广，这里暂不涉及。</li></ul><p><strong><font color="#8B008B">注：</font></strong> 这里阐述的MDP称为discounted MDP，即<font color="#00008B">带折扣因子的MDP</font>。有些MDP也可以定义为四元组： $( S , A , P , R )$ ，这是<em>因为这类MDP中使用的值函数不考虑折扣因子</em>。</p><blockquote><p>**<font color="#9932CC">马尔可夫性质</font>*<em>：当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布<font color="#006400">仅依赖于当前状态</font>；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是<font color="#1E90FF">条件独立</font>的，那么此随机过程即具有马尔可夫性质。<br>例如：</em>明天的天气（是否下大雨）仅与今天的天气（是否刮大风）有关，而与前天及以前的天气无关*。</p></blockquote><p>MDP过程具有马尔科夫性质，即给定当前状态，未来的状态与过去的状态无关。但与马尔科夫链不同的是，MDP还考虑了<strong>动作</strong>，也就是说MDP中状态的转移不仅和状态有关，还依赖于agent采取的动作。</p><p>我们可以通过下面表格了解各种马尔科夫模型的区别：</p><table><thead><tr><th></th><th>不考虑动作</th><th>考虑动作</th></tr></thead><tbody><tr><td>状态可观测</td><td>马尔科夫链（MC）</td><td>马尔科夫决策过程（MDP）</td></tr><tr><td>状态不完全可观测</td><td>隐马尔科夫模型（HMM）</td><td>不完全可观察马尔可夫决策过程（POMDP）</td></tr></tbody></table><p><strong><font color="#FF8C00">MDP的运行过程：</font></strong><br><img src="https://gitee.com/merlynr/img-store/raw/master/2021713/1626158954807.png" alt="MDP"></p><p>我们从初始状态 $s _ { 0 }$ 出发，执行某个动作 $a _ { 0 }$ ，根据转移概率分布确定下一个状态 $s _ { 1 }$ ∼ $P _ { s0a0 }$ ，接着执行动作 $a _ { 1 }$ ，再根据 $P _ { s1a1 }$ 确定 $s _ { 2 }$ …。</p><p>一个discounted MDP中，我们的目标最大化一个累积未来折扣回报:<br>$R _ { t } = \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } r _ { t + k + 1 }$</p>]]></content>
    
    
    <summary type="html">【可選】文章描述</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="machine leaning" scheme="https://merlynr.github.io/tags/machine-leaning/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>暑假学习安排以及前段学习总结</title>
    <link href="https://merlynr.github.io/2021/07/11/%E6%9A%91%E5%81%87%E5%AD%A6%E4%B9%A0%E5%AE%89%E6%8E%92%E4%BB%A5%E5%8F%8A%E5%89%8D%E6%AE%B5%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <id>https://merlynr.github.io/2021/07/11/%E6%9A%91%E5%81%87%E5%AD%A6%E4%B9%A0%E5%AE%89%E6%8E%92%E4%BB%A5%E5%8F%8A%E5%89%8D%E6%AE%B5%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</id>
    <published>2021-07-10T16:00:00.000Z</published>
    <updated>2021-07-10T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="已完成–前段时间"><a href="#已完成–前段时间" class="headerlink" title="已完成–前段时间"></a>已完成–前段时间</h2><ol><li>观看学习了吴恩达的机器学习课程视频，观看的主要的目的是对于机器学习有个整体的观念，同时我也学到了机器学习实验中简单的调参，验证以及最后结果的分析。</li><li>总结性的略读了几篇业务流程中对于异常的检测的paper，主要还是通过根据时间特征来检测事件的状态，主要包括有：<ul><li>基于时间边界的事件异常检测【通过对事件划分时间边界】</li><li>基于关键路径【确认业务流程关键路径，比对花费所时间】</li><li>通过对于执行时间建模方法来预测【即构建所有活动持续时间直立方图，然后通过比对当前节点和余下节点在图中占比来预测】</li><li>综合时间模型和流程步骤分析【综合运用时间统计模型和通过多个步骤分析方法生成运行时间概率分布、计算异常概率、与阈值比较的方法，提出一种基于运行的异常预测算法来预测工作流中的时间异常，该算法分为即设计时段和运行时段两个阶段，在设计时段，生成该模型所有可能产生的运行轨迹，并计算它们的预计执行时间的概率分布；在运行时段，通过分析计算流程超时的可能性与预设的阈值做比较来判断是否预测为异常】</li></ul></li><li>精读了2019年较为新的paper【基于机器学习的流程异常预测方法】，文中作者提出一种混合模型，就是结合了通过标注时间时间来对超期异常预测，但是由于系统执行时间很容易受到环境等因素导致超过人工标注时间，所以作者提出了通过机器学习学习各个事件在整个流程的流程所消费的时间占比来预测异常。自己对于其中的算法和模型都进行详细了解，同时结合以前略读的论文，感觉这篇论文就是结合了一下前人的方法，没有实际创新点。</li><li>整理了关于业务流程预测中的详细研究方向：<pre><code>- 预测完成一个案件的剩余时间- 流程预测结果评估【二元评估】- 预测下一个事件</code></pre></li><li>对于业务流程预测下一事件进行系统学习【只完成了部分,这一部分吗主要是2016年以前的论文，即普遍是用机器模型，而更复杂的深度学习模型是2016年后被应用到】：<ul><li>利用数据的时间分类特征来预测，使用更高阶马尔可夫模型（HMMs，这里指的是隐马尔可夫模型）和使用序列比对的流程结果来预测过程的下一个步骤。个人理解就是将序列KNN与HMMs相结合，通过HMMs学习数据，获得高阶模型，然后混合了序列KNN的替换矩阵，利用局部序列对比，<strong>降低了高阶HMMs的弱覆盖的缺陷影响</strong>。</li><li>针对处理半结构化业务流程数据，提出一种针对特定事件进程的模型（PPM），个人学习理解它作为过渡模型，主要负责学习记录半结构化业务流程的实例，和替换矩阵，然后映射到扩展马尔可夫链预测【当遇到并行事务的时候通过递归的方式学习转换矩阵】，作者的优点主要体现在针对<strong>半结构化业务流程数据</strong>进行预测。 </li></ul></li></ol><h2 id="未完成及原因–前段时间"><a href="#未完成及原因–前段时间" class="headerlink" title="未完成及原因–前段时间"></a>未完成及原因–前段时间</h2><ol><li>机器学习还是需要实操来检验自己所理解的知识，但是由于前期花费在理论上的时间比较多，同花了很多时间在数学上，虽然准备了环境准备实操但是也没有按时完成，也搁浅了。</li><li>在系统了解学习对于业务异常检测的时候中，还有两篇关于结合积极语义模型和受病毒传播启发病毒传染模型启发的时间延迟传播模型的预测检测的方法没有阅读，主要由于后面确认了自己要研究预测下一步，所以就没有继续阅读了。</li><li>在预测行为异常的论文中多次提到了对数据进行初始标注，但是都没有详细说明，所以这块没有详细研究，后面也没继续关注</li><li>本来应该在七月底将2016年前关于业务流程预测下一步的几篇全部看完，但是由于中间整理实验数据跑一下基本的模型【结果也没跑，数据量过大，笔记本不行，在挖掘流程的时候就很卡】，还有遇到了考试月就耽误了</li></ol><h2 id="暑假安排"><a href="#暑假安排" class="headerlink" title="暑假安排"></a>暑假安排</h2><ol><li>7.25号之前将剩下的四五篇关键论文看完【2016年前】</li><li>8.20系统学习看一下深度学习中在流程预测的使用，目前查阅的主要有循环神经网络，MMs，语法感知技术，基于过程发现技术，基于自动机的预测技术等，我现在收集了相关14篇论文【有一些是重复的改进模型】，开始可能看的慢一点，后面应该会快一点</li><li>读深度学习相关论文的时候，开始我会阅读最早的三四篇，然后后面倒着读剩下的，这样一旦有idea也能知道自己的想法是不是与最新的重复了。</li><li>9.17-9.27回家过个中秋，同时发小结婚</li></ol><h2 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h2><ol><li>老师前面提到与公司项目相结合，使用公司的数据，我当时考虑的是使用公工数据集，这样有对比也易于发表，现在我能想到的是，本来后面要帮老师做那个RPA程序，就想的后面在大论文中加入一部分流程挖掘，正好可以利用公司的日志信息挖掘，然后根据数据训练模型【这一块明天有空和老师聊一下哇，因为老师让我做的那块貌似没有日志哇，所以交流一下哇】</li><li>在我阅读业务流程预测论文的过程中，我开始发现预测事件由传统机器学习中的序列匹配开始向深度学习中的NLP靠拢，通过学习事件之间的因果来预测下一步，可能这块研究没有NLP那么超前，所以基本模型没有配套，但是依旧可以向他们学习，这样更加容易改进模型发表paper</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;已完成–前段时间&quot;&gt;&lt;a href=&quot;#已完成–前段时间&quot; class=&quot;headerlink&quot; title=&quot;已完成–前段时间&quot;&gt;&lt;/a&gt;已完成–前段时间&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;观看学习了吴恩达的机器学习课程视频，观看的主要的目的是对于机器学习有个整体的观念</summary>
      
    
    
    
    <category term="plane" scheme="https://merlynr.github.io/categories/plane/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="summarize" scheme="https://merlynr.github.io/tags/summarize/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>预测精度分析</title>
    <link href="https://merlynr.github.io/2021/07/08/%E9%A2%84%E6%B5%8B%E7%B2%BE%E5%BA%A6%E5%88%86%E6%9E%90/"/>
    <id>https://merlynr.github.io/2021/07/08/%E9%A2%84%E6%B5%8B%E7%B2%BE%E5%BA%A6%E5%88%86%E6%9E%90/</id>
    <published>2021-07-07T16:00:00.000Z</published>
    <updated>2021-07-07T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/qq_41196612/article/details/107265167">R语言实战——基于KNN聚类的时间序列分析预测_三只佩奇不结义的博客-CSDN博客_r语言knn回归及预测</a></p>]]></content>
    
    
    <summary type="html">【可選】文章描述</summary>
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>瘾</title>
    <link href="https://merlynr.github.io/2021/07/07/%E7%98%BE/"/>
    <id>https://merlynr.github.io/2021/07/07/%E7%98%BE/</id>
    <published>2021-07-06T16:00:00.000Z</published>
    <updated>2021-07-06T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul><li><input disabled="" type="checkbox"> 酒</li><li><input disabled="" type="checkbox"> 手机</li><li><input disabled="" type="checkbox"> 游戏</li><li><input disabled="" type="checkbox"> 小视频</li></ul><h2 id="HOW"><a href="#HOW" class="headerlink" title="HOW"></a>HOW</h2><ol><li>酒每个月少喝就行了</li><li>分析上瘾手机的原因，太无聊了，找到替代品</li><li>游戏本质就是单身狗</li><li>小视频太闲了，需要找到替代品</li></ol><ul><li><input disabled="" type="checkbox"> 手机不上床</li><li><input disabled="" type="checkbox"> 游戏emmmm</li><li><input disabled="" type="checkbox"> 小视频换个环境，聊聊天什么的就行了</li><li><input disabled="" type="checkbox"> 找个积极的上瘾</li></ul>]]></content>
    
    
    <summary type="html">每天/周学习计划</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://merlynr.github.io/2021/07/06/attachments/1625557760628.table/"/>
    <id>https://merlynr.github.io/2021/07/06/attachments/1625557760628.table/</id>
    <published>2021-07-06T07:49:20.731Z</published>
    <updated>2021-07-06T07:51:21.015Z</updated>
    
    <content type="html"><![CDATA[<table class='table table-celled table-component' border='1' style='border-collapse: collapse; width: 100%;' id='tinymcetable' > <tbody><tr><td>&nbsp;</td><td style="background-color: #d9caca;">&nbsp;晴</td><td style="background-color: #d9caca;">&nbsp;阴</td></tr><tr><td style="background-color: #d1c9c9;">&nbsp;晴</td><td>&nbsp;0.9</td><td>0.1&nbsp;</td></tr><tr><td style="background-color: #e3dcdc;">&nbsp;阴</td><td>&nbsp;0.5</td><td>0.5&nbsp;</td></tr></tbody> </table>]]></content>
    
    
      
      
    <summary type="html">&lt;table class=&#39;table table-celled table-component&#39; border=&#39;1&#39; style=&#39;border-collapse: collapse; width: 100%;&#39; id=&#39;tinymcetable&#39; &gt; &lt;tbody&gt;
&lt;tr</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>用于业务流程事件和结果预测的混合模型</title>
    <link href="https://merlynr.github.io/2021/07/05/%E7%94%A8%E4%BA%8E%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E4%BA%8B%E4%BB%B6%E5%92%8C%E7%BB%93%E6%9E%9C%E9%A2%84%E6%B5%8B%E7%9A%84%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/"/>
    <id>https://merlynr.github.io/2021/07/05/%E7%94%A8%E4%BA%8E%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E4%BA%8B%E4%BB%B6%E5%92%8C%E7%BB%93%E6%9E%9C%E9%A2%84%E6%B5%8B%E7%9A%84%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-07-04T16:00:00.000Z</published>
    <updated>2021-07-04T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>对于多样性流程进行异常预测</p><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><ol><li>序列k近邻法（KNN）</li><li>基于序列比对的马尔科夫模型扩展法</li></ol><p><font color="#1E90FF">思路：</font><br>利用数据的时间分类特征，利用高阶马尔可夫模型预测过程的下一步，并利用序列对比技术预测过程的结果。通过考虑基于k个最近邻的相似过程序列的子集，增加了数据的多样性方面。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>已经证明，通过一组实验，序列k最近邻提法比原始提供更好的结果;我们的扩展马尔可夫模型优于随机猜测、马尔可夫模型和隐马尔可夫模型。</p><blockquote><p><font color="#2e4e7e">知识补充</font><br><a href="https://blog.zuishuailcq.xyz/2021/07/05/KNN%EF%BC%88K%E8%BF%91%E9%82%BB%E6%B3%95%20K%20Nearest%20Neighbors%EF%BC%89/">KNN（K近邻法 K Nearest Neighbors） | 吾辈之人，自当自强不息！</a><br><a href="https://blog.zuishuailcq.xyz/2021/07/05/Markov%20Model%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%89/">Markov Model（马尔可夫模型） | 吾辈之人，自当自强不息！</a></p></blockquote><h2 id="阐述"><a href="#阐述" class="headerlink" title="阐述"></a>阐述</h2><p>在进行流程预测的前，我们需要从日志中挖掘流程。通过分析数据，可以得知数据为带有<strong>时间序列</strong>的数据。1999年已经有人证明MMs适用于研究用户网上浏览行为。同时事件序列也可用于训练已经<strong>编码后续事件之间的转换概率</strong>的马尔可夫模型，<br>类似其它机器学习模型，越是高阶的模型越是拟合数据，预测结果也更加准确。</p><blockquote><p><font color="#FF1493">知识补充</font><br>在马尔可夫链的每一步，系统根据概率分布，可以从一个状态变到另一个状态,也可以保持当前状态。状态的改变叫做过渡，与不同的状态改变相关的概率叫做<strong>过渡概率</strong><br><font color="#006400">TODO </font>默认预测的提出没有仔细研究<br>当给定数据集很少时，导致无法机器学习和准确预测，所以为了解决这个问题，提出了几个为了解决特征和默认事件之间的非线性依赖关系的模型，通过补充特征默认值来补充数据，进行预测。<strong>默认预测</strong></p></blockquote><p>但是当数据多样化同时不聚集的时候，会导致高阶模型弱覆盖，对于未被覆盖的序列，就需要默认预测。但是默认预测会降低模型的准确度。<br><em>在马尔可夫模型中</em>，为了平衡覆盖与准确性，一般解决思想是合并多个不同阶的MMs的转换状态，然后在预测的时候遵循“冗余”状态。例如可选择马尔可夫模型（selective Markov model）</p><blockquote><p>TODO<br>遵循“冗余”状态的实际意义是什么？它的实际表现是什么<br>an extension of All Kth order Markov models (Deshpande &amp; Karypis, 2004)</p></blockquote><blockquote><p><font color="#2F4F4F">知识补充</font><br>个人理解<strong>弱覆盖</strong>是由于特征广而弱，无法高效学习，导致有一些特征被丢弃没有学习到。</p></blockquote><p>第二部部分讲述的是采用kNN算法来预测过程结果，即通过比对给定领域内的序列，找到最相似的序列。06年有研究者发表“预测使用电信公司的数据流失”，用欧几里德距离来计算给定序列与样本之间的距离。</p><blockquote><p>TODO 生物顺序结合没懂</p></blockquote><p>在本文中作者将KNN与生物学的序列对齐组合形成顺序kNN。</p><blockquote><p><font color="#bf242a">知识补充</font><br><strong>编辑距离</strong>是针对二个字符串（例如英文字）的差异程度的量化量测，量测方式是看至少需要多少次的处理才能将一个字符串变成另一个字符串。</p></blockquote><h3 id="creation"><a href="#creation" class="headerlink" title="creation"></a>creation</h3><blockquote><p>TODO 不理解下面的匹配机制</p></blockquote><p>首先是为了解决高阶MMs弱覆盖，提出了MMs和序列对准融合的技术。当预测模型无法找到预测实例对应的序列时，<strong>应用匹配过程</strong>以便从与给定序列中最相似的转换矩阵中提取那些序列（图案）。</p><p>其次是提出一种预测结果的序列kNN方法，即通过比对序列局部结果，然后比对相似程度，获得预测结果</p><h2 id="序列比对"><a href="#序列比对" class="headerlink" title="序列比对"></a>序列比对</h2><p><font color="#FF1493">为了确定序列相似性</font><br>序列比对主要类似于生物学中的新的DNA序列与DNA数据库进行比较。将DNA序列的事务与蛋白质数据库进行比较，以验证它们之间的关系是否可能发生在发生概率之间。</p><blockquote><p>序列比对<br>包括全局比对和局部比对<br>全局比对是提供了全局优化解决方案，遍历所有查询序列的整个长度。<br>局部比对旨在从两个查询序列中找到最相似的段</p></blockquote><p>我们在MMs中并不是使用当前的序列比对算法，而是使用序列比对的思想；同时作者提出<strong>局部序列比对与kNNs的结合</strong>使用方法：【替换矩阵】</p><blockquote><p><font color="#006400">知识补充</font><br>在序列比对算法中的<strong>替换矩阵</strong>又称为打分矩阵，其数学本质是统计权重。<br><a href="https://zhuanlan.zhihu.com/p/150582377">替换矩阵（计分矩阵）| 原理和作用 - 知乎</a><br>在序列比对中，我们一般需要给出一个定量的数值来描述两者的一致性和相似性。在此过程中，替换矩阵用来评价碱基或残基之间的相似性，在长期实践中，人们发现一些特定的碱基替换或者残基替换的频率是要高于另一些替换的，因此人们可以通过统计方法或者基于进化的突变模型来给每一种替换定义不同的分值，来体现出不同碱基或残基之间发生替换的可能性。其可以分成核酸序列替换矩阵和蛋白质序列替换矩阵。</p></blockquote><p> <strong><font color="#9932CC">替换矩阵：</font></strong><br> <em>生物学中用于描述单位时间内，一个氨基酸转换为另一个氨基酸的速率</em>。本文中替换矩阵作为单位矩阵，主对角线的元素是1，其他元素都是0。为了呈现突变，使用了更复杂形式的替代矩阵。上边<font color="#1E90FF">知识补充</font>中提到不同的矩阵不同值可以更好表现序列单元之间转换的可能性与频率。<font color="#FF00FF">我的理解是序列对比的权重表</font>。</p><p> <strong><font color="#9932CC">打分矩阵</font></strong><br> 即模型打分矩阵函数：</p><p> <img src="https://gitee.com/merlynr/img-store/raw/master/2021710/1625901529848.png" alt="初始化"></p><p> <img src="https://gitee.com/merlynr/img-store/raw/master/2021710/1625901755301.png" alt="计算公式"><br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>j</mi></msub><mo>)</mo></math>是替换矩阵中Xi与Yj的替换分数。<br><font color="#483D8B">TODO</font> 公式中有个别参数读不懂</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021710/1625902300497.png" alt="栗子"></p><p><strong><font color="#FF8C00">案例：</font></strong><br>序列ABCDE与序列EBCAD对应的打分矩阵，根据分数高低来寻找最佳片段，然后沿着对角线从最高点到左上角，直至分数为0，来确定匹配序列，如图栗子中为BC。</p><h2 id="预测模型"><a href="#预测模型" class="headerlink" title="预测模型"></a>预测模型</h2><p><strong><font color="#9932CC">前提：</font></strong><br>本模型目的是在流程实例中预测<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>s</mi><mrow><mi>i</mi></mrow><mrow><mo>(</mo><mi>N</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msubsup></math>下一事件<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>(</mo><mi>N</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msubsup></math>。其中<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>S</mi><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow></msub></math>业务流程中的事件的预测是基于<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>S</mi><mo>=</mo><mo>{</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo>&#x22EF;</mo><msub><mi>S</mi><mi>N</mi></msub><mo>}</mo></math></p><p>其中一个业务流程实例（Sj）是按时间顺序排列的离散事件(或任务)的组合Sj=<img src="https://gitee.com/merlynr/img-store/raw/master/2021710/1625906179169.png" alt="enter description here">，而单个事件是来自于事件类型的有限集合<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>E</mi><mo>=</mo><mo>{</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msub><mi>e</mi><mi>L</mi></msub><mo>}</mo></math></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021710/1625907751234.png" alt="predict model"></p><h3 id="MMs"><a href="#MMs" class="headerlink" title="MMs"></a>MMs</h3><p><a href="https://blog.zuishuailcq.xyz/2021/07/05/Markov%20Model%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%89/">Markov Model（马尔可夫模型） | 吾辈之人，自当自强不息！</a></p><p>同时文中为了保障预测模型准确性，提出构建 <strong><font color="#006400">动态MMs</font></strong> ，通过存储单个事件在数据集中的次数、紧跟事件发生的下一个事件在数据集中的次数（事件A的下一个事件为B，这里指B的次数）和转换矩阵。还可以通过将折扣因子与事件数量结合，这样就可以在加入新数据时更新折扣因子来提供更多权重。</p><blockquote><p><strong><font color="#9400D3">知识补充</font></strong> 折扣因子<br><a href="https://www.zhihu.com/question/61389929">(95 封私信 / 79 条消息) 马尔可夫决策过程中为什么需要discount factor ，也就是问为啥时间近的状态影响越大？ - 知乎</a><br><a href="https://www.jianshu.com/p/678f57342d0b">#David Silver Reinforcement Learning # 笔记2-MDP - 简书</a><br><a href="https://www.cnblogs.com/wacc/p/5391209.html">强化学习笔记1 - Hiroki - 博客园</a></p></blockquote><h3 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h3><h3 id="混合模型"><a href="#混合模型" class="headerlink" title="混合模型"></a>混合模型</h3><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;h3 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h3&gt;&lt;p&gt;对于多</summary>
      
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="RPA" scheme="https://merlynr.github.io/tags/RPA/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>KNN（K近邻法 K Nearest Neighbors）</title>
    <link href="https://merlynr.github.io/2021/07/05/KNN%EF%BC%88K%E8%BF%91%E9%82%BB%E6%B3%95%20K%20Nearest%20Neighbors%EF%BC%89/"/>
    <id>https://merlynr.github.io/2021/07/05/KNN%EF%BC%88K%E8%BF%91%E9%82%BB%E6%B3%95%20K%20Nearest%20Neighbors%EF%BC%89/</id>
    <published>2021-07-04T16:00:00.000Z</published>
    <updated>2021-07-04T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/25994179">一文搞懂k近邻（k-NN）算法（一） - 知乎</a></p><p><a href="https://blog.csdn.net/qq_20412595/article/details/82013677">机器学习算法（2）之K近邻算法_不曾走远的博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/pxhdky/article/details/85080980">【机器学习】K近邻法（KNN）与kd树原理详解_齐在的专栏-CSDN博客</a></p><p>TODO 序列KNN</p><h2 id="KNN概述"><a href="#KNN概述" class="headerlink" title="KNN概述"></a>KNN概述</h2><ul><li>常用有监督学习方法</li><li>常用分类方法</li><li>同时也是回归方法</li><li>是懒惰学习</li></ul><blockquote><p><font color="#ff7500">扩展学习</font><br>懒惰学习是一种训练集处理方法，其会<font color="#012C54">在收到测试样本的同时进行训练</font>，与之相对的是急切学习，其会<font color="#8A2BE2">在训练阶段开始对样本进行学习</font>处理。</p></blockquote><p><font color="#FF8C00">基本思路：</font><br>如果一个待分类样本在特征空间中的k个最相似(即特征空间中K近邻)的样本中的大多数属于某一个类别，则该样本也属于这个类别，即近朱者赤，近墨者黑。</p><h2 id="KNN算法介绍"><a href="#KNN算法介绍" class="headerlink" title="KNN算法介绍"></a>KNN算法介绍</h2><h3 id="KNN模型"><a href="#KNN模型" class="headerlink" title="KNN模型"></a>KNN模型</h3><p><strong>kNN使用的模型实际上对应于对特征空间的划分。</strong></p><p><font color="#006400">由三个及基本要素组成：</font></p><ul><li>距离度量</li><li>k值的选择</li><li>决策规划</li></ul><ol><li>距离度量</li></ol><p>KNN中使用的距离度量可以是欧式距离、曼哈顿距离、切比雪夫距离或者一般的闵可夫斯基距离。</p><blockquote><p><font color="#9932CC">知识补充</font><br>设特征空间 $X$ 是 $n$ 维实数向量空间<math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mi>n</mi></msup></math>，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub></math>∈ $X$ ，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mo>(</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msubsup><mo>)</mo><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msubsup><msup><mo>)</mo><mi>T</mi></msup></math>，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>j</mi></msub><mo>=</mo><mo>(</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msubsup><mo>)</mo><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msubsup><msup><mo>)</mo><mi>T</mi></msup></math></p><ol><li><p>闵可夫斯基距离（Minkowski distance,<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi>p</mi></msub></math>距离）<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub></math>的<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi>p</mi></msub></math>距离定义为：<br><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625491452207.png"><br>其中，p ≥ 1 。 </p></li><li><p>曼哈顿距离（Manhattan distance）<br>当p = 1 时，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi>p</mi></msub></math>距离就变成了曼哈顿距离：<br><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625491491032.png"></p></li><li><p>欧式距离（Euclidean distance）<br>当p = 2时，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi>p</mi></msub></math>距离就变成了欧式距离：<br><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625491674920.png"></p></li><li><p>切比雪夫距离（Chebyshev distance）<br>当<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>=</mo><mo>&#x221E;</mo><mo>,</mo><msub><mi>L</mi><mi>p</mi></msub></math>距离就变成了切比雪夫距离，它是各个坐标距离的最大值：<br><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625491693716.png"></p></li></ol></blockquote><ol start="2"><li>k值选择（借鉴李航–统计学习方法）</li></ol><p>如果k值较小，则训练误差减少，只有与输入实例相似的训练实例才会对于预测结果起作用,“学习”<font color="#D2691E">近似误差会减小</font>，但泛化误差提高了，预测结果会对近邻实例点非常敏感。k值较小意味着模型变得复杂，容易发生<font color="#0000FF">过拟合</font>。</p><p>如果k值较大，可以减少泛化误差，其优点是可以<font color="#D2691E">减少学习的估计误差</font>，但训练误差会增加，这时与输入实例相差较远的训练实例也会对预测结果起作用。k值较大意味着模型变得简单，容易发生<font color="#0000FF">欠拟合</font>。</p><p>通常情况下，我们需要对 k 经过多种尝试，来决定到底使用多大的 k 来作为最终参数。k通常会在3～10直接取值，或者是k等于训练数据的<font color="#DC143C">平方根</font>。比如15个数据，可能会取k=4。<br>第二种方法，选择能使测试集达到最优的k kk，即能够使得如MAPE等衡量预测准确度的统计量达到最小；<br>第三种方法，同时训练多个函数不同参数k kk的模型，然后取所有模型的预测值的平均值作为最终的预测值。</p><p>当k = 1时，k近邻算法就是最近邻算法。k值一般<font color="#FF1493">采用交叉验证法选取最优值</font>。</p><ol start="3"><li>决策规划</li></ol><p>通常，在分类任务中使用投票法计算最终预测结果，在回归任务中使用平均法，还可基于距离远近进行加权平均或加权投票。</p><h3 id="KNN算法描述"><a href="#KNN算法描述" class="headerlink" title="KNN算法描述"></a>KNN算法描述</h3><p>下面以<font color="#008B8B">分类</font>任务为例，介绍KNN算法，回归任务与此类似，区别不大。</p><p>输入：训练数据集<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>D</mi><mo>=</mo><mo>{</mo><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)</mo><msubsup><mo>}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></math>    ，其中，<img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625492364666.png"> 是实例的类别。<br>过程：</p><ul><li>根据给定的距离度量，在训练集D中找出与x最邻近的k个点，涵盖着k 个点的领域记为<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></math>；</li><li>在<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></math>中根据分类决策规则决定x的类别y： <img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625492543638.png" alt="所属类别"><br>输出：测试样本x xx所属的类别y yy。</li></ul><h2 id="KNN算法实现"><a href="#KNN算法实现" class="headerlink" title="KNN算法实现"></a>KNN算法实现</h2><h3 id="KNN算法蛮力实现"><a href="#KNN算法蛮力实现" class="headerlink" title="KNN算法蛮力实现"></a>KNN算法蛮力实现</h3><p> 首先我们看看最想当然的方式。</p><pre><code> 既然我们要找到k个最近的邻居来做预测，那么我们只需要计算预测样本和所有训练集中的样本的距离，然后计算出最小的k个距离即可，接着多数表决，很容易做出预测。这个方法的确简单直接，在样本量少，样本特征少的时候有效。但是在实际运用中很多时候用不上，为什么呢？因为我们经常碰到样本的特征数有上千以上，样本量有几十万以上，如果我们这要去预测少量的测试集样本，算法的时间效率很成问题。因此，这个方法我们一般称之为蛮力实现。&lt;font color=&quot;#1E90FF&quot;&gt;比较适合于少量样本的简单模型的时候用&lt;/font&gt;。</code></pre><h3 id="KD树实现原理"><a href="#KD树实现原理" class="headerlink" title="KD树实现原理"></a>KD树实现原理</h3><pre><code>KD树算法没有一开始就尝试对测试样本分类，而是先对训练集建模，建立的模型就是KD树，建好了模型再对测试集做预测。所谓的KD树就是K个特征维度的树，注意这里的K和KNN中的K的意思不同。KNN中的K代表特征输出类别，KD树中的K代表样本特征的维数。为了防止混淆，后面我们称特征维数为n。</code></pre><p>KD树算法包括三步，第一步是建树，第二部是搜索最近邻，最后一步是预测。</p><h4 id="KD树的建立"><a href="#KD树的建立" class="headerlink" title="KD树的建立"></a>KD树的建立</h4><p>我们首先来看建树的方法。KD树建树采用的是从m个样本的n维特征中，分别计算n个特征的取值的方差，用<font color="#DC143C">方差最大</font>的第k维特征<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>k</mi></msub></math>来作为<font color="#B22222">根节点</font>。对于这个特征，我们选择特征<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>k</mi></msub></math>的取值的中位数<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>kv</mi></msub></math>对应的样本作为划分点，对于所有第k维特征的取值小于<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>kv</mi></msub></math>的样本，我们划入左子树，对于第k维特征的取值大于等于<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>kv</mi></msub></math>的样本，我们划入右子树，对于左子树和右子树，我们采用和刚才同样的办法来找方差最大的特征来做<font color="#B22222">根节点，递归</font>的生成KD树。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625494718364.png" alt="构建KD树"></p><p>比如我们有二维样本6个，{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，构建kd树的具体步骤为：</p><ol><li>找到划分的特征。6个数据点在x，y维度上的数据方差分别为6.97，5.37，所以在x轴上方差更大，用第1维特征建树。</li><li>确定划分点（7,2）。根据x维上的值将数据排序，6个数据的中值(所谓中值，即中间大小的值)为7，所以划分点的数据是（7,2）。这样，该节点的分割超平面就是通过（7,2）并垂直于：划分点维度的直线x=7；（很显然，中位数为6 ，这里选择（5,4）或者(7,2)都是可以的。这种情况任选一个即可）</li><li>确定左子空间和右子空间。 分割超平面x=7将整个空间分为两部分：x&lt;=7的部分为左子空间，包含3个节点={(2,3),(5,4),(4,7)}；另一部分为右子空间，包含2个节点={(9,6)，(8,1)}。</li><li>用同样的办法划分左子树的节点{(2,3),(5,4),(4,7)}和右子树的节点{(9,6)，(8,1)}。最终得到KD树。</li><li>后续步骤反复上面的，<font color="#8FBC8F">直到两个子区域没有实例存在时停止（这意味着最后所有训练实例都对应一个叶结点或内部结点），从而形成kd树的区域划分</font>。</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625495422262.png" alt="KD树"></p><p><font color="#DC143C">标准kNN算法的切分特征选择是按顺序的，后来对kd树的一个重大改进是选择方差最大的特征，方差越大，不同实例点区分越明显，更方便进行划分。</font></p><h4 id="KD树搜索最近邻"><a href="#KD树搜索最近邻" class="headerlink" title="KD树搜索最近邻"></a>KD树搜索最近邻</h4><p>当我们生成KD树以后，就可以去预测测试集里面的样本目标点了。对于一个目标点，我们<font color="#9932CC">首先在KD树里面找到包含目标点的叶子节点</font>。<font color="#0000FF">以目标点为圆心，以目标点到叶子节点样本实例的距离为半径，得到一个超球体</font>，<font color="#B22222">最近邻的点一定在这个超球体内部</font>。然后返回叶子节点的父节点，检查另一个子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点寻找是否有更加近的近邻,有的话就更新最近邻。如果不相交那就简单了，我们直接返回父节点的父节点，在另一个子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625496230689.png" alt="目标点为（2，4.5）"></p><p>从上面的描述可以看出，KD树划分后可以大大减少无效的最近邻搜索，很多<font color="#8A2BE2">样本点由于所在的超矩形体和超球体不相交，根本不需要计算距离。大大节省了计算时间。</font></p><p>先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径&lt;(7,2)，(5,4)，(4,7)&gt;，但 （4,7）与目标查找点的距离为3.202，而（5,4）与查找点之间的距离为3.041，所以（5,4）为查询点的最近点； 以（2，4.5）为圆心，以3.041为半径作圆，如下图所示。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找，也就是将（2,3）节点加入搜索路径中得&lt;(7,2)，(2,3)&gt;；于是接着搜索至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5；回溯查找至（5,4），直到最后回溯到根结点（7,2）的时候，以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如下图所示。至此，搜索路径回溯完，返回最近邻点（2,3），最近距离1.5。</p><h3 id="球树实现原理"><a href="#球树实现原理" class="headerlink" title="球树实现原理"></a>球树实现原理</h3><p>KD树算法虽然提高了KNN搜索的效率，但是在某些时候效率并不高，比如当处理不均匀分布的数据集时,不管是近似方形，还是矩形，甚至正方形，都不是最好的使用形状，因为他们都有角。一个例子如下图：<br><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625496491462.png" alt="enter description here"></p><p>　　如果黑色的实例点离目标点星点再远一点，那么虚线圆会如红线所示那样扩大，导致与左上方矩形的右下角相交，既然相 交了，那么就要检查这个左上方矩形，而实际上，最近的点离星点的距离很近，检查左上方矩形区域已是多余。于此我们看见，KD树把二维平面划分成一个一个矩形，但矩形区域的角却是个难以处理的问题。</p><p>　　为了优化超矩形体导致的搜索效率的问题，有人引入了球树，这种结构可以优化上面的这种问题。</p><p><strong><font color="#7FFF00">球树的建立</font></strong></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625496601030.png" alt="球树"></p><ol><li>先构建一个超球体，这个超球体是可以包含所有样本的最小球体。</li><li>从球中选择一个离球的中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个上，然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径。这样我们得到了两个子超球体，和KD树里面的左右子树对应。（PS:<font color="#B22222">这里选择两个点后，就以这两个点来聚类，所以先确定的是以这两个点为中心来计算其他点到该中心的距离。当所有点都确定自己的中心后，再重新计算一次该超球体的半径和球心</font>。）</li><li>对于这两个子超球体，递归执行步骤2，最终得到了一个球树。</li></ol><p>　　可以看出KD树和球树类似，主要区别在于球树得到的是节点样本组成的最小超球体，而KD得到的是节点样本组成的超矩形体，这个超球体要与对应的KD树的超矩形体小，这样在做最近邻搜索的时候，可以避免一些无谓的搜索。</p><h2 id="KNN优缺点"><a href="#KNN优缺点" class="headerlink" title="KNN优缺点"></a>KNN优缺点</h2><p>优点：</p><ol><li>结构简单；</li><li>无数据输入假定，准确度高，对异常点不敏感。</li><li> 训练时间复杂度比支持向量机之类的算法低，仅为O(n)</li><li> 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合</li></ol><p>缺点：</p><ol><li>计算复杂度高、空间复杂度高；</li><li>样本不平衡时，对稀有类别预测准确度低；</li><li>使用懒惰学习，预测速度慢。</li><li>KD树，球树之类的模型建立需要大量的内存</li><li>相比决策树模型，KNN模型可解释性不强<h2 id="什么情况下选择KNN"><a href="#什么情况下选择KNN" class="headerlink" title="什么情况下选择KNN"></a>什么情况下选择KNN</h2></li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/202177/1625667539256.png" alt="choose"></p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><p><a href="https://www.cnblogs.com/ybjourney/p/4702562.html">机器学习（一）——K-近邻（KNN）算法 - Yabea - 博客园</a></p><h2 id="序列KNN"><a href="#序列KNN" class="headerlink" title="序列KNN"></a>序列KNN</h2><p><a href="https://antkillerfarm.github.io/ml/2017/10/19/Machine_Learning_28.html">机器学习（二十八）——KNN, AutoML, 数据不平衡问题</a></p><p><a href="https://blog.csdn.net/qq_41196612/article/details/107265167">R语言实战——基于KNN聚类的时间序列分析预测_三只佩奇不结义的博客-CSDN博客_r语言knn回归及预测</a></p><p><a href="https://www.coder.work/article/383913">python - 如何使用 KNN/K-means 对数据帧中的时间序列进行聚类 - IT工具网</a></p><p><a href="https://github.com/iwuqing/Time-Series-Classification-based-on-KNN">iwuqing/Time-Series-Classification-based-on-KNN: 基于KNN聚类算法结合Dynamic Time Warping（动态时间调整）的时间序列分类</a></p><p><a href="https://github.com/vvanggeng/TSC-KNN">vvanggeng/TSC-KNN: 基于KNN和DTW的时间序列分类</a></p>]]></content>
    
    
    <summary type="html">【可選】文章描述</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="machine leaning" scheme="https://merlynr.github.io/tags/machine-leaning/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Markov Model（马尔可夫模型）</title>
    <link href="https://merlynr.github.io/2021/07/05/Markov%20Model%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%89/"/>
    <id>https://merlynr.github.io/2021/07/05/Markov%20Model%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%89/</id>
    <published>2021-07-04T16:00:00.000Z</published>
    <updated>2021-07-06T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/mantch/p/11203748.html">一次性弄懂马尔可夫模型、隐马尔可夫模型、马尔可夫网络和条件随机场！(词性标注代码实现) - mantch - 博客园</a></p><h2 id="马尔可夫网络、马尔可夫模型、马尔可夫过程、贝叶斯网络的区别"><a href="#马尔可夫网络、马尔可夫模型、马尔可夫过程、贝叶斯网络的区别" class="headerlink" title="马尔可夫网络、马尔可夫模型、马尔可夫过程、贝叶斯网络的区别"></a>马尔可夫网络、马尔可夫模型、马尔可夫过程、贝叶斯网络的区别</h2><p>共分六点说明这些概念【<font color="#DC143C">这6点是依次递进的，不要跳跃着看</font>】：</p><ol><li>将随机变量作为结点，若两个随机变量相关或者不独立，则将二者连接一条边；若给定若干随机变量，则形成一个有向图，即构成一个<strong>网络</strong>。</li><li>如果该网络是有向无环图，则这个网络称为<strong>贝叶斯网络</strong>。</li><li>如果这个图退化成线性链的方式，则得到<strong>马尔可夫模型</strong>；因为每个结点都是随机变量，将其看成各个时刻(或空间)的相关变化，以随机过程的视角，则可以看成是<strong>马尔可夫过程</strong>。</li><li>若上述网络是无向的，则是无向图模型，又称<strong>马尔可夫随机场</strong>或者<strong>马尔可夫网络</strong>。</li><li>如果在给定某些条件的前提下，研究这个马尔可夫随机场，则得到<strong>条件随机场</strong>。</li><li>如果使用条件随机场解决标注问题，并且进一步将条件随机场中的网络拓扑变成线性的，则得到<strong>线性链条件随机场</strong>。</li></ol><h2 id="马尔可夫模型"><a href="#马尔可夫模型" class="headerlink" title="马尔可夫模型"></a>马尔可夫模型</h2><h3 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h3><p>马尔可夫过程（Markov process）是一类<font color="#00FFFF">随机</font>过程。它的原始模型是马尔可夫链。<br>该过程具有如下特性：在已知目前状态（现在）的条件下，它未来的演变（将来）<font color="#0000FF">不依赖</font>于它以往的演变 (过去 )。</p><p>每个状态的转移只依赖于之前的n个状态，这个过程被称为1个n阶的模型，其中n是影响转移状态的数目。最简单的马尔可夫过程就是一阶过程，<font color="#006400">每一个状态的转移只依赖于其之前的那一个状态</font>，这个也叫作<strong>马尔可夫性质</strong>。</p><p>假设这个模型的每个状态都只依赖于之前的状态，这个假设被称为<font color="#1E90FF">马尔科夫假设</font>，这个假设可以大大的简化这个问题。显然，这个假设可能是一个非常糟糕的假设，导致很多重要的信息都丢失了。<br><img src="https://gitee.com/merlynr/img-store/raw/master/202176/1625557564343.png"></p><p>假设天气服从<strong>马尔可夫链</strong>：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202176/1625557700847.png" alt="天气"></p><p>从上面这幅图可以看出：</p><ul><li>假如今天是晴天，明天变成阴天的概率是0.1</li><li>假如今天是晴天，明天任然是晴天的概率是0.9，和上一条概率之和为1，这也符合真实生活的情况。</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/202178/1625728052403.png" alt="表格"></p><p>由上表我们可以得到马尔可夫链的<strong>状态转移矩阵</strong>：<br><img src="https://gitee.com/merlynr/img-store/raw/master/202176/1625557951076.png" alt="状态转移矩阵"></p><p>因此，一阶马尔可夫过程定义了以下三个部分：</p><ul><li>状态：晴天和阴天</li><li>初始向量：定义系统在时间为0的时候的状态的概率</li><li>状态转移矩阵：每种天气转换的概率</li></ul><p>马尔可夫模型（Markov Model）是一种<font color="#DC143C">统计模型</font>，广泛应用在语音识别，词性自动标注，音字转换，概率文法等各个自然语言处理等应用领域。经过长期发展，尤其是在语音识别中的成功应用，使它成为一种通用的统计工具。到目前为止，它一直被认为是实现快速精确的语音识别系统的最成功的方法。</p><h2 id="隐马尔可夫模型（HMM）"><a href="#隐马尔可夫模型（HMM）" class="headerlink" title="隐马尔可夫模型（HMM）"></a>隐马尔可夫模型（HMM）</h2><blockquote><p> 在某些情况下马尔科夫过程不足以描述我们希望发现的模式。回到之前那个天气的例子，一个隐居的人可能不能直观的观察到天气的情况，但是有一些海藻。民间的传说告诉我们海藻的状态在某种概率上是和天气的情况相关的。在这种情况下我们有两个状态集合，一个可以观察到的状态集合（海藻的状态）和一个隐藏的状态（天气的状况）。我们希望能找到一个算法可以根据海藻的状况和马尔科夫假设来预测天气的状况。</p></blockquote><p>而这个算法就叫做**隐马尔可夫模型(HMM)**。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202176/1625559690747.png" alt="HMM"></p><p>隐马尔可夫模型 (Hidden Markov Model) 是一种<strong>统计模型</strong>，用来描述一个含有隐含未知参数的马尔可夫过程。<strong>它是结构最简单的动态贝叶斯网，这是一种著名的有向图模型，</strong> 主要用于<font color="#FF00FF">时序</font>数据建模，在语音识别、自然语言处理等领域有广泛应用。</p><h3 id="隐马尔可夫三大问题"><a href="#隐马尔可夫三大问题" class="headerlink" title="隐马尔可夫三大问题"></a>隐马尔可夫三大问题</h3><p><font color="#9400D3">注意</font></p><ol><li>给定模型，如何有效计算产生观测序列的概率？换言之，如何评估模型与观测序列之间的<font color="#FF1493">匹配程度</font>？</li><li>给定模型和观测序列，如何找到与此观测序列最匹配的状态序列？换言之，如何根据观测序列推断出隐藏的<font color="#B22222">模型状态</font>？</li><li>给定观测序列，如何调整模型参数使得该序列出现的概率最大？换言之，如何训练模型使其能最好地<font color="#B22222">描述</font>观测数据？</li></ol><p>前两个问题是模式识别的问题：1) 根据隐马尔科夫模型得到一个可观察状态序列的概率(<strong>评价</strong>)；2) 找到一个隐藏状态的序列使得这个序列产生一个可观察状态序列的概率最大(<strong>解码</strong>)。第三个问题就是根据一个可以观察到的状态序列集产生一个隐马尔科夫模型（<strong>学习</strong>）。</p><p>对应的三大问题解法：</p><ol><li>向前算法(Forward Algorithm)、向后算法(Backward Algorithm)</li><li>维特比算法(Viterbi Algorithm)</li><li>鲍姆-韦尔奇算法(Baum-Welch Algorithm) (约等于EM算法)</li></ol><blockquote><p>小明现在有三天的假期，他为了打发时间，可以在每一天中选择三件事情来做，这三件事情分别是散步、购物、打扫卫生(<strong>对应着可观测序列</strong>)，可是在生活中我们所做的决定一般都受到天气的影响，可能晴天的时候想要去购物或者散步，可能下雨天的时候不想出门，留在家里打扫卫生。而天气(晴天、下雨天)就属于<strong>隐藏状态</strong>，用一幅概率图来表示这一马尔可夫过程：</p></blockquote><p><img src="https://gitee.com/merlynr/img-store/raw/master/202176/1625563725550.png" alt="场景"></p><p>那么，我们提出三个问题，分别对应马尔可夫的<font color="#B22222">三大</font>问题：</p><ol><li>已知整个模型，我观测到连续三天做的事情是：散步，购物，收拾。那么，根据模型，计算产生这些行为的概率是多少。</li><li>同样知晓这个模型，同样是这三件事，我想猜，这三天的天气是怎么样的。</li><li>最复杂的，我只知道这三天做了这三件事儿，而其他什么信息都没有。我得建立一个模型，晴雨转换概率，第一天天气情况的概率分布，根据天气情况选择做某事的概率分布。</li></ol><h3 id="第一个问题解法"><a href="#第一个问题解法" class="headerlink" title="第一个问题解法"></a>第一个问题解法</h3><ol><li><strong>遍历算法</strong></li></ol><p>假设第一天(T=1 时刻)是晴天，想要购物，那么就把图上的对应概率相乘就能够得到了。<br>第二天(T=2 时刻)要做的事情，在第一天的概率基础上乘上第二天的概率，依次类推，最终得到这三天(T=3 时刻)所要做的事情的概率值，这就是遍历算法，简单而又粗暴。但问题是<font color="#2F4F4F">用遍历算法的复杂度会随着观测序列和隐藏状态的增加而成指数级增长。</font></p><p><font color="#B22222">复杂度为</font>：<math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn><mi>T</mi><msup><mi>N</mi><mi>T</mi></msup></math></p><p><font color="#8A2BE2">理解：</font>每次计算行为发生概率都从最开始遍历计算</p><ol start="2"><li><p><strong>前向算法</strong></p><ol><li>假设第一天要购物，那么就计算出第一天购物的概率(包括晴天和雨天)；假设第一天要散步，那么也计算出来，依次枚举。</li><li>假设前两天是购物和散步，也同样计算出这一种的概率；假设前两天是散步和打扫卫生，同样计算，枚举出前两天行为的概率。</li><li>第三步就是计算出前三天行为的概率。</li></ol></li></ol><p>第二步中要求的概率可以在第一步的基础上进行，同样的，第三步也会<font color="#0000FF">依赖</font>于第二步的计算结果。那么这样做就能够<strong>节省很多计算环节，类似于动态规划</strong>。</p><p><font color="#B22222">复杂度为</font>：<math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>N</mi><mn>2</mn></msup><mi>T</mi></math></p><ol start="3"><li>后向算法</li></ol><p>跟前向算法相反，我们知道总的概率肯定是1，那么B_t=1，也就是最后一个时刻的概率合为1，先计算前三天的各种可能的概率，在计算前两天、前一天的数据，<font color="#696969">跟前向算法相反</font>的计算路径。</p><h3 id="第二个问题解法"><a href="#第二个问题解法" class="headerlink" title="第二个问题解法"></a>第二个问题解法</h3><ol><li>维特比算法（Viterbi）</li></ol><blockquote><p>维特比算法是一个特殊但应用最广的<strong>动态规划算法</strong>。利用动态规划，可以解决任何一个图中的<strong>最短</strong>路径问题。而维特比算法是针对一个特殊的图—篱笆网络（Lattice）的有向图最短路径问题而提出的。它之所以重要，是因为凡是使用<font color="#057748">隐含马尔可夫模型</font>描述的问题都可以用它来解码，包括今天的数字通信、语音识别、机器翻译、拼音转汉字、分词等。</p></blockquote><p>维特比算法一般用于模式识别，通过观测数据来<font color="#FF1493">反推出隐藏状态</font>。</p><p>因为是要根据观测数据来反推，所以这里要进行一个假设，<strong>假设这三天所做的行为分别是：散步、购物、打扫卫生</strong>，那么我们要求的是这三天的天气(路径)分别是什么。</p><ol><li>初始计算第一天下雨和第一天晴天去散步的概率值：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>△</mi><mn>1</mn></msub><mo>(</mo><mi>R</mi><mo>)</mo></math>表示第一天下雨的概率<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x3C0;</mi><mi>R</mi></msub></math>表示中间的状态(下雨)概率<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>b</mi><mi>R</mi></msub><mo>(</mo><msub><mi>O</mi><mn>1</mn></msub><mo>=</mo><mi>w</mi><mo>)</mo></math>表示下雨并且散步的概率<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>a</mi><mrow><mi>R</mi><mo>-</mo><mi>R</mi></mrow></msub></math>表示下雨天到下雨天的概率</li></ol><p><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>△</mi><mn>1</mn></msub><mo>(</mo><mi>R</mi><mo>)</mo></math>=<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x3C0;</mi><mi>R</mi></msub><mo>*</mo><msub><mi>b</mi><mi>R</mi></msub><mo>(</mo><msub><mi>O</mi><mn>1</mn></msub><mo>=</mo><mi>w</mi><mo>)</mo></math>=0.6 * 0.1 = 0.06</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>△</mi><mn>1</mn></msub><mo>(</mo><mi>S</mi><mo>)</mo></math>=<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x3C0;</mi><mi>S</mi></msub><mo>*</mo><msub><mi>b</mi><mi>S</mi></msub><mo>(</mo><msub><mi>O</mi><mn>1</mn></msub><mo>=</mo><mi>w</mi><mo>)</mo></math>=0.4 * 0.6 = 0.24</p><p><font color="#006400">初始路径</font>为：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x3D5;</mi><mn>1</mn></msub><mo>(</mo><mi>R</mi><mo>)</mo></math>=Rainy<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x3D5;</mi><mn>1</mn></msub><mo>(</mo><mi>S</mi><mo>)</mo></math>=Sunny</p><ol start="2"><li>计算第二天下雨和第二天晴天去购物的概率值:</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/202177/1625641072263.png" alt="行为概率"></p><p><font color="#00FFFF">对应路径为：</font></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202177/1625642052293.png" alt="对应路径"></p><ol start="3"><li>计算第三天下雨和第三天晴天去打扫卫生的概率值：</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/202177/1625642832307.png" alt="第三天概率"></p><p><font color="#228B22">对应路径为：</font></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202177/1625642928241.png" alt="行为路径"></p><ol start="4"><li><p>比较每一步中△的概率大小，选取最大值并找到对应的路径，依次类推就能找到最有可能的隐藏状态路径。</p><ol><li>第一天的概率最大值为 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>△</mi><mn>1</mn></msub><mi>S</mi></math>，对应路径为Sunny，</li><li>第二天的概率最大值为 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>△</mi><mn>2</mn></msub><mi>S</mi></math>，对应路径为Sunny，</li><li>第三天的概率最大值为 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>△</mi><mn>3</mn></msub><mi>S</mi></math>，对应路径为Rainy。</li></ol></li><li><p>合起来的路径就是Sunny-&gt;Sunny-&gt;Rainy，这就是我们所求。</p></li></ol><h3 id="第三个问题解法"><a href="#第三个问题解法" class="headerlink" title="第三个问题解法"></a>第三个问题解法</h3><p>鲍姆-韦尔奇算法(Baum-Welch Algorithm) (约等于<strong>EM</strong>算法)</p><p>如果训练数据只有观测序列而没有状态序列，即{<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>O</mi><mn>1</mn></msub><mo>,</mo><msub><mi>O</mi><mn>2</mn></msub><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msub><mi>O</mi><mi>S</mi></msub></math>}此时HMM的学习就得使用EM算法了，这是<font color="#FF1493">非监督</font>学习。</p><p>通常，如果给定数据和已经模型，那么求模型参数我们会用<font color="#8A2BE2">极大似然估计法</font>，但是<font color="#8B008B">如果变量中含有隐变量，无法用极大似然求解</font>（对数式子里面有求和，难以求出解析解），此时就可以使用EM算法。考虑HMM，观测序列 O是显变量，而状态变量I  则是隐变量，所以HMM实际上是<font color="#556B2F">含有隐变量的概率模型</font></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202177/1625646003916.png" alt="HMM的概率模型 | λ为模型参数"></p><blockquote><p><font color="#00FFFF">知识补充</font><br>极大似然估计<br>利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！<br>换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“<font color="#FF8C00">模型已定，参数未知</font>”。</p></blockquote><p>可以使用EM算法来求得模型参数。</p><p>关于EM算法流程，有多个版本，但是仔细学习可以发现是大同小异的，以下使用《统计学习方法》上介绍的EM算法流程。</p><p><a href="https://blog.csdn.net/qq_37334135/article/details/86302735">HMM学习笔记（二）：监督学习方法与Baum-Welch算法_成都往右的博客-CSDN博客</a></p><h2 id="马尔可夫网络"><a href="#马尔可夫网络" class="headerlink" title="马尔可夫网络"></a>马尔可夫网络</h2><h3 id="因子图"><a href="#因子图" class="headerlink" title="因子图"></a>因子图</h3><p>WiKIpedia：将一个具有多变量的全局函数因子分解，得到几个局部函数的乘积，以此为基础得到的一个双向图叫做<font color="#00CED1">因子图</font>（Factor Graph）。</p><p>通俗来讲，所谓因子图就是对函数进行因子分解得到的一种<strong>概率图</strong>。一般内含两种节点：变量节点和函数节点。我们知道，一<font color="#FF1493">个全局函数通过因式分解能够分解为多个局部函数的乘积</font>，这些局部函数和对应的变量关系就体现在因子图上。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202177/1625649785149.png" alt="栗子"></p><p>其中fA,fB,fC,fD,fE为各函数，表示变量之间的关系，可以是条件概率也可以是其他关系。其对应的因子图为：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202177/1625659847778.png" alt="变量-函数之间因子图"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202177/1625659895044.png" alt="变量-函数之间因子图"></p><h3 id="马尔可夫网络-1"><a href="#马尔可夫网络-1" class="headerlink" title="马尔可夫网络"></a>马尔可夫网络</h3><blockquote><p>我们已经知道，<strong>有向</strong>图模型，又称作<font color="#bf242a">贝叶斯网络</font>，但在有些情况下，强制对某些结点之间的边增加方向是不合适的。<strong>使用没有方向的无向边，形成了无向图模型</strong>（Undirected Graphical Model,UGM）, 又被称为<strong>马尔可夫随机场或者马尔可夫网络</strong>（Markov Random Field, MRF or Markov network）。</p></blockquote><p><img src="https://gitee.com/merlynr/img-store/raw/master/202177/1625660204643.png" alt="MRF"></p><p>设X=(X1,X2…Xn)和Y=(Y1,Y2…Ym)都是<font color="#006400">联合随机变量</font>，若随机变量Y构成一个无向图 G=(V,E)表示的马尔可夫随机场（MRF），则条件概率分布P(Y|X)称为<strong>条件随机场</strong>（Conditional Random Field, 简称CRF，后续新的博客中可能会阐述CRF）。如下图所示，便是一个线性链条件随机场的无向图模型：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202177/1625664493380.png" alt="CRF"></p><p>在概率图中，求某个变量的边缘分布是常见的问题。这问题有很多求解方法，其中之一就是<font color="#A52A2A">把贝叶斯网络或马尔可夫随机场转换成因子图，然后用sum-product算法求解</font>。换言之，基于因子图可以用<strong>sum-product 算法</strong>高效的求各个变量的边缘分布。</p><p>详细的sum-product算法过程，请查看博文：<a href="https://blog.csdn.net/v_july_v/article/details/40984699">从贝叶斯方法谈到贝叶斯网络_结构之法 算法之道-CSDN博客_贝叶斯</a></p><h2 id="条件随机场-CRF"><a href="#条件随机场-CRF" class="headerlink" title="条件随机场(CRF)"></a>条件随机场(CRF)</h2><p><strong>一个通俗的例子</strong></p><p>假设你有许多小明同学一天内不同时段的照片，从小明提裤子起床到脱裤子睡觉各个时间段都有（小明是照片控！）。现在的任务是对这些照片进行分类。比如有的照片是吃饭，那就给它打上吃饭的标签；有的照片是跑步时拍的，那就打上跑步的标签；有的照片是开会时拍的，那就打上开会的标签。问题来了，你准备怎么干？</p><p>一个简单直观的办法就是，不管这些照片之间的时间顺序，想办法训练出一个多元分类器。就是用一些打好标签的照片作为训练数据，训练出一个模型，直接根据照片的特征来分类。例如，如果照片是早上6:00拍的，且画面是黑暗的，那就给它打上睡觉的标签;如果照片上有车，那就给它打上开车的标签。</p><p>乍一看可以！但实际上，由于我们忽略了这些照片之间的时间顺序这一重要信息，我们的分类器会有缺陷的。举个例子，假如有一张小明闭着嘴的照片，怎么分类？显然难以直接判断，需要参考闭嘴之前的照片，如果之前的照片显示小明在吃饭，那这个闭嘴的照片很可能是小明在咀嚼食物准备下咽，可以给它打上吃饭的标签；如果之前的照片显示小明在唱歌，那这个闭嘴的照片很可能是小明唱歌瞬间的抓拍，可以给它打上唱歌的标签。</p><p>所以，为了让我们的分类器能够有更好的表现，<strong>在为一张照片分类时，我们必须将与它相邻的照片的标签信息考虑进来</strong>。这——就是条件随机场(CRF)大显身手的地方！这就有点类似于词性标注了，只不过把照片换成了句子而已，本质上是一样的。</p><p>如同马尔可夫随机场，条件随机场为具有<strong>无向</strong>的图模型，图中的顶点代表随机变量，顶点间的连线代表随机变量间的相依关系，在条件随机场中，随机变量Y 的分布为条件机率，给定的观察值则为随机变量 X。下图就是一个线性连条件随机场。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202177/1625665022911.png" alt="线性连条件随机场"></p><p>条件概率分布P(Y|X)称为条件随机场。</p><h2 id="EM算法、HMM、CRF的比较"><a href="#EM算法、HMM、CRF的比较" class="headerlink" title="EM算法、HMM、CRF的比较"></a>EM算法、HMM、CRF的比较</h2><ol><li><strong>EM算法</strong>是<font color="#8FBC8F">用于含有隐变量模型</font>的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说<font color="#B22222">EM算法不能保证找到全局最优值</font>。对于EM的导出方法也应该掌握。</li><li><strong>隐马尔可夫模型</strong>是<font color="#8FBC8F">用于标注问题的生成模型</font>。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。马尔科夫三个基本问题：<br>  <strong>概率计算问题</strong>：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法<br> <strong>学习问题</strong>：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。<br> <strong>预测问题</strong>：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径）</li><li><strong>条件随机场CRF</strong>，给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为<font color="#B22222">极大似然估计或正则化的极大似然估计。</font></li><li>之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔可夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。</li><li>HMM和CRF对比：其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/mantch/p/11203748.html&quot;&gt;一次性弄懂马尔可夫模型、隐马尔可夫模型、马尔可夫网络和条件随机场！(词性标注代码实现) - mantch - 博客园&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;马尔可夫网</summary>
      
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="machine leaning" scheme="https://merlynr.github.io/tags/machine-leaning/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>七月前完成</title>
    <link href="https://merlynr.github.io/2021/06/27/%E4%B8%83%E6%9C%88%E5%89%8D%E5%AE%8C%E6%88%90/"/>
    <id>https://merlynr.github.io/2021/06/27/%E4%B8%83%E6%9C%88%E5%89%8D%E5%AE%8C%E6%88%90/</id>
    <published>2021-06-26T16:00:00.000Z</published>
    <updated>2021-06-26T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul><li><input disabled="" type="checkbox"> 机器学习</li><li><input disabled="" type="checkbox"> 预测算法</li><li><input disabled="" type="checkbox"> 五篇小论文</li><li><input disabled="" type="checkbox"> 考虑数据源与公司想结合</li></ul><h2 id="done"><a href="#done" class="headerlink" title="done"></a>done</h2><ol><li>XES文档2.5</li><li>使用ProMLite</li></ol>]]></content>
    
    
    <summary type="html">周学习计划</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
    <category term="daily/weekly" scheme="https://merlynr.github.io/tags/daily-weekly/"/>
    
  </entry>
  
  <entry>
    <title>深度学习与循环神经网络在预测下一个过程事件问题上的初步应用</title>
    <link href="https://merlynr.github.io/2021/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9C%A8%E9%A2%84%E6%B5%8B%E4%B8%8B%E4%B8%80%E4%B8%AA%E8%BF%87%E7%A8%8B%E4%BA%8B%E4%BB%B6%E9%97%AE%E9%A2%98%E4%B8%8A%E7%9A%84%E5%88%9D%E6%AD%A5%E5%BA%94%E7%94%A8/"/>
    <id>https://merlynr.github.io/2021/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9C%A8%E9%A2%84%E6%B5%8B%E4%B8%8B%E4%B8%80%E4%B8%AA%E8%BF%87%E7%A8%8B%E4%BA%8B%E4%BB%B6%E9%97%AE%E9%A2%98%E4%B8%8A%E7%9A%84%E5%88%9D%E6%AD%A5%E5%BA%94%E7%94%A8/</id>
    <published>2021-06-24T16:00:00.000Z</published>
    <updated>2021-06-24T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><font color="#DC143C">TITLE</font>: A Deep Learning Approach for Predicting Process Behaviour at Runtime</p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>描述深度学习与循环神经网络在预测下一个过程事件问题上的初步应用</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>主要介绍了流程预测与自然语言的处理有很多地方类似，同时也有不同之处。</p><p><font color="#FF1493">流程预测与自然语言不同：</font></p><ul><li>过程预测（事件类型数量）中词汇量的大小远小于自然语言词汇的大小</li><li>轨迹的长度远远超过自然语言中的典型句子长度</li><li>通过内部过程逻辑确定或约束过程事件序列，通常通过基于案例数据确定的决策规则确定。然而，以语法和形态规则的形式，自然语言也受到限制</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p><strong><font color="#B22222">这块主要讲了一下业务流程预测相关的研究</font></strong></p><h3 id="预测完成一个案件的剩余时间"><a href="#预测完成一个案件的剩余时间" class="headerlink" title="预测完成一个案件的剩余时间"></a>预测完成一个案件的剩余时间</h3><ol><li>使用事件频率、事件时间和案例数据的增强回归</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624673781121.png" alt="When will this case finally be finished?"><br>2. 将隐马尔可夫模型应用于事件序列和执行时间[基于一个带注释的转换系统]</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624674466379.png" alt="A test-bed for the evaluation of bussiness process prediction techniques"><br>3. 使用聚类树和有限状态机(FSM)来预测运行过程案例的剩余时间</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624674870904.png" alt="Context-aware predictions on bussiness processes: An ensemble-based solution"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624674967541.png" alt="Discovering context-aware models for predicting business process performances"><br>4. 将复杂的事件处理（CEP）应用于事件序列，并培训以预测其未来行为</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675079138.png" alt="Facilitating predictive event-driven process analytics"><br>5. 使用随机petri网模拟</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675153578.png" alt="Prediction of remaining service execution time using stochastic petri nets with arbitrary firing delays"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675288618.png" alt="Prediction of bussiness process durations using non-markovian stochastic petri nets"><br>6. 基于案例数据聚类和回归的预测技术</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675498447.png" alt="A data-driven prediction framework for analyzing and monitoring business process performances"><br>7. 对部分和全部案例采用聚类方法</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675713998.png" alt="Process remaining time prediction using query catalogs"><br>8. 提出了两种基于带注释的转换系统的方法，以及支持向量回归和朴素贝叶斯分类器</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675768877.png" alt="Data-aware remaining time prediction of business process instances"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675838918.png" alt="Time and activity sequence prediction of business process instances"></p><h3 id="流程预测结果评估【二元评估】"><a href="#流程预测结果评估【二元评估】" class="headerlink" title="流程预测结果评估【二元评估】"></a>流程预测结果评估【二元评估】</h3><ol><li>在时间、资源和案例数据上使用决策树</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624676985594.png" alt="Predictive business operations management"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624677076066.png" alt="Business process intelligence"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624677129344.png" alt="Improving bussiness "><br>2. 使用支持向量机</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624686889124.png" alt="Periodic berformance prediction for real-time business process monitoring"><br>3. 基于聚类和局部离群点检测</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624687171172.png" alt="Real-time business process monitoring method for prediction of abnormal termination using knni-based LOF prediction"><br>4. 使用决策树来预测违反线性时序逻辑限制</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624687372135.png" alt="Predictive monitoring of business processes"><br><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624687442053.png" alt="Predictive monitoring of business processes"><br>5. 使用随机森林</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624687491226.png" alt="Complex symbolic sequence encodings for predictive monitoring of business processes"><br>6. 采用神经网络、约束满足和服务质量聚合</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624687569443.png" alt="Comparing and combining predictive business process monitoring techniques"><br>7. 聚类和回归</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624687617567.png" alt="A prediction framework for proactive monitoring aggregate process-performance indicators"></p><h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><h2 id="Process-Prediction-using-RNN"><a href="#Process-Prediction-using-RNN" class="headerlink" title="Process Prediction using RNN"></a>Process Prediction using RNN</h2><h2 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h2><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><h2 id="扩展学习"><a href="#扩展学习" class="headerlink" title="扩展学习"></a>扩展学习</h2><h3 id="阅读paper36【P10】"><a href="#阅读paper36【P10】" class="headerlink" title="阅读paper36【P10】"></a>阅读paper36【P10】</h3><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021627/1624759564094.png" alt="paper36"></p><h3 id="递归神经网络与循环神经网络"><a href="#递归神经网络与循环神经网络" class="headerlink" title="递归神经网络与循环神经网络"></a>递归神经网络与循环神经网络</h3><p><font color="#0000FF">Recursive Neural Network || Recurrent Neural Network</font></p><h3 id="Hidden-Markov-Models-HHM"><a href="#Hidden-Markov-Models-HHM" class="headerlink" title="Hidden Markov Models(HHM)"></a>Hidden Markov Models(HHM)</h3><h3 id="LSTM与RNN"><a href="#LSTM与RNN" class="headerlink" title="LSTM与RNN"></a>LSTM与RNN</h3>]]></content>
    
    
    <summary type="html">对于深度学习在预测中应用的总结</summary>
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>总结“基于机器学习的业务流程系统的预测”中的技术点</title>
    <link href="https://merlynr.github.io/2021/06/24/%E6%80%BB%E7%BB%93%E2%80%9C%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%A2%84%E6%B5%8B%E2%80%9D%E4%B8%AD%E7%9A%84%E6%8A%80%E6%9C%AF%E7%82%B9/"/>
    <id>https://merlynr.github.io/2021/06/24/%E6%80%BB%E7%BB%93%E2%80%9C%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%A2%84%E6%B5%8B%E2%80%9D%E4%B8%AD%E7%9A%84%E6%8A%80%E6%9C%AF%E7%82%B9/</id>
    <published>2021-06-23T16:00:00.000Z</published>
    <updated>2021-06-23T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="论文总体流程"><a href="#论文总体流程" class="headerlink" title="论文总体流程"></a>论文总体流程</h2><p>本文针对三个预测任务提 出 了 两个预测模型 。 一个预测模型是 用来预测流 程结果 的 模 型 ， 本文提 出 了 利 用 深度 学 习 中 序 列 处理 网 络 Ｌ Ｓ ＴＭ算法模 型去<font color="#183B64">预测流程结果 </font>的方法 ， 此方法 旨 在将流程结果 的预测 问 题与 自 然语言处理方 向 相 结合 ， 提供 一个新 的解决思路 。另 一个预测模型 则是用 来<font color="#1E90FF">预测事件活动与 时 间 相关任务</font> 的模 型 ， 此预测 模型 将本文研 究 的预测流程下 一时刻活动与 时 间 、预测流程后续时刻事件 活动 与 时 间 （ 即 剩余周 期 时 间 ） 两个预测任务利 用 一个预测模型 实现 。</p><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><h3 id="关注点"><a href="#关注点" class="headerlink" title="关注点"></a>关注点</h3><p><font color="#9400D3">关注点</font></p><ul><li>预测业务流程的<font color="#008B8B">下一时刻活动和与时间相关活动</font></li><li>预测业务流程中运行案例的未来路径</li><li>预测业务流程运行的剩余周期时间</li><li>预测业务流程执行<font color="#006400">结果</font>以及预测业务流程执行结束后的性能</li></ul><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><h4 id="预测流程结果"><a href="#预测流程结果" class="headerlink" title="预测流程结果"></a>预测流程结果</h4><ol><li>序列处理网络LSTM算法模型【将流程结果的预测问题与自然语言处理方向相结合】</li><li>决策树</li><li>使用支持向量机（SVM）【此方法可以提供描述实时业务的流程当前性能的实施指标，缺点是评估指标仅为准确率，较为单一】</li><li>基于KNN算法和局部异常值检测，此方法提出了 一个通过替换未观察到的属性来生成实例的插补方法，但并未验证其泛用性</li></ol><h4 id="预测事件活动与时间相关任务的模型"><a href="#预测事件活动与时间相关任务的模型" class="headerlink" title="预测事件活动与时间相关任务的模型"></a>预测事件活动与时间相关任务的模型</h4><ol><li>利用自然语言处理中的GRU网络结构、双向循环网络结构、Word2vec技术以及Attention机制进行预测【将本文研 究 的预测流程下一时刻活动与时间、预测流程后续时刻事件活动与时间（  剩余周期时间）两个预测任务利用一个预测模型实现】</li></ol><h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
    
    
    <summary type="html">【可選】文章描述</summary>
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="summarize" scheme="https://merlynr.github.io/tags/summarize/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="process mining" scheme="https://merlynr.github.io/tags/process-mining/"/>
    
  </entry>
  
  <entry>
    <title>2021-6-22[daily]</title>
    <link href="https://merlynr.github.io/2021/06/21/2021-6-22[daily]/"/>
    <id>https://merlynr.github.io/2021/06/21/2021-6-22[daily]/</id>
    <published>2021-06-20T16:00:00.000Z</published>
    <updated>2021-06-21T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NEED-TODO"><a href="#NEED-TODO" class="headerlink" title="NEED TODO"></a>NEED TODO</h2><ul><li><input disabled="" type="checkbox"> 阅读论文SpinalFlow: An Architecture and Dataflow Tailored for Spiking Neural Networks</li><li><input disabled="" type="checkbox"> 制作相关演讲PPT</li><li><input disabled="" type="checkbox"> 学习三个吴恩达机器学习视频</li><li><input disabled="" type="checkbox"> 学习一个中等，一个简单算法</li><li><input disabled="" type="checkbox"> 学习Java相关进阶</li></ul><h2 id="Plan"><a href="#Plan" class="headerlink" title="Plan"></a>Plan</h2><ol><li>10.20-11.20，阅读论文</li><li>13.40-15.30，阅读论文</li><li>15.40-16.20，看机器学习视频</li><li>16.30-17.20，学习算法</li><li>18.10-21.10，看论文</li><li>21.20-22.40，看Java进阶</li></ol><h2 id="Completion-Status"><a href="#Completion-Status" class="headerlink" title="Completion Status"></a>Completion Status</h2><h2 id="TODO-Tomorrow"><a href="#TODO-Tomorrow" class="headerlink" title="TODO Tomorrow"></a>TODO Tomorrow</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;NEED-TODO&quot;&gt;&lt;a href=&quot;#NEED-TODO&quot; class=&quot;headerlink&quot; title=&quot;NEED TODO&quot;&gt;&lt;/a&gt;NEED TODO&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; </summary>
      
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="daily" scheme="https://merlynr.github.io/tags/daily/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>2021-6-24[daily]</title>
    <link href="https://merlynr.github.io/2021/06/21/2021-6-24[daily]/"/>
    <id>https://merlynr.github.io/2021/06/21/2021-6-24[daily]/</id>
    <published>2021-06-20T16:00:00.000Z</published>
    <updated>2021-06-23T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NEED-TODO"><a href="#NEED-TODO" class="headerlink" title="NEED TODO"></a>NEED TODO</h2><ul><li><input disabled="" type="checkbox"> 阅读论文基于机器学习的业务流程系统的预测，总结其中技术点</li><li><input disabled="" type="checkbox"> 学习三个吴恩达机器学习视频</li><li><input disabled="" type="checkbox"> 学习一个中等，一个简单算法</li><li><input disabled="" type="checkbox"> 学习Java相关进阶</li></ul><h2 id="Plan"><a href="#Plan" class="headerlink" title="Plan"></a>Plan</h2><ul><li><input checked="" disabled="" type="checkbox"> <ol><li>10.20-11.20，阅读论文</li></ol></li><li><input disabled="" type="checkbox"> <ol start="2"><li>13.40-15.30，阅读论文</li></ol></li><li><input disabled="" type="checkbox"> <ol start="3"><li>15.40-16.20，看机器学习视频</li></ol></li><li><input disabled="" type="checkbox"> <ol start="4"><li>16.30-17.20，学习算法</li></ol></li><li><input disabled="" type="checkbox"> <ol start="5"><li>18.10-21.10，看论文</li></ol></li><li><input disabled="" type="checkbox"> <ol start="6"><li>21.20-22.40，看Java进阶</li></ol></li></ul><h2 id="Completion-Status"><a href="#Completion-Status" class="headerlink" title="Completion Status"></a>Completion Status</h2><h2 id="TODO-Tomorrow"><a href="#TODO-Tomorrow" class="headerlink" title="TODO Tomorrow"></a>TODO Tomorrow</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;NEED-TODO&quot;&gt;&lt;a href=&quot;#NEED-TODO&quot; class=&quot;headerlink&quot; title=&quot;NEED TODO&quot;&gt;&lt;/a&gt;NEED TODO&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; </summary>
      
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="daily" scheme="https://merlynr.github.io/tags/daily/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>555~四级</title>
    <link href="https://merlynr.github.io/2021/06/01/555~%E5%9B%9B%E7%BA%A7/"/>
    <id>https://merlynr.github.io/2021/06/01/555~%E5%9B%9B%E7%BA%A7/</id>
    <published>2021-05-31T16:00:00.000Z</published>
    <updated>2021-06-01T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h2><p><img src="https://gitee.com/merlynr/img-store/raw/master/202161/1622551964785.png" alt="作文要求"></p><ol><li>时间很紧，第一遍就需要写在纸上</li><li>手写，需要注意字体，建议衡水体</li><li>写的时候右侧也对齐，同时写85%，这样美观</li><li>背一些句型和一些常用词汇的替换词</li><li>切记不要跑题，介意通过直接更改题干来立题</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/202161/1622552071072.png" alt="衡水体"></p><h3 id="衔接词"><a href="#衔接词" class="headerlink" title="衔接词"></a>衔接词</h3><p>due to 因为<br>in spite of尽管，<br>thus因此，<br>on the contrary相反地<br>首先 in the first place, to begin with, first of all, for one thing<br>然后，而且 in addition, what’s more,moreover, besides, for another thing<br>最后 last but not the least<br>表举例 for instance<br>表对比 in contrast, on the contrary</p><h3 id="专业词汇"><a href="#专业词汇" class="headerlink" title="专业词汇"></a>专业词汇</h3><p><a href="https://www.bilibili.com/video/BV1MN411Z7Lu?spm_id_from=333.788.b_765f64657363.3">热点词</a></p><h3 id="例句"><a href="#例句" class="headerlink" title="例句"></a>例句</h3><ol><li>It is obvious that the cartoon is trying to tell us…</li><li>Currently, there is a growing tendency that people in mounting numbers are showing great enthusiasm for sth.</li><li>From my perspective/As for me, at no time should we ignore the importance of A.</li><li>“<strong><strong><strong>” is the opinion held by</strong></strong></strong> . This remark has been confirmed time and again by more and more people. “______”是______的观点，而且被越来越多的人反复证实。</li><li>The advantages of A are much greater than those of B.</li><li>A number of factors are accountable for this situation. 造成这种情况的因素有很多。</li></ol><h2 id="听力"><a href="#听力" class="headerlink" title="听力"></a>听力</h2><ol><li>利用一切时间，读题，找关键词</li><li>可以去预判一些常考的考点【因果、并列、转折、举例、男女对话即为换题的标志】</li><li>一般事请都会有波折，即往不好的方向发展</li><li>当没听清时，往主旨上蒙，越是详细越可能出错</li></ol><h2 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a>阅读</h2><p><font color="#00CED1">20mins</font></p><ol><li>原句<font color="#FF1493">重复出现，200%错</font>。正确的都是有改动的，即同意替换。</li><li>文章是<font color="#9400D3">按顺序出题</font>的。你要觉得不是，就是你做错了。</li><li>选项中意思完全相反的2个选项，<font color="#E9967A">其中之一是对的</font>。（要有这个意识）。</li><li>就一般而言，some people，表作者不认同的观点。<font color="#006400">few people，表作者的观点</font>。</li><li>用文章里<font color="#9400D3">举例的句子来作为选项</font>，直接排除。200%错。（要有能辨别这个选项是不是文章中例子的能力）。</li><li>某某人说的话，或者是带引号的，一定要高度<font color="#8B008B">重视</font>。尤其是在段落的后半部分。很有可能就是某个问题的同意替换。即题眼。</li><li>有的时候，一句话可以设2个问题。不过这种情况很少出现了，非常少。。</li><li>文章基本以5段为主（也有6段、7段的），要把握每段之间的关系。一般来说，一段一个题，只是一般来说喔。。</li><li>一篇文章总会有5、6+个长难句，且总会在这里设问题。所以，<font color="#FF8C00">长难句必须要拿下</font>！！</li><li><font color="#9932CC">每段的第一句很重要</font>。尤其总分结构的段。有的时候第一句话就是题眼。考研英语，总分结构或者总分总的段落很多。。</li><li>若文章首段以why为开头的，这里若设题的话，选项里有because的，往往就是正确选项。不过这种类型的题，很少见了。。</li><li>有时候每段的第一句话，仅仅是一个表述。而在第2或3句以后，会出现对比或者转折。一般来说，<font color="#00FFFF">转折后面的是作者的态度</font>。你要注意的是，作者对什么进行了转折。那个关键词你要找出来。</li><li>在应该出现答案的地方，没有答案。。接着往下读。答案可能会在下一段的开头部分。因为文章都是接着说的。要有连贯性。这和7选5的技巧有些相似。不过这种情况并不多见。。</li><li>一个长句看不懂，接着往下看，下一句可能是这个长句的解释说明。是的话，这的地方可能会出题。出的话，答案就在这附近。而实际情况是，文章在谈论某个问题或提出某个观点时，有时会再做进一步的解释说明。这种情况下，这里往往会设问题。不过，这种情况很少见了。。</li><li>有些句子仅仅是解释补充，或者是起过渡作用的。这样句子的特点是，句子比较短。注意，答案一般不会在这儿出现。选项中出现，肯定是<font color="#9932CC">干扰项</font>。你要知道的是，同意替换的句子，大都是长难句。一些作为过渡的句子，不可能是答案。在你读不懂的情况下，要有这个判断力。</li><li>正确选项都是原文中的个别几个词的<font color="#D2691E">同义替换</font>。阅读理解历年的所有真题，都是同意替换！！就看你能不能找得到。考研英语，考的就是这个！！那个关键词，就看你找没找得到，不管是什么类型的题。。</li><li>每一个问题，在原文中，都要有一个定位。然后精读，找出那个中心句或者关键词。要抓文章的中心主旨和各段落的大意，阅读理解考的就是这个“<font color="#057748">中心句</font>”。</li><li>选项中的几个单词，是该段中不同句子里的单词拼凑的，有时看上去很舒服，注意，干扰项。还有从不同的段落里的词拼凑到一起的，直接排除。总之，选项的单词是<font color="#A52A2A">拼凑的</font>，肯定错。</li><li>一定要注意文章中句子的宾语部分，尤其是长难句中主干的宾语。上面说了，考研英语大都是长难句里设题。你要知道的是，长难句里，最可能是出题的就是句子的主干部分！主干的主语、宾语是什么，一定要知道。<font color="#FF1493">正确选项</font>的题眼往往就在这儿。当然，还有一些起修饰、限定作用的词，一定要看仔细。小心陷阱。</li><li>若某个问题，是特别长的一个句子，一定要看清问的是什么，别打马虎眼。这是做题时需要留意的地方。</li><li>注意问题的主语是谁，它和原文题眼的主语原则上是一致的。主语不一致，一般来说，都是<font color="#9400D3">错的。</font></li><li>即第6条，某某人说的话，尤其特别长的句子，或者是带引号的。60%以上会出题。题眼就在这儿。这里又提了一遍，就是要引起你的<font color="#057748">重视</font>。</li><li>错误的选项，往往是就文章某一方面而说的，其特点是：所涉及的，仅仅是某一个小问题，或者很具体，<font color="#B22222">非常具体的一件实事</font>。200%错误选项。这是考研英语最经常遇到的干扰项。一定要会识别。</li><li>中国人出的题。多是总-分结构，或者总-分-总。所以每段开头结尾，都要注意。（这里指的是中间没有出现转折的段落）。整篇文章的<font color="#057748">开头结尾</font>。也要重视。</li><li>文章的结构，要么总-分或总-分-总，要么转折、对比，要么举例说明。就这么几个套路。</li><li>对选项中的“重点词”（即主语、宾语、修饰语）都要看清楚。有的时候，选项中，会对原文中本来正确的事做错误的修改，来作为干扰项。你要注意的是，选项句子的主语（与原文）是否一致、宾语是否符合原文意思，或者用一些牵强的修饰词，来做一些特殊的限定。要看清楚。这是干扰项的特点之一。</li><li>某人说过的话，有时并不是题眼，但可以从侧面或某个角度来反映作者的观点，也就是作者想表达的。<font color="#00FFFF">正确答案都是和这样的观点相一致的</font>。要把握关键词，有感情色彩的词。做题时，要有这个意识。</li><li>就某个词或者某个句子设问题，不用猜词。<font color="#057748">就一条，文章主旨</font>！ 不用去研究这个词什么意思，把握主旨即可。全文主旨和段落主旨（前者更重要）。</li><li>接着28条说，不管什么题型，上面说的还是其他别的题型。很绝对的说，反映主旨的肯定对，前提是你能确定它就是主旨。考研英语，一直到2011年，这一条还没变过。所以，文章读不太懂，但能把握作者想表达的意思即可。如2011年争议题37题。</li><li>注意中心句（即题眼）和前后句子之间的关系，是接着说的，还是转折关系。这里出题的话，要把握和<font color="#8A2BE2">前后句子之间的关系</font>。是并列关系的，可以从这些句子里找同义词。是转折关系的，就通过转折关系句子里的关键词的相反意思来判断。前提是在你读不懂的情况下。</li><li>凡是举例的，都是为了说明观点的。那么，这个观点（中心句），一般来说，会在举例之前就表达了。但有时候也在举例之后。总之，<font color="#bf242a">作者举例想说明的这个观点，你一定要找出来</font>。</li><li>排除2个选项以后，选出和文章主旨相关的选项即可。不知道主旨就把握关键词。</li><li>词汇题的正确答案，往往隐藏在原文的该处附近（就是那个<font color="#0000FF">同义替换词</font>），原文这附近的句子，是并列关系或者解释说明句的，就从这些句子的关键词的相近意思去把握。是转折关系的，就从关键词的相反意思去把握。总之，你要找的就是那个关键词。和30条一起理解吧。。</li><li>如果原文中出现“ A is B and C”。若某一问题，选项中出现了B没C，或者只出现C没B。<font color="#A52A2A">肯定错</font>，直接排除。可能你会问了，同时出现B and C 咋办？ 目前还没出现过这种情况。。注意，这里说的B和C，是单词或者短语。。这是干扰项的特点之一。 实际情况是，这个<font color="#B22222">句子不是题眼</font>。</li><li>接34题说，还一种情况是，若B和C是2个长句子，中间用分号隔开的。且这两个句子都是作者想表达的，选项中都出现了。。一般来说，选项中会对其中之一做错误的修改来作为干扰项。而另一个是对的。（如05年TEXT1 ，第一题。不过总体来说，这种题型非常非常少见。偶在这里想说的是34条。这样的干扰项，你要会识别。）</li><li>注意几个词，yet表转折，hardly表否定。while 有时是比较，有时也表转折。比较的时候，注意比较的对象，要弄清楚。转折的时候，你<font color="#0000FF">要知道作者对什么进行了转折</font>。</li><li>如果你对“关键词”比较蒙，或者你想问：我怎么知道哪个是关键词？解释一下，关键词就是句子中主干的宾语。尤其是一些你觉得比较重要的句子。这样的句子多数是长难句。一般来说，一个句子主干的主语，宾语，和其他的修饰部分，都是很重要的！！ 宾语是主语的宾语，所以，和主语是要对上号的，对不上不行。（也就是26条的主语是否一致）。至于修饰的部分，干扰项常常在这里做手脚，比如会有一些特殊的限定，千万要留意，别疏忽了。。</li><li>什么是中心句？即反应文章的主旨和每一段的中心意思的一句话。这句话是客观存在的。也就是作者的观点。中心句即题眼，选出正确答案，看的就是中心句。只有中心句才能选出正确答案。所以，中心句不知道在哪，或者读不懂，很难选出正确答案。中心句的具体位置，见下条。</li><li><font color="#7FFF00">很关键的一条</font>，抓住每段的中心意思，也就是中心句。每段至少一句，最多2句。 一般来说，总分结构的段落，中心句一般在段首。举例段一般在举例前后。转折段，中心句在出现转折的地方，或者后一句（一般来说在该段的第三行上下浮动）。再就是某某人说的话。要注意这句话和前后句的关系，是并列还是转折。然后来把握这句话的意思，把握不了就通过前后句是并列还是转折关系的关键词来把握。</li><li>每个问题，要还原到文章具体的某一段落。若此问题在某段的后半部分，且你没有太看懂，这段已经完事了。。要养成一个习惯。<font color="#1E90FF">接着看一下段的第一句话</font>。实在做不出来的话，就选那个和下一段第一句话的意思差不多的选项。只能这样了。。 （貌似是13条的重复）补充下，这只是小技巧，只起补充作用，有时候用不上。。</li><li>每段的第三行，一般来说，也是该段的第3句话（也可能是第2、4句话）。其特点是：句子很长，由两句或者两句以上组成，是个长难句。尤其是 that mean ，the notion is that 之类的，一定要重视。要把握句子的主干。作者想说的是什么（把握作者强调的是哪个句子）。看清楚哪句话是为了修饰哪句话的。这样的句子，若出题的话，句子的主干就是正确选项。起补充修饰作用的一定要看清楚。。每段最重要的三个地方：<font color="#0000FF">段首，段尾，和这儿</font>。再就是带引号的。中心句一般就在这几个地方。 其实也就这么几个地方。。别的地方一般都是过渡句。。</li><li>若是转折段的话，要注意转折的那个句子，一般都是在<font color="#0000FF">41条</font>的那个地方（即第三行上下浮动）。转折前后都要看，看对比的是什么。在看不懂的情况下，通过前面的，来翻译后面的（<font color="#00FFFF">反向翻译</font>），来找关键词。反之亦然。</li><li>最后一段，主要看段首和段尾。（最后一段是转折段的情况很少）。若是叙事段的话，叙事部分以外的，重点看。叙事部分尽量看懂。<font color="#725e82">非叙事部分非常重要</font>。一般段首若出现答案的话，段尾可能会作干扰项（见54条），但也不是绝对的（有时段首段尾都会有答案的提示）。段尾若出现答案的话，段首可能会很普通。 一定要把握哪一句话是重点，选项中有相近意思的不是片面的叙述，一般就是正确答案。要把握重点的句子提到的被说明对象（句子主干的宾语），也就是作者关注的。</li><li>选项中出现<font color="#7FFF00">ONLY </font>的，目前还没有对的。</li><li>说明原因的，且<font color="#057748">仅仅是说明原因</font>而已。目前没有对的。</li><li>中心句特别长的，2小句组成，选项中这2句都出现了，怎么排除？反映主旨的是对的。就是作者关注的对象！还一选项是对其进行具体的解释说明，或者补充，或是对主旨的一个具体现象的反应，或是对其造成的后果的叙述。这一选项一般会做错误的修改而作为<font color="#A52A2A">干扰项</font>（即使不做错误的修改也一样是干扰项）总之，这样的题，符合<font color="#1E90FF">28、29</font>条的就是对的。符合<font color="#B22222">23</font>条的，就是错的。</li><li>一定要注意，谁是用来修饰谁的。<font color="#B22222">起修饰作用</font>的词或句子，来做选项，一般是错的。<font color="#057748">被修饰的那部分</font>来作选项，一般是对的。</li><li>因果关系的题，很直接、很简单的因果关系，直接排除。间接的因果，反映主旨的，可能是对的。 总之，因果关系的题，把握主旨就可以了。文中提到的直接因果，如具体的事或是什么的。<font color="#8B0000">都是干扰项</font>。</li><li>48的补充，正确选项反应的，往往是实质的，根本的内容。选项反应的若是<font color="#bf242a">很具体</font>的某一表现，一般都是干扰项。</li><li>干扰项有时出现的生词（可能是你不认识的），是与文章主题无关的词，而非同意替换。（这就需要你的基本功了）</li><li>新趋势，有些题要懂文章才能做出来。读不懂很难选出来。而且，长难句明显增多。有时，它会让你崩溃到单词都认识，却不知道文章说的是什么。这时候什么技巧都不好使了。所以，一定要提高基本功。起码你要知道文章大概说的是啥，也就是谁和谁的关系。任何一篇文章的主旨，基本上都可以用“谁和谁的关系”来概括。</li><li>、通过首段或者前两段，来把握信息点。也就是作者想说的，<font color="#008B8B">是谁和谁的关系</font>？</li><li>接着上面说，一篇文章谈的是什么，或者说“谁与谁的关系”，一定要弄懂。这个具体的什么“关系”弄不懂的话，“谁与谁”一定要弄明白。比如，<font color="#00008B">一篇文章说的是A与B之间如何如何。若问题问你A，选项有B的，往往就是正确答案。若问你B，你就可以先把没有A的选项排除</font>。</li><li>最新趋势，最后一段，段尾很明显不是总结，而是以补充为主的句子。注意，这里可能会<font color="#FF1493">以干扰项的形式出现。</font></li><li>如上所说，中心句出现的地方无非就是段首、段中、段尾，或者带引号的句子。但是，这也是干扰项常常出现的地方。所以，你的基本功，对文章理解的程度，是你必须具备的能力。任何一门考试都有技巧，但是想拿理想的分数，光靠技巧是不现实的。</li><li>有的时候，你会遇到出现2到3个否定词的句子。否定再否定，或者否定否定再否定。遇到了，尤其是3重否定的，基本上<font color="#00008B">这里会设题</font>，这句话里的关键词一定要找出来。这个地方是要练的，到时候出现了，别蒙，别犯怵。。</li><li>再补充一条，<font color="#00008B">however 后面的句子一定要重视</font>。比如有一年的其中一篇的3个题，题眼都是however 后面的句子。 所以，这个词一定要敏感。</li></ol><p><strong><font color="#FF8C00">技巧done</font></strong></p><h2 id="段落匹配"><a href="#段落匹配" class="headerlink" title="段落匹配"></a>段落匹配</h2><p><font color="#9400D3">10mins</font></p><ol><li>看选项，勾关键词【能看懂的】<font color="#FF8C00">3~4mins</font></li><li>找对应的两个及以上的关键词</li></ol><p><strong><font color="#FF8C00">技巧done</font></strong></p><h2 id="选词填空"><a href="#选词填空" class="headerlink" title="选词填空"></a>选词填空</h2><p><font color="#9400D3">10mins</font></p><ol><li>先标词性在选词 </li></ol><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><ol><li>替换+尬写</li><li>看一下<a href="https://www.bilibili.com/video/BV1MN411Z7Lu?spm_id_from=333.788.b_765f64657363.3">热点词</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;写作&quot;&gt;&lt;a href=&quot;#写作&quot; class=&quot;headerlink&quot; title=&quot;写作&quot;&gt;&lt;/a&gt;写作&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://gitee.com/merlynr/img-store/raw/master/202161/162255</summary>
      
    
    
    
    <category term="exam" scheme="https://merlynr.github.io/categories/exam/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
    <category term="exam" scheme="https://merlynr.github.io/tags/exam/"/>
    
  </entry>
  
  <entry>
    <title>孤立森林（Isolation Forest）</title>
    <link href="https://merlynr.github.io/2021/05/31/%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97%EF%BC%88Isolation%20Forest%EF%BC%89/"/>
    <id>https://merlynr.github.io/2021/05/31/%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97%EF%BC%88Isolation%20Forest%EF%BC%89/</id>
    <published>2021-05-30T16:00:00.000Z</published>
    <updated>2021-05-30T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622426214147.png" alt="数据蛋糕"></p><p>假设我们用一个随机超平面来切割（split）数据空间（data space）, 切一次可以生成两个子空间（想象拿刀切蛋糕一分为二）。</p><p>之后我们再继续用一个随机超平面来切割每个子空间，循环下去，直到每子空间里面只有一个数据点为止。</p><blockquote><p><font color="#8B008B">满足的条件</font></p><ul><li>数据本身不可再分割</li><li>二叉树达到限定的最大深度</li></ul></blockquote><p>直观上来讲，我们可以发现那些密度很高的簇是可以被切很多次才会停止切割，但是那些密度很低的点很容易很早的就停到一个子空间里了。</p><p><font color="#6495ED">异常检测原理的理解：</font>由于异常值的数量较少且与大部分样本的疏离性，因此，异常值会被更早的孤立出来，也即异常值会距离iTree的根节点更近，而正常值则会距离根节点有更远的距离。</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>孤立森林算法主要针对的是<strong>连续型结构化</strong>数据中的异常点。</p><p><font color="#FF1493">理论前提</font></p><ul><li>异常数据占总样本量的比例很小</li><li>异常点的特征值与正常点的差异很大</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622426925434.png" alt="数据"></p><p>上图中，中心的白色空心点为正常点，即处于高密度群体中。四周的黑色实心点为异常点，散落在高密度区域以外的空间。</p><h3 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h3><p>孤立森林算法是基于 <strong>Ensemble</strong> 的异常检测方法，因此具有<font color="#7FFF00">线性的时间复杂度</font>。且精准度较高，在处理大数据时速度快，所以目前在工业界的应用范围比较广。常见的场景包括：网络安全中的攻击检测、金融交易欺诈检测、疾病侦测、噪声数据过滤（数据清洗）等。</p><blockquote><p><font color="#006400">知识补充</font>集成学习算法 (Ensemble Learning)<br>统机器学习算法 (例如：决策树，人工神经网络，支持向量机，朴素贝叶斯等) 的目标都是寻找一个最优分类器尽可能的将训练数据分开。集成学习 (Ensemble Learning) 算法的基本思想就是将多个分类器<font color="#8FBC8F">组合</font>，从而实现一个预测效果更好的<font color="#8A2BE2">集成分类器</font>。</p></blockquote><blockquote><p><font color="#FF00FF">知识补充：</font><br><a href="https://blog.zuishuailcq.xyz/2021/05/31/%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/">算法的性能指标 | 吾辈之人，自当自强不息！</a></p></blockquote><h2 id="孤立森林的创新点"><a href="#孤立森林的创新点" class="headerlink" title="孤立森林的创新点"></a>孤立森林的创新点</h2><ol><li><font color="#D2691E">Partial models</font>：在训练过程中，每棵孤立树都是随机选取部分样本</li><li><font color="#D2691E">No distance or density measures</font>：不同于 KMeans、DBSCAN 等算法，孤立森林不需要计算有关距离、密度的指标，可大幅度提升速度，减小系统开销</li><li><font color="#D2691E"> Linear time complexity</font>：因为基于 ensemble，所以有线性时间复杂度。通常树的数量越多，算法越稳定</li><li><font color="#D2691E">Handle extremely large data size</font>：由于每棵树都是独立生成的，因此可部署在大规模分布式系统上来加速运算</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;理解&quot;&gt;&lt;a href=&quot;#理解&quot; class=&quot;headerlink&quot; title=&quot;理解&quot;&gt;&lt;/a&gt;理解&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://gitee.com/merlynr/img-store/raw/master/2021531/16224</summary>
      
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="data mining" scheme="https://merlynr.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>拟合</title>
    <link href="https://merlynr.github.io/2021/05/31/%E6%8B%9F%E5%90%88/"/>
    <id>https://merlynr.github.io/2021/05/31/%E6%8B%9F%E5%90%88/</id>
    <published>2021-05-30T16:00:00.000Z</published>
    <updated>2021-06-02T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/72038532#:~:text=%E5%AF%B9%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%88%96%E6%9C%BA%E5%99%A8,%E7%A7%B0%E4%B8%BA%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE%E3%80%82">欠拟合、过拟合及如何防止过拟合 - 知乎</a></p><h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p>对于深度学习或机器学习模型而言，我们不仅要求它对训练数据集有很好的拟合（训练误差），同时也希望它可以对未知数据集（测试集）有很好的拟合结果（泛化能力），所产生的测试误差被称为泛化误差。度量泛化能力的好坏，最直观的表现就是模型的过拟合（overfitting）和欠拟合（underfitting）。过拟合和欠拟合是用于描述模型在训练过程中的两种状态。一般来说，训练过程会是如下所示的一个曲线图。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622461280602.png" alt="训练过程"></p><p>训练刚开始的时候，模型还在学习过程中，处于欠拟合区域。随着训练的进行，训练误差和测试误差都下降。在到达一个临界点之后，训练集的误差下降，测试集的误差上升了，这个时候就进入了过拟合区域——由于训练出来的网络过度拟合了训练集，对训练集以外的数据却不有效。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622461630220.png" alt="拟合"></p><h2 id="欠拟合"><a href="#欠拟合" class="headerlink" title="欠拟合"></a>欠拟合</h2><p><font color="#9932CC">欠拟合</font>是指模型不能在训练集上获得足够低的误差。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622461604985.png" alt="欠拟合"></p><p><strong>如何解决欠拟合？</strong></p><p>欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过<font color="#7FFF00">增加网络复杂度</font>或者在模型中<font color="#7FFF00">增加特征</font>，这些都是很好解决欠拟合的方法。</p><h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><p><font color="#9932CC">过拟合</font>是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，<font color="#ff7500">模型在训练集上表现很好，但在测试集上却表现很差。</font>模型对训练集”死记硬背”（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，泛化能力差。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622461615228.png" alt="过拟合"></p><p><font color="#FF1493">出现原因</font></p><ol><li><strong>训练数据集样本单一，样本不足。</strong> 如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。</li><li><strong>训练数据中噪声干扰过大。</strong> 噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。</li><li><strong>模型过于复杂</strong>。模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。</li></ol><p><strong><font color="#FF8C00">如何防止过拟合</font></strong><br>要想解决过拟合问题，就要显著减少测试误差而不过度增加训练误差，从而提高模型的泛化能力。我们可以使用正则化（Regularization）方法。</p><blockquote><p>正则化是指修改学习算法，使其降低泛化误差而非训练误差。</p></blockquote><h3 id="正则化方法"><a href="#正则化方法" class="headerlink" title="正则化方法"></a>正则化方法</h3><blockquote><p>常用的正则化方法根据具体的使用<font color="#D2691E">策略</font>不同可分为：<br>（1）直接提供正则化约束的参数正则化方法，如L1/L2正则化；<br>（2）通过工程上的技巧来实现更低泛化误差的方法，如提前终止(Early stopping)和Dropout；<br>（3）不直接提供约束的隐式正则化方法，如数据增强等。</p></blockquote><ol><li> 获取和使用更多的数据（数据集增强）——解决过拟合的<font color="#DC143C">根本性</font>方法</li></ol><p>让机器学习或深度学习模型泛化能力更好的办法就是使用更多的数据进行训练。但是，在实践中，我们拥有的数据量是有限的。解决这个问题的一种方法就是<font color="#7FFF00">创建“假数据”并添加到训练集中——数据集增强</font>。通过增加训练集的额外副本来增加训练集的大小，进而改进模型的泛化能力。</p><ol start="2"><li>采用合适的模型（控制模型的复杂度）</li></ol><p>过于复杂的模型会带来过拟合问题。对于模型的设计，目前公认的一个深度学习规律”deeper is better”。国内外各种大牛通过实验和竞赛发现，对于CNN来说，层数越多效果越好，但是也更容易产生过拟合，并且计算所耗费的时间也越长。</p><p>根据<font color="#E9967A">奥卡姆剃刀</font>法则：在同样能够解释已知观测现象的假设中，我们应该挑选“最简单”的那一个。对于模型的设计而言，我们应该选择简单、合适的模型解决复杂的问题。</p><ol start="3"><li>降低特征的数量</li></ol><p>对于一些特征工程而言，可以降低特征的数量——<font color="#006400">删除冗余特征</font>，人工选择保留哪些特征。这种方法也可以解决过拟合问题。</p><ol start="4"><li>L1 / L2 正则化</li></ol><p><a href="https://www.cnblogs.com/zingp/p/10375691.html#_label0">深入理解L1、L2正则化 - ZingpLiu - 博客园</a></p><ul><li>L1正则化</li></ul><p>L1正则化可以使得参数稀疏化，即得到的参数是一个稀疏矩阵，可以用于特征选择。</p><pre><code>    稀疏性，说白了就是模型的很多参数是0。通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，很多参数是0，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，即使去掉对模型也没有什么影响，此时我们就可以只</code></pre><p>在原始的损失函数后面加上一个L1正则化项，即<strong>全部权重 $w$ 的绝对值的和，再乘以λ/n</strong>。则损失函数变为：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mfrac><mi>&#x3BB;</mi><mi>n</mi></mfrac><munder><mo>&#x2211;</mo><mi>i</mi></munder><mfenced close="|" open="|"><msub><mi>w</mi><mi>i</mi></msub></mfenced></math></p><p>对应的梯度（导数）：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mo>&#x2202;</mo><mi>C</mi></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>&#x2202;</mo><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac><mo>+</mo><mfrac><mi>&#x3BB;</mi><mi>n</mi></mfrac><mo>sgn</mo><mo>(</mo><mi>w</mi><mo>)</mo></math></p><p>其中 <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>sgn</mo><mo>(</mo><mi>w</mi><mo>)</mo></math> 只是简单地取 $w1$ 各个元素地正负号。</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>sgn</mo><mo>(</mo><mi>w</mi><mo>)</mo><mo>=</mo><mfenced close="" open="{"><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>,</mo><mi>w</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>,</mo><mi>w</mi><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mo>-</mo><mn>1</mn><mo>,</mo><mi>w</mi><mo>&lt;</mo><mn>0</mn></mtd></mtr></mtable></mfenced></math></p><p>梯度下降时权重 $w$ 更新变为：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi><mo>&#x2192;</mo><msup><mi>w</mi><mo>‘</mo></msup><mo>=</mo><mi>w</mi><mo>-</mo><mfrac><mrow><mi>&#x3B7;</mi><mi>&#x3BB;</mi></mrow><mi>n</mi></mfrac><mo>sgn</mo><mo>(</mo><mi>w</mi><mo>)</mo><mo>-</mo><mi>&#x3B7;</mi><mfrac><mrow><mo>&#x2202;</mo><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac></math></p><p>当 $w=0$ 时，|w|是不可导的。所以我们仅仅能依照原始的未经正则化的方法去更新 $w$  。<br>当 $w&gt;0$  时，sgn( $w$  )&gt;0, 则梯度下降时更新后的 $w$  变小。<br>当 $w&lt;0$  时，sgn( $w$  )&gt;0, 则梯度下降时更新后的 $w$  变大。换句换说，L1正则化使得权重 $w$ 往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。</p><p>这也就是<font color="#6495ED">L1正则化会产生更稀疏（sparse）的解</font>的原因。此处稀疏性指的是最优值中的一些参数为0。<font color="#1E90FF">L1正则化的稀疏性质已经被广泛地应用于特征选择</font>机制，从可用的特征子集中选择出有意义的特征。</p><ul><li>L2 正则化</li></ul><p>L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合。</p><p>拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是抗扰动能力强。</p><p>L2正则化通常被称为<strong>权重衰减</strong>（weight decay），就是在原始的损失函数后面再加上一个L2正则化项，即<strong>全部权重</strong> $w$  的平方和，再乘以λ/2n。则损失函数变为：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mfrac><mi>&#x3BB;</mi><mrow><mn>2</mn><mi>n</mi></mrow></mfrac><mo>&#xB7;</mo><mo>&#x2211;</mo><msubsup><mi>w</mi><mi>i</mi><mn>2</mn></msubsup></math></p><p>对应的梯度（导数）：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mo>&#x2202;</mo><mi>C</mi></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>&#x2202;</mo><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac><mo>+</mo><mfrac><mi>&#x3BB;</mi><mi>n</mi></mfrac><mi>w</mi></math></p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mo>&#x2202;</mo><mi>C</mi></mrow><mrow><mo>&#x2202;</mo><mi>b</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>&#x2202;</mo><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mo>&#x2202;</mo><mi>b</mi></mrow></mfrac></math></p><p>能够发现L2正则化项对偏置 b 的更新没有影响，可是对于权重 $w$  的更新有影响：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi><mo>&#x2192;</mo><mi>w</mi><mo>-</mo><mi>&#x3B7;</mi><mfrac><mrow><mo>&#x2202;</mo><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac><mo>-</mo><mfrac><mrow><mi>&#x3B7;</mi><mi>&#x3BB;</mi></mrow><mi>n</mi></mfrac><mi>w</mi></math><br><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>=</mo><mfenced><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mi>&#x3B7;</mi><mi>&#x3BB;</mi></mrow><mi>n</mi></mfrac></mrow></mfenced><mi>w</mi><mo>-</mo><mi>&#x3B7;</mi><mfrac><mrow><mo>&#x2202;</mo><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac></math><br>这里的<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3B7;</mi></math>、 $n$ 、<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3BB;</mi></math>都是大于0的， 所以 <math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mo>-</mo><mfrac><mrow><mi>&#x3B7;</mi><mi>&#x3BB;</mi></mrow><mi>n</mi></mfrac></math>小于1。因此在梯度下降过程中，权重 $w$ 将逐渐减小，趋向于0但不等于0。这也就是<strong>权重衰减</strong>（weight decay）的由来。</p><p>L2正则化起到使得权重参数 $w$ 变小的效果，为什么能防止过拟合呢？因为更小的权重参数  意味着模型的复杂度更低，对训练数据的拟合刚刚好，不会过分拟合训练数据，从而提高模型的泛化能力。</p><ol start="5"><li>Dropout</li></ol><p>  Dropout是在训练网络时用的一种技巧（trike），相当于在隐藏单元增加了噪声。<strong>Dropout 指的是在训练过程中每次按一定的概率（比如50%）随机地“删除”一部分隐藏单元（神经元）</strong>。所谓的“删除”不是真正意义上的删除，其实就是将该部分神经元的激活函数设为0（激活函数的输出为0），让这些神经元不计算而已。</p><p>  <img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622466578527.png" alt="Dropout"></p><p><font color="#006400">  <strong>Dropout为什么有助于防止过拟合呢？</strong></font></p><p>（a）在训练过程中会产生不同的训练模型，不同的训练模型也会产生不同的的计算结果。随着训练的不断进行，计算结果会在一个范围内波动，但是均值却不会有很大变化，因此可以把最终的训练结果看作是不同模型的平均输出。<br>（b）它消除或者减弱了神经元节点间的联合，降低了网络对单个神经元的依赖，从而增强了泛化能力。</p><blockquote><p><font color="#00008B">理解</font><br>通过加入噪声，在训练模型时，扩展模型的接受范围，避免过拟合</p></blockquote><ol start="6"><li>Early stopping（提前终止）</li></ol><p>对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）。Early stopping是一种迭代次数截断的方法来防止过拟合的方法，<font color="#7FFF00">即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合</font>。</p><p>为了获得性能良好的神经网络，训练过程中可能会经过很多次epoch（遍历整个数据集的次数，一次为一个epoch）。如果epoch数量太少，网络有可能发生欠拟合；如果epoch数量太多，则有可能发生过拟合。Early stopping旨在解决epoch数量需要手动设置的问题。具体做法：每个epoch（或每N个epoch）结束后，在验证集上获取测试结果，随着epoch的增加，如果在验证集上发现测试误差上升，则停止训练，将停止之后的权重作为网络的最终参数。</p><p><font color="#E9967A">为什么能防止过拟合？</font></p><p>当还未在神经网络运行太多迭代过程的时候，w参数[误差]接近于0，因为随机初始化w值的时候，它的值是较小的随机值。当你开始迭代过程，w的值会变得越来越大。到后面时，w的值已经变得十分大了。所以early stopping要做的就是在中间点停止迭代过程。我们将会得到一个中等大小的w参数，会得到与L2正则化相似的结果，选择了w参数较小的神经网络。</p><p><font color="#A52A2A">Early Stopping缺点</font><br><strong>没有采取不同的方式来解决优化损失函数和过拟合这两个问题</strong>，而是用一种方法同时解决两个问题 ，结果就是要考虑的东西变得更复杂。之所以不能独立地处理，因为如果你停止了优化损失函数，你可能会发现损失函数的值不够小，同时你又不希望过拟合。</p>]]></content>
    
    
    <summary type="html">机器学习</summary>
    
    
    
    <category term="machine learning" scheme="https://merlynr.github.io/categories/machine-learning/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="data mining" scheme="https://merlynr.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>算法的性能指标</title>
    <link href="https://merlynr.github.io/2021/05/31/%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/"/>
    <id>https://merlynr.github.io/2021/05/31/%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/</id>
    <published>2021-05-30T16:00:00.000Z</published>
    <updated>2021-05-30T16:00:00.000Z</updated>
    
    
    <summary type="html">=</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>二分类之类别不平衡</title>
    <link href="https://merlynr.github.io/2021/05/31/%E4%BA%8C%E5%88%86%E7%B1%BB%E4%B9%8B%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1/"/>
    <id>https://merlynr.github.io/2021/05/31/%E4%BA%8C%E5%88%86%E7%B1%BB%E4%B9%8B%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1/</id>
    <published>2021-05-30T16:00:00.000Z</published>
    <updated>2021-06-08T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/massquantity/p/8550875.html">机器学习之类别不平衡问题 (1) —— 各种评估指标 - massquantity - 博客园</a><br><a href="https://www.zhihu.com/question/269698662">欠采样（undersampling）和过采样（oversampling）会对模型带来怎样的影响？</a><br><a href="https://www.cnblogs.com/inchbyinch/p/12642760.html">详解类别不平衡问题 - 天地辽阔 - 博客园</a></p><h2 id="类别不平衡-class-imbalance"><a href="#类别不平衡-class-imbalance" class="headerlink" title="类别不平衡(class-imbalance)"></a>类别不平衡(class-imbalance)</h2><blockquote><p><font color="#D2691E"> 惯例</font><br>在二分类问题中，一般将数目少的类别视为正例，数目多的类别视为负例</p></blockquote><p><font color="#228B22">也叫数据倾斜，数据不平衡指分类任务中不同类别的训练样例数目差别很大的情况。</font></p><h2 id="各种评估指标"><a href="#各种评估指标" class="headerlink" title="各种评估指标"></a>各种评估指标</h2><p><a href="https://www.cnblogs.com/massquantity/p/8550875.html">机器学习之类别不平衡问题 (1) —— 各种评估指标 - massquantity - 博客园</a></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202167/1623024781731.png" alt="混淆矩阵图"></p><ul><li>True Positive(真正例，TP)：实例为正例，预测为正例</li><li>False Negative (假负例，FN)：实际为正例，预测为负例。</li><li>True Negative (真负例，TN)：实际为负例，预测为负例。</li><li>False Positive (假正例，FP)：实际为负例，预测为正例。</li></ul><ol><li>Precision (查准率) = <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math> ，Precision衡量的是<font color="#7FFF00">所有被预测为正例的样本中有多少是真正例</font>。<font color="#A52A2A">但Precision并没有表现有多少正例是被错判为了负例(即FN)</font>，举个极端的例子，分类器只将一个样本判为正例，其他所有都判为负例，这种情况下Precision为100%，但其实遗漏了很多正例，所以Precision常和下面的Recall (TPR) 相结合。</li><li>True Positive Rate (TPR，真正例率) = <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math> ，又称__Recall__(查全率,召回率)，Sensitivity(灵敏性)。Recall (TPR)衡量的是所有的正例中有多少是被<font color="#008B8B">正确分类</font>了，也可以看作是为了<font color="#057748">避免假负例(FN)的发生</font>，<font color="#0000FF">即将真正例分类到真正中而不是通过假负来判断的</font>，因为TPR高意味着FN低。Recall的问题和Precision正相反，没有表现出有多少负例被错判为正例(即FP)，若将所有样本全划为正例，则Recall为100%，但这样也没多大用。</li><li>True Negative Rate (TNR，真负例率) = <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math> ，又称Specificity(特异性)。Specificity衡量的是<font color="#006400">所有的负例中有多少是被正确分类</font>了，由于<font color="#1E90FF">类别不平衡问题中通常关注正例能否正确被识别，Specificity高则FP低，意味着很少将正例错判为负例，即该分类器对正例的判别具有“特异性”，在预测为正例的样本中很少有负例混入</font>。</li><li>False Positive Rate (FPR，假正例率) = <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math> = 1− $TNR$ , 由混淆矩阵可以看出该指标的<font color="#D2691E">着眼点</font>在于负例，意为有多少负例被错判成了正例。在ROC曲线中分别以TPR和FPR作为纵、横轴作图，显示出一种正例与负例之间的“<font color="#9400D3">博弈</font>”，在下篇文章中详解。</li></ol><p>F1 score = <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mn>2</mn><mrow><mfrac><mn>1</mn><mtext>&#xA0;recall&#xA0;</mtext></mfrac><mo>+</mo><mfrac><mn>1</mn><mtext>&#xA0;precision&#xA0;</mtext></mfrac></mrow></mfrac><mo>=</mo><mfrac><mrow><mn>2</mn><mo>&#xD7;</mo><mtext>&#xA0;precision&#xA0;</mtext><mo>&#xD7;</mo><mtext>&#xA0;recall&#xA0;</mtext></mrow><mrow><mtext>&#xA0;precision&#xA0;</mtext><mo>+</mo><mtext>&#xA0;recall&#xA0;</mtext></mrow></mfrac></math></p><p>F1分数（F1-Score），又称为平衡F分数（BalancedScore），是一个综合指标,它被定义为精确率和召回率的调和平均数 (harmonic mean),数值上一般接近于二者中的<font color="#1E90FF">较小值</font>，因此如果F1 score比较高的话，意味着Precision和Recall都较高。</p><blockquote><p><font color="#7FFF00"> 知识补充</font><br>调和平均数（harmonic mean）又称倒数平均数，是总体各统计变量倒数的算术平均数的倒数。调和平均数是平均数的一种。<br>算数平均数中，重要性取决于绝对值大的一方（强），而在调和平均数中，<font color="#057748">重要性</font>取决于<font color="#8B0000">绝对值小的一方</font>（弱）。</p></blockquote><p>FP和FN还有个还有个与之相关的概念，那就是统计假设检验中的<font color="#483D8B">第一类错误</font> (Type I error)和<font color="#483D8B">第二类错误 (Type II error)</font> 。由于我们比较关心正例，所以将负例视为零假设，正例视为备选假设，则第一类错误为错误地拒绝零假设 (负例)，选择备选假设，则为FP；第二类错误为错误地接受零假设，则为FN。</p><blockquote><p><font color="#006400">知识补充</font><br>零假设的内容一般是希望证明其错误的假设。</p></blockquote><hr><p>上面介绍的这些指标都没有考虑检索结果的先后顺序，而像搜索问题中我们通常希望第一个结果是与查询最相关的，第二个则是次相关的，以此类推，因而有时候不仅要预测准确，<font color="#6495ED">对于相关性的顺序也非常看重</font>。所以最后介绍两个广泛应用的<font color="#9400D3">排序指标</font>。</p><p>Mean Average Precision (MAP，平均准确率均值)，对于<font color="#B8860B">单个</font>信息需求，返回结果中在每篇相关文档上 Precision 的平均值被称为 Average Precision (AP)，然后对<font color="#D2691E">所有</font>查询取平均得到 MAP。<br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>AP</mi><mo>=</mo><mfrac><mrow><msubsup><mo>&#x2211;</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi>P</mi><mo>(</mo><mi>k</mi><mo>)</mo><mo>&#xD7;</mo><mo>rel</mo><mo>(</mo><mi>k</mi><mo>)</mo></mrow><mi>M</mi></mfrac></math><br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>MAP</mi><mo>=</mo><munderover><mo>&#x2211;</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mfrac><msub><mi>AP</mi><mi>q</mi></msub><mi>Q</mi></mfrac></math><br>其中 P(k) 为前 k 个结果的 Precision，又可写为P@k。 rel(k) 表示第 k 个结果是否为相关文档，相关为1不相关为0，M 表示所有相关文档的数量，n 表示所有文档数量。如果只关心<font color="#00008B">前 K 个查询的情况</font>，则是下式：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>AP</mi><mo>@</mo><mi>K</mi><mo>=</mo><mfrac><mrow><msubsup><mo>&#x2211;</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mi>P</mi><mo>(</mo><mi>k</mi><mo>)</mo><mo>&#xD7;</mo><mo>rel</mo><mo>(</mo><mi>k</mi><mo>)</mo></mrow><msub><mi>M</mi><mi>K</mi></msub></mfrac></math><br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>MAP</mi><mo>@</mo><mi>K</mi><mo>=</mo><munderover><mo>&#x2211;</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mfrac><mrow><msub><mi>AP</mi><mi>q</mi></msub><mo>@</mo><mi>K</mi></mrow><mi>Q</mi></mfrac></math><br>这里的 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>M</mi><mi>K</mi></msub></math> 为前 K 个结果中相关文档的数量。</p><p>对于单个信息需求来说，Average Precision 是<font color="#bf242a"> PR 曲线</font>下面积的近似值，因此 MAP 可粗略地认为是某个查询集合对应的多条 PR 曲线下面积的平均值。</p><p><strong>Normalized Discounted Cumulative Gain</strong> (NDCG，归一化折扣累计增益) 。如果说 <font color="#0000FF">MAP 是基于 0/1 二值描述相关性</font>，那么 <font color="#9932CC">NDCG 则是可将相关性分为多个等级的指标</font>。<br>对于信息检索和推荐之类的问题，每一个返回的结果都被赋予一个相关性分数 rel，则 NDCG 中的 CG 表示前 k 个结果的分数之和，即累计增益 ：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>CG</mi><mi>k</mi></msub><mo>=</mo><munderover><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>rel</mi><mi>i</mi></msub></math></p><p>CG 没有考虑推荐的次序，所以在此基础上引入对结果顺序的考虑，即<font color="#DC143C">相关性高的结果</font>若排在后面则会受更多的惩罚，于是就有了 DCG (discounted CG)，折扣累积增益。公式如下：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>DCG</mi><mi>k</mi></msub><mo>=</mo><munderover><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mfrac><mrow><msup><mn>2</mn><msub><mi>rel</mi><mi>i</mi></msub></msup><mo>-</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></mfrac></math></p><p>i 表示一个结果在结果集中的顺序，如果该结果 rel 很高，但排在后面，意味着分母 log2(i+1) 会变大，则相应的总体 DCG 会变小 (注意这里的 log 是以 2 为底的)。</p><p>对于不同的查询，往往会返回不同的结果集，而不同结果集之间因为大小不同难以直接用 DCG 进行比较，所以需要进行<font color="#006400">归一化</font>，这其实和机器学习中不同特征因量纲不同要进行归一化差不多意思。这个归一化后的指标就是 NDCG ：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>NDCG</mi><mi>k</mi></msub><mo>=</mo><mfrac><msub><mi>DCG</mi><mi>k</mi></msub><msub><mi>IDCG</mi><mi>k</mi></msub></mfrac></math><br>其中 IDCG 表示 Ideal DCG， 指<font color="#006400">某个查询所能返回的最好结果集</font>，IDCG 的值也是结果集中最大的。将所有结果按相关性大小排序，计算出的 DCG 即为前 k 个结果的 IDCG：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>IDCG</mi><mi>k</mi></msub><mo>=</mo><munderover><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo>|</mo><mi>R</mi><mi>E</mi><mi>L</mi><mo>|</mo></mrow></munderover><mfrac><mrow><msup><mn>2</mn><msub><mi>rel</mi><mi>i</mi></msub></msup><mo>-</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></mfrac></math><br>其中 |REL| 表示按相关性顺序排列的结果集。因此 DCG 的值介于 (0, IDCG] ，故 NDCG 的值介于(0,1]，这样就起到了归一化的效果。不同查询或用户的 NDCG 平均起来可以用以评估一个搜索引擎或推荐系统的整体效果。</p><p>NDCG 的缺点是<font color="#483D8B">需要预先指定每一个返回结果的相关性</font>，这个超参数需要人为指定。</p><h2 id="常用的评估方法"><a href="#常用的评估方法" class="headerlink" title="常用的评估方法"></a>常用的评估方法</h2><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>ROC曲线常用于二分类问题中的模型比较，主要表现为一种<font color="#0000FF">真正例率 (TPR) </font>和<font color="#0000FF">假正例率 (FPR) </font>的权衡。</p><p><strong><font color="#ff7500">概述：</font></strong> 是在不同的分类阈值 (threshold) 设定下分别以TPR和FPR为纵、横轴作图。由ROC曲线的两个指标，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mi>P</mi></mfrac><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mi>N</mi></mfrac><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow></mfrac></math> 可以看出，当一个样本被分类器判为正例，若其本身是正例，则TPR增加；若其本身是负例，则FPR增加，因此ROC曲线可以看作是随着阈值的不断移动，所有样本中正例与负例之间的“对抗”。曲线越靠近左上角，意味着<font color="#FF1493">越多的正例优先于负例，模型的整体表现也就越好</font>。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202167/1623072321656.png" alt="ROC曲线"></p><p> <strong><font color="#008B8B">AUC (Area Under the Curve)</font></strong></p><p> <img src="https://gitee.com/merlynr/img-store/raw/master/202167/1623073223653.png" alt="ROC space"></p><p>先看一下ROC曲线中的随机线，图中[0,0]到[1,1]的虚线即为随机线，该线上所有的点都<font color="#00FFFF">表示该阈值下TPR=FPR</font><br>根据定义，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mi>P</mi></mfrac></math>，表示所有正例中被预测为正例的概率；<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mi>N</mi></mfrac></math>，表示所有负例中被被预测为正例的概率。<font color="#B8860B">若二者相等，意味着无论一个样本本身是正例还是负例，分类器预测其为正例的概率是一样的，这等同于随机猜测</font>（注意这里的“随机”不是像抛硬币那样50%正面50%反面的那种随机）。</p><p>上图中B点就是一个随机点，无论是样本数量和类别如何变化，始终将75%的样本分为正例。</p><p><font color="#B8860B">ROC曲线围成的面积 (即AUC)可以解读为</font>：从所有正例中随机选取一个样本A，再从所有负例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。可以看到位于随机线上方的点(如图中的A点)被认为好于随机猜测。在这样的点上TPR总大于FPR，意为正例被判为正例的概率大于负例被判为正例的概率。<br>从另一个角度看，由于画ROC曲线时都是先将所有样本按分类器的<font color="#1E90FF">预测概率</font>排序，<font color="#B22222">所以AUC反映的是分类器对样本的排序能力</font>，依照上面的例子就是A排在B前面的概率。<font color="#008B8B">AUC越大，自然排序能力越好</font>，即分类器将越多的正例排在负例之前。</p><p><font color="#8B0000">ROC曲线的绘制方法</font>：假设有P个正例，N个反例，首先拿到分类器对于每个样本预测为正例的概率，根据概率对所有样本进行<font color="#006400">逆序排列</font>，然后将<font color="#0000FF">分类阈值设为最大</font>，即把所有样本均预测为反例，此时图上的点为 (0,0)。然后将分类阈值依次设为每个样本的预测概率，即依次将每个样本划分为正例，如果该样本为真正例，则TP+1，即<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mi>P</mi><mi>R</mi><mo>+</mo><mfrac><mn>1</mn><mi>P</mi></mfrac></math>; 如果该样本为负例，则FP+1，即<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mi>P</mi><mi>R</mi><mo>+</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></math>。最后的到所有样本点的TPR和FPR值，用线段相连。</p><blockquote><p><a href="https://github.com/massquantity/Class-Imbalance/tree/master/">massquantity/Class-Imbalance: 《机器学习之类别不平衡问题》文章代码</a></p></blockquote><h4 id="ROC的优点"><a href="#ROC的优点" class="headerlink" title="ROC的优点"></a>ROC的优点</h4><p><img src="https://gitee.com/merlynr/img-store/raw/master/202168/1623131643148.png" alt="混淆矩阵图"></p><ol><li><p>兼顾正例和负例的权衡。因为TPR聚焦于正例，FPR聚焦于与负例，使其成为一个比较均衡的评估方法。</p></li><li><p>ROC曲线选用的两个指标，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mi>P</mi></mfrac><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mi>N</mi></mfrac><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow></mfrac></math>，都不依赖于具体的类别分布。</p><p> 注意TPR用到的TP和FN同属<font color="#FF1493">P</font>列，FPR用到的FP和TN同属<font color="#1E90FF">N</font>列，<font color="#7FFF00">所以即使P或N的整体数量发生了改变，也不会影响到另一列</font>。也就是说，即使正例与负例的比例发生了很大变化，ROC曲线也不会产生大的变化，而像Precision使用的TP和FP就分属两列，则易受类别分布改变的影响。</p></li></ol><p><a href="https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf">参考文献</a>中举了个例子，负例增加了10倍，ROC曲线没有改变，而PR曲线则变了很多。作者认为这是ROC曲线的优点，即具有<font color="#0000FF">鲁棒性</font>，在类别分布发生明显改变的情况下依然能客观地识别出较好的分类器。</p><blockquote><p><font color="#006400">代码验证</font><br><a href="https://www.cnblogs.com/massquantity/p/8592091.html">相关资料</a></p></blockquote><h4 id="ROC的缺点"><a href="#ROC的缺点" class="headerlink" title="ROC的缺点"></a>ROC的缺点</h4><ol><li>上文提到ROC曲线的优点是不会随着类别分布的改变而改变，但这在某种程度上也是其缺点。因为负例N增加了很多，而曲线却没变，这等于产生了大量FP。像信息检索中如果主要关心正例的预测准确性的话，这就不可接受了。</li><li>在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分乐观的效果估计。ROC曲线的横轴采用FPR，根据<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>FPR</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mi>N</mi></mfrac><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow></mfrac></math>，当负例N的数量远超正例P时，FP的大幅增长只能换来FPR的微小改变。<font color="#B22222">结果是虽然大量负例被错判成正例，在ROC曲线上却无法直观地看出来。</font>（当然也可以只分析ROC曲线左边一小段）<br>举个例子，假设一个数据集有正例20，负例10000，开始时有20个负例被错判，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mn>20</mn><mrow><mn>20</mn><mo>+</mo><mn>9980</mn></mrow></mfrac><mo>=</mo><mn>0</mn><mo>.</mo><mn>002</mn></math>，接着又有20个负例错判，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mi>P</mi><msub><mi>R</mi><mn>2</mn></msub><mo>=</mo><mfrac><mn>40</mn><mrow><mn>40</mn><mo>+</mo><mn>9960</mn></mrow></mfrac><mo>=</mo><mn>0</mn><mo>.</mo><mn>004</mn></math>，在ROC曲线上这个变化是很细微的。而与此同时Precision则从原来的0.5下降到了0.33，在PR曲线上将会是一个大幅下降。</li></ol><h3 id="PR-Precision-Recall-曲线"><a href="#PR-Precision-Recall-曲线" class="headerlink" title="PR(Precision Recall)曲线"></a>PR(Precision Recall)曲线</h3><p>PR曲线展示的是Precision vs Recall的曲线，PR曲线与ROC曲线的相同点是都采用了TPR (Recall)，都可以用AUC来衡量分类器的效果。不同点是ROC曲线使用了FPR，而PR曲线使用了Precision，因此<font color="#8A2BE2">PR曲线的两个指标都聚焦于正例</font>。<font color="#8A2BE2">类别不平衡问题中由于主要关心正例</font>，所以在此情况下PR曲线被广泛认为<font color="#FF8C00">优于</font>ROC曲线。</p><p>PR曲线的绘制与ROC曲线类似，PR曲线的AUC面积计算公式为：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><munder><mo>&#x2211;</mo><mi>n</mi></munder><mo>(</mo><msub><mi>R</mi><mi>n</mi></msub><mo>-</mo><msub><mi>R</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>)</mo><msub><mi>P</mi><mi>n</mi></msub></math></p><blockquote><p><a href="https://github.com/massquantity/Class-Imbalance/tree/master/">massquantity/Class-Imbalance: 《机器学习之类别不平衡问题》文章代码</a></p></blockquote><p><strong><font color="#FF00FF">使用场景</font></strong></p><ol><li>ROC曲线由于<font color="#1E90FF">兼顾</font>正例与负例，所以适用于评估分类器的<font color="#B22222">整体性</font>能，相比而言PR曲线完全聚焦于<font color="#FF00FF">正例</font>。</li><li>如果有多份数据且存在不同的类别分布，比如信用卡欺诈问题中每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合，因为<font color="#1E90FF">类别分布改变</font>可能使得PR曲线发生变化时好时坏，这种时候难以进行模型比较；反之，如果想<font color="#FF00FF">测试不同类别分布下对分类器的性能</font>的影响，则PR曲线比较适合。</li><li>如果想要评估在相同的类别分布下正例的预测情况，则宜选PR曲线。</li><li>类别不平衡问题中，ROC曲线通常会给出一个乐观的效果估计，所以大部分时候还是PR曲线更好。</li><li>最后可以根据具体的应用，在曲线上找到最优的点，得到相对应的precision，recall，f1 score等指标，去调整模型的阈值，从而得到一个符合具体应用的模型。</li></ol><h2 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h2><blockquote><p><font color="#FF1493">前提：</font>章节二三主要谈的是类别不平衡的评估指标，因此我们可以选择选择具体的类别不平衡问题的方法。</p></blockquote><p>采样方法大致可分为<font color="#00CED1">过采样 (oversampling)</font> 和<font color="#2F4F4F">欠采样 (undersampling) </font>，虽然过采样和降采样主题思想简单，但这些年来研究出了很多变种，本篇挑一些来具体阐述。见下思维导图：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202168/1623137294136.png" alt="采样方法"></p><h3 id="过采样"><a href="#过采样" class="headerlink" title="过采样"></a>过采样</h3><ol><li>随机过采样</li></ol><p>随机过采样顾名思义就是从样本少的类别中随机抽样，再将抽样得来的样本添加到数据集中。然而这种方法如今已经不大使用了，因为重复采样往往会导致<font color="#1E90FF">严重的过拟合</font>，因而现在的主流过采样方法是通过某种方式人工合成一些少数类样本，从而达到类别平衡的目的，而这其中的鼻祖就是SMOTE。</p><ol start="2"><li>SMOTE</li></ol><p>SMOTE (synthetic minority oversampling technique) 的思想概括起来就是在<font color="#00FFFF">少数类</font>样本之间进行插值来产生额外的样本。具体地，对于一个少数类样本<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">X</mi><mi>i</mi></msub></math>使用K近邻法(k值需要提前指定)，求出离<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">X</mi><mi>i</mi></msub></math>距离最近的k个少数类样本，其中距离定义为样本之间n维特征空间的欧氏距离。然后从k个近邻点中随机选取一个，使用下列公式生成新样本：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">x</mi><mtext>new&#xA0;</mtext></msub><mo>=</mo><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub><mo>+</mo><mfenced><mrow><msub><mover><mi mathvariant="bold">x</mi><mo>^</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub></mrow></mfenced><mo>&#xD7;</mo><mi>&#x3B4;</mi></math><br>其中 <math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi mathvariant="bold">X</mi><mo>^</mo></mover></math> 为选出的k近邻点，δ∈[0,1]是一个随机数。下图就是一个SMOTE生成样本的例子，使用的是3-近邻，可以看出SMOTE生成的样本一般就在<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub></math>和<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mover><mi mathvariant="bold">x</mi><mo>^</mo></mover><mi>i</mi></msub></math>相连的直线上：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623220386942.png" alt="SMOTE生成的样本"></p><p>SMOTE会随机选取少数类样本用以合成新样本，而不考虑周边样本的情况，这样容易带来两个<font color="#FF1493">问题</font>：</p><ol><li>如果选取的少数类样本周围也都是少数类样本，则新合成的样本不会提供太多有用信息。这就像支持向量机中远离margin的点对决策边界影响不大。</li><li>如果选取的少数类样本周围都是多数类样本，这类的样本可能是噪音，则新合成的样本会与周围的多数类样本产生大部分重叠，致使分类困难。</li></ol><p>总的来说我们希望新合成的少数类样本能处于两个类别的边界附近，这样往往能提供足够的信息用以分类。而这就是下面的 <strong>Border-line SMOTE</strong> 算法要做的事情。</p><blockquote><p><font color="#bf242a">知识补充</font><a href="https://blog.csdn.net/lemonaha/article/details/53410465#31-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95">k近邻法–统计学习方法总结_lemonaha的博客-CSDN博客</a><br> k近邻法（k-nearest neighbor,<font color="#0000FF"> k-NN</font>）是一种基本分类与回归方法。这里只讨论分类问题中的k近邻法。k近邻法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方法进行预测。因此，k近邻法不具有显式的学习过程。k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。**<font color="#1E90FF">k值的选择、距离度量及分类决策规则是k近邻法的三个基本要素</font>**。</p></blockquote><ol start="3"><li>Border-line SMOTE</li></ol><p>这个算法会先将所有的少数类样本分成三类，如下图所示：</p><ul><li>“noise” ： 所有的k近邻个样本都属于多数类</li><li>“danger” ： 超过一半的k近邻样本属于<font color="#0000FF">多</font>数类</li><li>“safe”： 超过一半的k近邻样本属于<font color="#0000FF">少</font>数类</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623220611894.png" alt="Border-line SMOTE"></p><p>  <font color="#ff7500">Border-line SMOTE</font>算法只会从处于”<em>danger</em>“状态的样本中随机选择，然后用SMOTE算法产生新的样本。处于”danger“状态的样本代表靠近”边界“附近的少数类样本，而处于边界附近的样本往往更<font color="#B8860B">容易被误分类</font>。因而 Border-line SMOTE 只对那些靠近”边界“的少数类样本进行人工合成样本，而 SMOTE 则对所有少数类样本一视同仁。</p><p>Border-line SMOTE 分为两种: Borderline-1 SMOTE 和 Borderline-2 SMOTE。 Borderline-1 SMOTE 在合成样本时,是式中的<math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi mathvariant="bold">x</mi><mo>^</mo></mover></math>是一个<font color="#1E90FF">少数类样本</font>，而 Borderline-2 SMOTE 中的<math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi mathvariant="bold">x</mi><mo>^</mo></mover></math>则是k近邻中的<font color="#FF1493">任意</font>一个样本。</p><ol start="4"><li>ADASYN</li></ol><p><font color="#8B008B">ADASYN</font>名为自适应合成抽样(adaptive synthetic sampling)，其最大的特点是<font color="#006400">采用某种机制自动决定每个少数类样本需要产生多少合成样本</font>，而不是像SMOTE那样对每个少数类样本合成同数量的样本。具体流程如下：</p><ol><li><p>首先计算需要合成的样本总量：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi><mo>=</mo><mfenced><mrow><msub><mi>S</mi><mrow><mi>m</mi><mi>a</mi><mi>j</mi></mrow></msub><mo>-</mo><msub><mi>S</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced><mo>&#xD7;</mo><mi>&#x3B2;</mi></math><br>其中<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>S</mi><mrow><mi>m</mi><mi>a</mi><mi>j</mi></mrow></msub></math>为多数类样本数量，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>S</mi><mi>min</mi></msub></math>为少数类样本数量，β∈[0,1]为系数。G即为总共想要<font color="#8A2BE2">合成的少数类样本数量</font>，如果β=1则是合成后各类别数目相等。</p></li><li><p>对于每个少类别样本xi，找出其K近邻个点，并计算：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x393;</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>&#x394;</mi><mi>i</mi></msub><mo>/</mo><mi>K</mi></mrow><mi>Z</mi></mfrac></math><br>其中Δi为K近邻个点中多数类样本的数量，Z为规范化因子以确保 Γ 构成一个分布。这样若一个少数类样本<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub></math>的周围多数类样本越多，则其 Γi 也就越高。</p></li><li><p>最后对每个少类别样本<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub></math>计算需要合成的样本数量<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">g</mi><mi>i</mi></msub></math>，再用SMOTE算法合成新样本：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>g</mi><mi>i</mi></msub><mo>=</mo><msub><mi>&#x393;</mi><mi>i</mi></msub><mo>&#xD7;</mo><mi>G</mi></math><br>可以看到ADASYN利用分布Γ来自动决定每个少数类样本所需要合成的样本数量，这等于是给每个少数类样本施加了一个权重，周围的多数类样本越多则权重越高。ADASYN的缺点是<font color="#A52A2A">易受离群点的影响</font>，如果一个少数类样本的K近邻都是多数类样本，则其权重会变得相当大，进而会在其周围生成较多的样本。</p></li></ol><p>下面利用sklearn中的 <em>make_classification</em> 构造了一个不平衡数据集，各类别比例为{0:54, 1:946}。原始数据，SMOTE，Borderline-1 SMOTE，Borderline-2 SMOTE和ADASYN的比较见下图，<font color="#0000FF">左侧为过采样后的决策边界</font>，<font color="#8B008B">右侧为过采样后的样本分布情况</font>，<font color="#B8860B">可以看到过采样后原来少数类的决策边界都扩大了，导致更多的多数类样本被划为少数类了</font>：</p><blockquote><p><font color="#0000FF">知识补充</font><br>决策边界顾名思义就是需要分类的数据中，区分不同类别的边界。</p></blockquote><pre><code>    原始数据</code></pre><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623222264899.png" alt="原始数据"><br>        SMOTE<br><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623222317451.png" alt="SMOTE过采样"><br>        Borderline-1 SMOTE<br><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623222382482.png" alt="Borderline-1 SMOTE"><br>        Borderline-2 SMOTE<br><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623222404250.png" alt="Borderline-2 SMOTE"><br>        ADASYN<br><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623222425227.png" alt="ADASYN"></p><p>从上图我们也可以比较几种过采样方法各自的特点。用 <code>SMOTE</code> 合成的样本分布比较平均，而<code>Border-line SMOTE</code>合成的样本则集中在类别边界处。<code>ADASYN</code>的特性是一个少数类样本周围多数类样本越多，则算法会为其生成越多的样本，从图中也可以看到生成的样本大都来自于原来与多数类比较靠近的那些少数类样本。</p><h3 id="欠采样"><a href="#欠采样" class="headerlink" title="欠采样"></a>欠采样</h3><ol><li><p>随机欠采样</p><p> 随机欠采样的思想同样比较简单，就是从多数类样本中随机选取一些剔除掉。这种方法的缺点是<font color="#0000FF">被剔除的样本可能包含着一些重要信息</font>，致使学习出来的模型效果不好。</p></li><li><p>EasyEnsemble 和 BalanceCascade<br>  EasyEnsemble和BalanceCascade采用集成学习机制来<font color="#8A2BE2">处理传统随机欠采样中的信息丢失</font>问题。</p></li></ol><ul><li>EasyEnsemble将多数类样本随机<font color="#00FFFF">划分成n个子集</font>，每个子集的数量等于少数类样本的数量，这相当于欠采样。接着将每个子集与少数类样本结合起来分别训练一个模型，最后将n个模型集成，这样虽然每个子集的样本少于总体样本，但集成后总信息量并不减少。</li><li>如果说EasyEnsemble是基于无监督的方式从多数类样本中生成子集进行欠采样，那么BalanceCascade则是采用了<font color="#7FFFD4">有监督</font>结合Boosting的方式。在第n轮训练中，将从多数类样本中抽样得来的子集与少数类样本结合起来训练一个基学习器H，训练完后多数类中能被H正确分类的样本会被剔除。在接下来的第n+1轮中，从被剔除后的多数类样本中产生子集用于与少数类样本结合起来训练，最后将不同的基学习器集成起来。BalanceCascade的有监督表现在每一轮的基学习器起到了在多数类中选择样本的作用，而其Boosting<font color="#bf242a">特点则体现在每一轮丢弃被正确分类的样本，进而后续基学习器会更注重那些之前分类错误的样本。</font></li></ul><blockquote><p><font color="#0000FF">知识补充</font>基学习器<br><a href="https://www.biaodianfu.com/boosting.html">机器学习算法之Boosting – 标点符</a><br>同质集成中的个体学习器又称为基学习器（base learner），相应的学习算法也被称为基学习算法（base learning algorithm）。</p></blockquote><ol start="3"><li>NearMiss</li></ol><p><font color="#725e82"><strong>NearMiss</strong></font>本质上是一种<font color="#BDB76B">原型选择</font>(prototype selection)方法，即从多数类样本中选取最具代表性的样本用于训练，主要是为了缓解随机欠采样中的信息丢失问题。NearMiss采用一些<font color="#A52A2A">启发式的规则</font>来选择样本，根据规则的不同可分为3类：</p><ul><li>NearMiss-1：选择到最近的K个少数类样本平均距离最近的多数类样本</li><li>NearMiss-2：选择到最远的K个少数类样本平均距离最近的多数类样本</li><li>NearMiss-3：对于每个少数类样本选择K个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围</li></ul><p>NearMiss-1和NearMiss-2的计算<font color="#0000FF">开销很大</font>，因为需要计算每个多类别样本的K近邻点。另外，NearMiss-1易受离群点的影响，如下面第二幅图中合理的情况是处于边界附近的多数类样本会被选中，然而由于右下方一些少数类离群点的存在，其附近的多数类样本就被选择了。相比之下NearMiss-2和NearMiss-3不易产生这方面的问题。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623224933174.png" alt="图一Oniginal data"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623225034300.png" alt="图二Resampling using Nearmiss-1"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623225086130.png" alt="图三Resampling using Nearmiss-2"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623225124104.png" alt="图四Resampling using Nearmiss-3"></p><ol start="4"><li>数据清洗方法 (data cleaning tichniques)</li></ol><p>这类方法主要<font color="#8A2BE2">通过某种规则来清洗重叠的数据</font>，从而达到欠采样的目的，而这些规则往往也是启发性的，下面进行简要阐述：</p><ul><li><p><font color="#ff7500">Tomek Link</font>：Tomek Link表示<font color="#7FFFD4">不同类别</font>之间距离最近的一对样本，即<font color="#bf242a">这两个样本互为最近邻且分属不同类别</font>。这样如果两个样本形成了一个Tomek Link，则要么其中一个是噪音，要么两个样本都在边界附近。这样通过移除Tomek Link就能“清洗掉”类间重叠样本，使得互为最近邻的样本皆属于同一类别，从而能更好地进行分类。</p><pre><code>  下图一上为原始数据，图二上为SMOTE后的数据，图三虚线标识出Tomek Link，图四为移除Tomek Link后的数据集，可以看到不同类别之间样本重叠减少了很多。</code></pre></li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623226151018.png" alt="图一"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623226162443.png" alt="图二"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623226175192.png" alt="图三"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623226186988.png" alt="图四"></p><ul><li><font color="#ff7500"> Edited Nearest Neighbours(ENN)</font>：对于属于多数类的一个样本，如果其K个近邻点有超过一半都不属于多数类，则这个样本会被剔除。这个方法的另一个变种是所有的K个近邻点都不属于多数类，则这个样本会被剔除。、</li></ul><p>最后，数据清洗技术<font color="#0000FF">最大的缺点</font>是无法控制欠采样的数量。由于都在某种程度上采用了K近邻法，而事实上大部分多数类样本周围也都是多数类，因而能剔除的多数类样本比较有限。</p><h3 id="过采样和欠采样结合"><a href="#过采样和欠采样结合" class="headerlink" title="过采样和欠采样结合"></a>过采样和欠采样结合</h3><p>上文中提到SMOTE算法的缺点是生成的少数类样本容易与周围的多数类样本产生重叠难以分类，而数据清洗技术恰好可以处理掉重叠样本，所以可以将二者结合起来形成一个pipeline，先过采样再进行数据清洗。主要的方法是 <code>SMOTE + ENN</code> 和 <code>SMOTE + Tomek</code> ，其中 <code>SMOTE + ENN</code> 通常能清除更多的重叠样本，如下图：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623227628385.png" alt="Resampling using Original"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623227661187.png" alt="Resampling using SMOTE"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623227708226.png" alt="Resampling using SMOTE + ENN"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623227766137.png" alt="Resampling using SMOTE + TOMEK"></p><hr><p><strong><font color="#DC143C">★ 采样方法的效果</font></strong></p><p><a href="https://www.cnblogs.com/massquantity/p/9382710.html">机器学习之类别不平衡问题 (3) —— 采样方法 - massquantity - 博客园</a></p><h2 id="省心的方法"><a href="#省心的方法" class="headerlink" title="省心的方法"></a>省心的方法</h2><h3 id="主动收集数据"><a href="#主动收集数据" class="headerlink" title="主动收集数据"></a>主动收集数据</h3><p>针对少量样本数据，可以尽可能去扩大这些少量样本的数据集，或者尽可能去增加他们特有的特征来丰富数据的多样性（尽量转化成情况1）。譬如，如果是一个情感分析项目，在分析数据比例时发现负样本（消极情感）的样本数量较少，那么我们可以尽可能在网站中搜集更多的负样本数量，或者花钱去买，毕竟数据少了会带来很多潜在的问题。</p><h3 id="将任务转换成异常检测问题、"><a href="#将任务转换成异常检测问题、" class="headerlink" title="将任务转换成异常检测问题、"></a>将任务转换成异常检测问题、</h3><p>如果少数类样本太少，少数类的结构可能并不能被少数类样本的分布很好地表示，那么用平衡数据或调整算法的方法不一定有效。如果这些少数类样本在特征空间中再分布的比较散，情况会更加糟糕。这时候不如将其转换为无监督的异常检测算法，不用过多的去考虑将数据转换为平衡问题来解决。</p><h3 id="调整权重"><a href="#调整权重" class="headerlink" title="调整权重"></a>调整权重</h3><p>可以简单的设置损失函数的权重，让模型增加对多数类的惩罚，更多的关注少数类。在python的scikit-learn中我们可以使用class_weight参数来设置权重。</p><p>另外，调整权重方法也适合于这种情况：不同类型的错误所造成的后果不同。例如在医疗诊断中，错误地把健康人诊断为患者可能会带来进一步检查的麻烦，但是错误地把患者诊断为健康人，则可能会丧失了拯救生命的最佳时机；再如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故；在信用卡盗用检查中，将正常使用误认为是盗用，可能会使用户体验不佳，但是将盗用误认为是正常使用，会使用户承受巨大的损失。为了权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”（unequal cost）。</p><h3 id="阈值调整（threshold-moving）"><a href="#阈值调整（threshold-moving）" class="headerlink" title="阈值调整（threshold moving）"></a>阈值调整（threshold moving）</h3><p>直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将原本默认为0.5的阈值调整到 <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mo>|</mo><mi>P</mi><mo>|</mo></mrow><mrow><mo>(</mo><mo>|</mo><mi>P</mi><mo>|</mo><mo>+</mo><mo>|</mo><mi>N</mi><mo>|</mo><mo>)</mo></mrow></mfrac></math>即可。（大部分是负样本，因此分类器倾向于给出较低的分数）</p><h2 id="类别不平横影响模型的输出"><a href="#类别不平横影响模型的输出" class="headerlink" title="类别不平横影响模型的输出"></a>类别不平横影响模型的输出</h2><p>许多模型的输出是基于阈值的，大部分模型的默认阈值为输出值的中位数。比如逻辑回归的输出范围为[0,1]，当某个样本的输出大于0.5就会被划分为正例，反之为反例。在数据的类别不平衡时，采用默认的分类阈值可能会导致输出全部为反例，产生虚假的高准确度，导致分类失败。</p>]]></content>
    
    
    <summary type="html">目中出现了二分类数据不平横问题，研究总结下对于类别不平横问题的处理经验</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="data mining" scheme="https://merlynr.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>kernal</title>
    <link href="https://merlynr.github.io/2021/05/30/kernal/"/>
    <id>https://merlynr.github.io/2021/05/30/kernal/</id>
    <published>2021-05-29T16:00:00.000Z</published>
    <updated>2021-05-29T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p><font color="#6495ED">核方法</font>是一类把低维空间的非线性可分问题，转化为高维空间的线性可分问题的方法。</p><blockquote><p><font color="#A9A9A9">理论基础:</font>核方法的理论基础是Cover’s theorem，指的是<font color="#FF8C00">对于非线性可分的训练集，可以大概率通过将其非线性映射到一个高维空间来转化成线性可分的训练集。</font></p></blockquote><p><font color="#9400D3">核函数</font>是映射关系 的内积，映射函数本身仅仅是一种映射关系，并没有增加维度的特性，不过可以利用核函数的特性，构造可以增加维度的核函数。</p><p>设 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">X</mi></math>是输入空间（即 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>i</mi></msub><mo>&#x2208;</mo><mi mathvariant="script">X</mi></math> ， <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">X</mi></math>  是 <math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi mathvariant="normal">&#x211D;</mi><mi>n</mi></msup></math> 的子集或离散集合 ），又设<math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">H</mi></math>  为特征空间（<math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">H</mi></math> 是希尔伯特空间），如果存在一个从 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">X</mi></math> 到 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">H</mi></math> 的映射</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3D5;</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>:</mo><mi mathvariant="script">X</mi><mo>&#x2192;</mo><mi mathvariant="script">H</mi></math></p><p>使得对所有 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi><mo>,</mo><mi>z</mi><mo>&#x2208;</mo><mi mathvariant="script">X</mi></math>,函数<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo>)</mo></math>满足条件</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo>)</mo><mo>=</mo><mo>&#x27E8;</mo><mi>&#x3D5;</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>,</mo><mi>&#x3D5;</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>&#x27E9;</mo></math></p><p>则称 $K$ 为核函数。其中 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3D5;</mi><mo>(</mo><mi>x</mi><mo>)</mo></math> 为映射函数， <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&#x27E8;</mo><mo>&#xB7;</mo><mo>,</mo><mo>&#xB7;</mo><mo>&#x27E9;</mo></math>为内积。</p><p>即核函数输入两个向量，它返回的值<font color="#FF1493">等于</font>这两个向量分别作 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3D5;</mi></math> 映射然后点积【内积】的结果。</p><p><font color="#008B8B">核技巧</font>是一种利用核函数直接计算 <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&#x27E8;</mo><mi>&#x3D5;</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>,</mo><mi>&#x3D5;</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>&#x27E9;</mo></math> ，以避开分别计算<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3D5;</mi><mo>(</mo><mi>x</mi><mo>)</mo></math>  和<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3D5;</mi><mo>(</mo><mi>z</mi><mo>)</mo></math>  ，从而加速核方法计算的技巧。</p><blockquote><p><font color="#FF1493">注意</font><br>得益于<font color="#FF8C00">SVM对偶问题</font>的表现形式，核技巧可以应用于SVM。<br><font color="#7FFF00">TODO  </font>没有了解<br>核函数的选择是SVM的<font color="#B8860B">最大变数</font>，如果核函数选择不适，那么  将不能将输入空间映射到线性可分的特征空间。</p></blockquote><h2 id="判断核函数"><a href="#判断核函数" class="headerlink" title="判断核函数"></a>判断核函数</h2><p><font color="#bf242a">不知道 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3D5;</mi></math> 的情况下，如何判断某个 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math> 是不是核函数？</font></p><p><strong>答案:</strong> 是 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math> 是核函数当且仅当对任意数据 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>D</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>&#x2026;</mo><mo>,</mo><msub><mi>x</mi><mi>m</mi></msub></math> ，核矩阵(kernal matrix,gram matrix)总是半正定的</p><blockquote><p><font color="#368AF8">知识补充：</font><strong>实对称矩阵</strong><br>如果有n阶矩阵A，其矩阵的元素都为实数，且矩阵A的转置等于其本身（aij=aji），(i,j为元素的脚标），则称A为实对称矩阵。</p></blockquote><blockquote><p><font color="#6495ED"><a href="https://zhuanlan.zhihu.com/p/44860862">知识补充</a>：</font><font color="#8B0000">「正定矩阵」(positive definite)</font>和<font color="#8B0000">「半正定矩阵」(positive semi-definite)</font><br><strong>正定矩阵：</strong> 给定一个大小为 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>&#xD7;</mo><mi>n</mi></math> 的实对称矩阵<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math>  ，若对于任意长度为 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math> 的<font color="#A66766">非零向量</font> <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">x</mi></math>，有 <math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi mathvariant="bold-italic">x</mi><mi>T</mi></msup><mi>A</mi><mi mathvariant="bold-italic">x</mi><mo>&gt;</mo><mn>0</mn></math> 恒成立，则矩阵 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math>  是一个正定矩阵。<br><strong>半正定矩阵：</strong> 给定一个大小为 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>&#xD7;</mo><mi>n</mi></math> 的实对称矩阵<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math>  ，若对于任意长度为 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math> 的<font color="#A66766">向量</font> <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">x</mi></math>，有 <math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi mathvariant="bold-italic">x</mi><mi>T</mi></msup><mi>A</mi><mi mathvariant="bold-italic">x</mi><mo>&gt;</mo><mn>0</mn></math> 恒成立，则矩阵 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math>  是一个正定矩阵。<br><font color="#FF00FF">半正定矩阵包括了正定矩阵，核矩阵与协方差矩阵都要半正定</font></p></blockquote><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622369097125.png" alt="核矩阵"></p><h2 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h2><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622366953315.png" alt="常用核函数"></p><h2 id="栗子"><a href="#栗子" class="headerlink" title="栗子"></a>栗子</h2><p>举一个<a href="https://zhuanlan.zhihu.com/p/95362628">栗子</a><br>下面这张图位于第一、二象限内。我们关注红色的门，以及“北京四合院”这几个字下面的紫色的字母。我们把红色的门上的点看成是“+”数据，紫色字母上的点看成是“-”数据，它们的横、纵坐标是两个特征。显然，在这个二维空间内，“+”“-”两类数据不是线性可分的。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622369536767.png" alt="二维"></p><p>我们现在考虑核函数<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mfenced><mrow><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub></mrow></mfenced><mo>=</mo><mo>&lt;</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><msup><mo>&gt;</mo><mn>2</mn></msup></math>，即“内积平方”。<br>这里面<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>v</mi><mn>1</mn></msub><mo>=</mo><mfenced><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub></mrow></mfenced><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo>=</mo><mfenced><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub></mrow></mfenced></math>是二维空间中的两个点。</p><p>这个核函数对应着一个二维空间到三维空间的映射，它的表达式是：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo><mo>=</mo><mfenced><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>,</mo><msqrt><mn>2</mn></msqrt><mi>x</mi><mi>y</mi><mo>,</mo><msup><mi>y</mi><mn>2</mn></msup></mrow></mfenced></math><br>可以验证，<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622369667776.png" alt="核函数"></p><p>在P这个映射下，原来二维空间中的图在三维空间中的像是这个样子：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622369693244.png" alt="三维"></p><p><font color="#D2691E">注意</font>到绿色的平面可以完美地分割红色和紫色，也就是说，两类数据在三维空间中变成线性可分的了。</p><p>而三维中的这个判决边界，再映射回二维空间中是这样的：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622369730773.png" alt="再二维"></p><p>这是一条双曲线，它不是线性的。</p><p><font color="#A52A2A">通过高维映射使得特征线性可分，换种思路就是当两个特征值无法将数据分开时，就将两个特征值进行点交，形成第三个特征，这个时候就有三个特征值，然后构成三位空间，进行分类</font></p>]]></content>
    
    
    <summary type="html">核函数相关的笔记</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="data mining" scheme="https://merlynr.github.io/tags/data-mining/"/>
    
    <category term="kernal method" scheme="https://merlynr.github.io/tags/kernal-method/"/>
    
    <category term="kernal trick" scheme="https://merlynr.github.io/tags/kernal-trick/"/>
    
    <category term="kernal function" scheme="https://merlynr.github.io/tags/kernal-function/"/>
    
  </entry>
  
</feed>

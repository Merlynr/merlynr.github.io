<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>吾辈之人，自当自强不息！</title>
  
  <subtitle>博客</subtitle>
  <link href="https://merlynr.github.io/atom.xml" rel="self"/>
  
  <link href="https://merlynr.github.io/"/>
  <updated>2021-07-04T16:00:00.000Z</updated>
  <id>https://merlynr.github.io/</id>
  
  <author>
    <name>Merlynr</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>KNN（K近邻法 K Nearest Neighbors）</title>
    <link href="https://merlynr.github.io/2021/07/05/KNN%EF%BC%88K%E8%BF%91%E9%82%BB%E6%B3%95%20K%20Nearest%20Neighbors%EF%BC%89/"/>
    <id>https://merlynr.github.io/2021/07/05/KNN%EF%BC%88K%E8%BF%91%E9%82%BB%E6%B3%95%20K%20Nearest%20Neighbors%EF%BC%89/</id>
    <published>2021-07-04T16:00:00.000Z</published>
    <updated>2021-07-04T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/25994179">一文搞懂k近邻（k-NN）算法（一） - 知乎</a></p><p><a href="https://blog.csdn.net/qq_20412595/article/details/82013677">机器学习算法（2）之K近邻算法_不曾走远的博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/pxhdky/article/details/85080980">【机器学习】K近邻法（KNN）与kd树原理详解_齐在的专栏-CSDN博客</a></p><p>TODO 序列KNN</p><h2 id="KNN概述"><a href="#KNN概述" class="headerlink" title="KNN概述"></a>KNN概述</h2><ul><li>常用有监督学习方法</li><li>常用分类方法</li><li>同时也是回归方法</li><li>是懒惰学习</li></ul><blockquote><p><font color="#ff7500">扩展学习</font><br>懒惰学习是一种训练集处理方法，其会<font color="#012C54">在收到测试样本的同时进行训练</font>，与之相对的是急切学习，其会<font color="#8A2BE2">在训练阶段开始对样本进行学习</font>处理。</p></blockquote><p><font color="#FF8C00">基本思路：</font><br>如果一个待分类样本在特征空间中的k个最相似(即特征空间中K近邻)的样本中的大多数属于某一个类别，则该样本也属于这个类别，即近朱者赤，近墨者黑。</p><h2 id="KNN算法介绍"><a href="#KNN算法介绍" class="headerlink" title="KNN算法介绍"></a>KNN算法介绍</h2><h3 id="KNN模型"><a href="#KNN模型" class="headerlink" title="KNN模型"></a>KNN模型</h3><p><strong>kNN使用的模型实际上对应于对特征空间的划分。</strong></p><p><font color="#006400">由三个及基本要素组成：</font></p><ul><li>距离度量</li><li>k值的选择</li><li>决策规划</li></ul><ol><li>距离度量</li></ol><p>KNN中使用的距离度量可以是欧式距离、曼哈顿距离、切比雪夫距离或者一般的闵可夫斯基距离。</p><blockquote><p><font color="#9932CC">知识补充</font><br>设特征空间 $X$ 是 $n$ 维实数向量空间<math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mi>n</mi></msup></math>，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub></math>∈ $X$ ，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mo>(</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msubsup><mo>)</mo><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msubsup><msup><mo>)</mo><mi>T</mi></msup></math>，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>j</mi></msub><mo>=</mo><mo>(</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msubsup><mo>)</mo><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msubsup><msup><mo>)</mo><mi>T</mi></msup></math></p><ol><li><p>闵可夫斯基距离（Minkowski distance,<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi>p</mi></msub></math>距离）<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub></math>的<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi>p</mi></msub></math>距离定义为：<br><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625491452207.png"><br>其中，p ≥ 1 。 </p></li><li><p>曼哈顿距离（Manhattan distance）<br>当p = 1 时，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi>p</mi></msub></math>距离就变成了曼哈顿距离：<br><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625491491032.png"></p></li><li><p>欧式距离（Euclidean distance）<br>当p = 2时，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi>p</mi></msub></math>距离就变成了欧式距离：<br><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625491674920.png"></p></li><li><p>切比雪夫距离（Chebyshev distance）<br>当<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>=</mo><mo>&#x221E;</mo><mo>,</mo><msub><mi>L</mi><mi>p</mi></msub></math>距离就变成了切比雪夫距离，它是各个坐标距离的最大值：<br><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625491693716.png"></p></li></ol></blockquote><ol start="2"><li>k值选择（借鉴李航–统计学习方法）</li></ol><p>如果k值较小，则训练误差减少，只有与输入实例相似的训练实例才会对于预测结果起作用,“学习”<font color="#D2691E">近似误差会减小</font>，但泛化误差提高了，预测结果会对近邻实例点非常敏感。k值较小意味着模型变得复杂，容易发生<font color="#0000FF">过拟合</font>。</p><p>如果k值较大，可以减少泛化误差，其优点是可以<font color="#D2691E">减少学习的估计误差</font>，但训练误差会增加，这时与输入实例相差较远的训练实例也会对预测结果起作用。k值较大意味着模型变得简单，容易发生<font color="#0000FF">欠拟合</font>。</p><p>通常情况下，我们需要对 k 经过多种尝试，来决定到底使用多大的 k 来作为最终参数。k通常会在3～10直接取值，或者是k等于训练数据的<font color="#DC143C">平方根</font>。比如15个数据，可能会取k=4。</p><p>当k = 1时，k近邻算法就是最近邻算法。k值一般<font color="#FF1493">采用交叉验证法选取最优值</font>。</p><ol start="3"><li>决策规划</li></ol><p>通常，在分类任务中使用投票法计算最终预测结果，在回归任务中使用平均法，还可基于距离远近进行加权平均或加权投票。</p><h3 id="KNN算法描述"><a href="#KNN算法描述" class="headerlink" title="KNN算法描述"></a>KNN算法描述</h3><p>下面以<font color="#008B8B">分类</font>任务为例，介绍KNN算法，回归任务与此类似，区别不大。</p><p>输入：训练数据集<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>D</mi><mo>=</mo><mo>{</mo><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)</mo><msubsup><mo>}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></math>    ，其中，<img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625492364666.png"> 是实例的类别。<br>过程：</p><ul><li>根据给定的距离度量，在训练集D中找出与x最邻近的k个点，涵盖着k 个点的领域记为<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></math>；</li><li>在<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></math>中根据分类决策规则决定x的类别y： <img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625492543638.png" alt="所属类别"><br>输出：测试样本x xx所属的类别y yy。</li></ul><h2 id="KNN算法实现"><a href="#KNN算法实现" class="headerlink" title="KNN算法实现"></a>KNN算法实现</h2><h3 id="KNN算法蛮力实现"><a href="#KNN算法蛮力实现" class="headerlink" title="KNN算法蛮力实现"></a>KNN算法蛮力实现</h3><p> 首先我们看看最想当然的方式。</p><pre><code> 既然我们要找到k个最近的邻居来做预测，那么我们只需要计算预测样本和所有训练集中的样本的距离，然后计算出最小的k个距离即可，接着多数表决，很容易做出预测。这个方法的确简单直接，在样本量少，样本特征少的时候有效。但是在实际运用中很多时候用不上，为什么呢？因为我们经常碰到样本的特征数有上千以上，样本量有几十万以上，如果我们这要去预测少量的测试集样本，算法的时间效率很成问题。因此，这个方法我们一般称之为蛮力实现。&lt;font color=&quot;#1E90FF&quot;&gt;比较适合于少量样本的简单模型的时候用&lt;/font&gt;。</code></pre><h3 id="KD树实现原理"><a href="#KD树实现原理" class="headerlink" title="KD树实现原理"></a>KD树实现原理</h3><pre><code>KD树算法没有一开始就尝试对测试样本分类，而是**先对训练集建模，建立的模型就是KD树，建好了模型再对测试集做预测**。所谓的KD树就是K个特征维度的树，注意这里的K和KNN中的K的意思不同。KNN中的K代表特征输出类别，KD树中的K代表样本特征的维数。为了防止混淆，后面我们称特征维数为n。</code></pre><p>KD树算法包括三步，第一步是建树，第二部是搜索最近邻，最后一步是预测。</p><h4 id="KD树的建立"><a href="#KD树的建立" class="headerlink" title="KD树的建立"></a>KD树的建立</h4><p>我们首先来看建树的方法。KD树建树采用的是从m个样本的n维特征中，分别计算n个特征的取值的方差，用<font color="#DC143C">方差最大</font>的第k维特征<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>k</mi></msub></math>来作为<font color="#B22222">根节点</font>。对于这个特征，我们选择特征<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>k</mi></msub></math>的取值的中位数<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>kv</mi></msub></math>对应的样本作为划分点，对于所有第k维特征的取值小于<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>kv</mi></msub></math>的样本，我们划入左子树，对于第k维特征的取值大于等于<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>kv</mi></msub></math>的样本，我们划入右子树，对于左子树和右子树，我们采用和刚才同样的办法来找方差最大的特征来做<font color="#B22222">根节点，递归</font>的生成KD树。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202175/1625494718364.png" alt="构建KD树"></p><p>比如我们有二维样本6个，{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，构建kd树的具体步骤为：</p><ol><li>找到划分的特征。6个数据点在x，y维度上的数据方差分别为6.97，5.37，所以在x轴上方差更大，用第1维特征建树。</li><li>确定划分点（7,2）。根据x维上的值将数据排序，6个数据的中值(所谓中值，即中间大小的值)为7，所以划分点的数据是（7,2）。这样，该节点的分割超平面就是通过（7,2）并垂直于：划分点维度的直线x=7；（很显然，中位数为6 ，这里选择（5,4）或者(7,2)都是可以的。这种情况任选一个即可）</li><li>确定左子空间和右子空间。 分割超平面x=7将整个空间分为两部分：x&lt;=7的部分为左子空间，包含3个节点={(2,3),(5,4),(4,7)}；另一部分为右子空间，包含2个节点={(9,6)，(8,1)}。</li><li>用同样的办法划分左子树的节点{(2,3),(5,4),(4,7)}和右子树的节点{(9,6)，(8,1)}。最终得到KD树。</li><li>后续步骤反复上面的，<font color="#8FBC8F">直到两个子区域没有实例存在时停止（这意味着最后所有训练实例都对应一个叶结点或内部结点），从而形成kd树的区域划分</font>。</li></ol>]]></content>
    
    
    <summary type="html">【可選】文章描述</summary>
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>Markov Model（马尔可夫模型）</title>
    <link href="https://merlynr.github.io/2021/07/05/Markov%20Model%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%89/"/>
    <id>https://merlynr.github.io/2021/07/05/Markov%20Model%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%89/</id>
    <published>2021-07-04T16:00:00.000Z</published>
    <updated>2021-07-04T16:00:00.000Z</updated>
    
    
    <summary type="html">【可選】文章描述</summary>
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>用于业务流程事件和结果预测的混合模型</title>
    <link href="https://merlynr.github.io/2021/07/05/%E7%94%A8%E4%BA%8E%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E4%BA%8B%E4%BB%B6%E5%92%8C%E7%BB%93%E6%9E%9C%E9%A2%84%E6%B5%8B%E7%9A%84%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/"/>
    <id>https://merlynr.github.io/2021/07/05/%E7%94%A8%E4%BA%8E%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E4%BA%8B%E4%BB%B6%E5%92%8C%E7%BB%93%E6%9E%9C%E9%A2%84%E6%B5%8B%E7%9A%84%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-07-04T16:00:00.000Z</published>
    <updated>2021-07-04T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>对于多样性流程进行异常预测</p><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><ol><li>序列k近邻法（KNN）</li><li>基于序列比对的马尔科夫模型扩展法</li></ol><p><font color="#1E90FF">思路：</font><br>利用数据的时间分类特征，利用高阶马尔可夫模型预测过程的下一步，并利用序列对比技术预测过程的结果。通过考虑基于k个最近邻的相似过程序列的子集，增加了数据的多样性方面。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>已经证明，通过一组实验，序列k最近邻提法比原始提供更好的结果;我们的扩展马尔可夫模型优于随机猜测、马尔可夫模型和隐马尔可夫模型。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h2 id="序列比对"><a href="#序列比对" class="headerlink" title="序列比对"></a>序列比对</h2><h2 id="预测模型"><a href="#预测模型" class="headerlink" title="预测模型"></a>预测模型</h2><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;h3 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h3&gt;&lt;p&gt;对于多</summary>
      
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="RPA" scheme="https://merlynr.github.io/tags/RPA/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>七月前完成</title>
    <link href="https://merlynr.github.io/2021/06/27/%E4%B8%83%E6%9C%88%E5%89%8D%E5%AE%8C%E6%88%90/"/>
    <id>https://merlynr.github.io/2021/06/27/%E4%B8%83%E6%9C%88%E5%89%8D%E5%AE%8C%E6%88%90/</id>
    <published>2021-06-26T16:00:00.000Z</published>
    <updated>2021-06-26T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul><li><input disabled="" type="checkbox"> 机器学习</li><li><input disabled="" type="checkbox"> 预测算法</li><li><input disabled="" type="checkbox"> 五篇小论文</li><li><input disabled="" type="checkbox"> 考虑数据源与公司想结合</li></ul><h2 id="done"><a href="#done" class="headerlink" title="done"></a>done</h2><ol><li>XES文档2.5</li><li>使用ProMLite</li></ol>]]></content>
    
    
    <summary type="html">周学习计划</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
    <category term="daily/weekly" scheme="https://merlynr.github.io/tags/daily-weekly/"/>
    
  </entry>
  
  <entry>
    <title>深度学习与循环神经网络在预测下一个过程事件问题上的初步应用</title>
    <link href="https://merlynr.github.io/2021/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9C%A8%E9%A2%84%E6%B5%8B%E4%B8%8B%E4%B8%80%E4%B8%AA%E8%BF%87%E7%A8%8B%E4%BA%8B%E4%BB%B6%E9%97%AE%E9%A2%98%E4%B8%8A%E7%9A%84%E5%88%9D%E6%AD%A5%E5%BA%94%E7%94%A8/"/>
    <id>https://merlynr.github.io/2021/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9C%A8%E9%A2%84%E6%B5%8B%E4%B8%8B%E4%B8%80%E4%B8%AA%E8%BF%87%E7%A8%8B%E4%BA%8B%E4%BB%B6%E9%97%AE%E9%A2%98%E4%B8%8A%E7%9A%84%E5%88%9D%E6%AD%A5%E5%BA%94%E7%94%A8/</id>
    <published>2021-06-24T16:00:00.000Z</published>
    <updated>2021-06-24T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><font color="#DC143C">TITLE</font>: A Deep Learning Approach for Predicting Process Behaviour at Runtime</p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>描述深度学习与循环神经网络在预测下一个过程事件问题上的初步应用</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>主要介绍了流程预测与自然语言的处理有很多地方类似，同时也有不同之处。</p><p><font color="#FF1493">流程预测与自然语言不同：</font></p><ul><li>过程预测（事件类型数量）中词汇量的大小远小于自然语言词汇的大小</li><li>轨迹的长度远远超过自然语言中的典型句子长度</li><li>通过内部过程逻辑确定或约束过程事件序列，通常通过基于案例数据确定的决策规则确定。然而，以语法和形态规则的形式，自然语言也受到限制</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p><strong><font color="#B22222">这块主要讲了一下业务流程预测相关的研究</font></strong></p><h3 id="预测完成一个案件的剩余时间"><a href="#预测完成一个案件的剩余时间" class="headerlink" title="预测完成一个案件的剩余时间"></a>预测完成一个案件的剩余时间</h3><ol><li>使用事件频率、事件时间和案例数据的增强回归</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624673781121.png" alt="When will this case finally be finished?"><br>2. 将隐马尔可夫模型应用于事件序列和执行时间[基于一个带注释的转换系统]</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624674466379.png" alt="A test-bed for the evaluation of bussiness process prediction techniques"><br>3. 使用聚类树和有限状态机(FSM)来预测运行过程案例的剩余时间</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624674870904.png" alt="Context-aware predictions on bussiness processes: An ensemble-based solution"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624674967541.png" alt="Discovering context-aware models for predicting business process performances"><br>4. 将复杂的事件处理（CEP）应用于事件序列，并培训以预测其未来行为</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675079138.png" alt="Facilitating predictive event-driven process analytics"><br>5. 使用随机petri网模拟</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675153578.png" alt="Prediction of remaining service execution time using stochastic petri nets with arbitrary firing delays"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675288618.png" alt="Prediction of bussiness process durations using non-markovian stochastic petri nets"><br>6. 基于案例数据聚类和回归的预测技术</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675498447.png" alt="A data-driven prediction framework for analyzing and monitoring business process performances"><br>7. 对部分和全部案例采用聚类方法</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675713998.png" alt="Process remaining time prediction using query catalogs"><br>8. 提出了两种基于带注释的转换系统的方法，以及支持向量回归和朴素贝叶斯分类器</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675768877.png" alt="Data-aware remaining time prediction of business process instances"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624675838918.png" alt="Time and activity sequence prediction of business process instances"></p><h3 id="流程预测结果评估【二元评估】"><a href="#流程预测结果评估【二元评估】" class="headerlink" title="流程预测结果评估【二元评估】"></a>流程预测结果评估【二元评估】</h3><ol><li>在时间、资源和案例数据上使用决策树</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624676985594.png" alt="Predictive business operations management"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624677076066.png" alt="Business process intelligence"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624677129344.png" alt="Improving bussiness "><br>2. 使用支持向量机</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624686889124.png" alt="Periodic berformance prediction for real-time business process monitoring"><br>3. 基于聚类和局部离群点检测</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624687171172.png" alt="Real-time business process monitoring method for prediction of abnormal termination using knni-based LOF prediction"><br>4. 使用决策树来预测违反线性时序逻辑限制</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624687372135.png" alt="Predictive monitoring of business processes"><br><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624687442053.png" alt="Predictive monitoring of business processes"><br>5. 使用随机森林</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624687491226.png" alt="Complex symbolic sequence encodings for predictive monitoring of business processes"><br>6. 采用神经网络、约束满足和服务质量聚合</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624687569443.png" alt="Comparing and combining predictive business process monitoring techniques"><br>7. 聚类和回归</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021626/1624687617567.png" alt="A prediction framework for proactive monitoring aggregate process-performance indicators"></p><h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><h2 id="Process-Prediction-using-RNN"><a href="#Process-Prediction-using-RNN" class="headerlink" title="Process Prediction using RNN"></a>Process Prediction using RNN</h2><h2 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h2><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><h2 id="扩展学习"><a href="#扩展学习" class="headerlink" title="扩展学习"></a>扩展学习</h2><h3 id="阅读paper36【P10】"><a href="#阅读paper36【P10】" class="headerlink" title="阅读paper36【P10】"></a>阅读paper36【P10】</h3><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021627/1624759564094.png" alt="paper36"></p><h3 id="递归神经网络与循环神经网络"><a href="#递归神经网络与循环神经网络" class="headerlink" title="递归神经网络与循环神经网络"></a>递归神经网络与循环神经网络</h3><p><font color="#0000FF">Recursive Neural Network || Recurrent Neural Network</font></p><h3 id="Hidden-Markov-Models-HHM"><a href="#Hidden-Markov-Models-HHM" class="headerlink" title="Hidden Markov Models(HHM)"></a>Hidden Markov Models(HHM)</h3><h3 id="LSTM与RNN"><a href="#LSTM与RNN" class="headerlink" title="LSTM与RNN"></a>LSTM与RNN</h3>]]></content>
    
    
    <summary type="html">对于深度学习在预测中应用的总结</summary>
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>总结“基于机器学习的业务流程系统的预测”中的技术点</title>
    <link href="https://merlynr.github.io/2021/06/24/%E6%80%BB%E7%BB%93%E2%80%9C%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%A2%84%E6%B5%8B%E2%80%9D%E4%B8%AD%E7%9A%84%E6%8A%80%E6%9C%AF%E7%82%B9/"/>
    <id>https://merlynr.github.io/2021/06/24/%E6%80%BB%E7%BB%93%E2%80%9C%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%A2%84%E6%B5%8B%E2%80%9D%E4%B8%AD%E7%9A%84%E6%8A%80%E6%9C%AF%E7%82%B9/</id>
    <published>2021-06-23T16:00:00.000Z</published>
    <updated>2021-06-23T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="论文总体流程"><a href="#论文总体流程" class="headerlink" title="论文总体流程"></a>论文总体流程</h2><p>本文针对三个预测任务提 出 了 两个预测模型 。 一个预测模型是 用来预测流 程结果 的 模 型 ， 本文提 出 了 利 用 深度 学 习 中 序 列 处理 网 络 Ｌ Ｓ ＴＭ算法模 型去<font color="#183B64">预测流程结果 </font>的方法 ， 此方法 旨 在将流程结果 的预测 问 题与 自 然语言处理方 向 相 结合 ， 提供 一个新 的解决思路 。另 一个预测模型 则是用 来<font color="#1E90FF">预测事件活动与 时 间 相关任务</font> 的模 型 ， 此预测 模型 将本文研 究 的预测流程下 一时刻活动与 时 间 、预测流程后续时刻事件 活动 与 时 间 （ 即 剩余周 期 时 间 ） 两个预测任务利 用 一个预测模型 实现 。</p><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><h3 id="关注点"><a href="#关注点" class="headerlink" title="关注点"></a>关注点</h3><p><font color="#9400D3">关注点</font></p><ul><li>预测业务流程的<font color="#008B8B">下一时刻活动和与时间相关活动</font></li><li>预测业务流程中运行案例的未来路径</li><li>预测业务流程运行的剩余周期时间</li><li>预测业务流程执行<font color="#006400">结果</font>以及预测业务流程执行结束后的性能</li></ul><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><h4 id="预测流程结果"><a href="#预测流程结果" class="headerlink" title="预测流程结果"></a>预测流程结果</h4><ol><li>序列处理网络LSTM算法模型【将流程结果的预测问题与自然语言处理方向相结合】</li><li>决策树</li><li>使用支持向量机（SVM）【此方法可以提供描述实时业务的流程当前性能的实施指标，缺点是评估指标仅为准确率，较为单一】</li><li>基于KNN算法和局部异常值检测，此方法提出了 一个通过替换未观察到的属性来生成实例的插补方法，但并未验证其泛用性</li></ol><h4 id="预测事件活动与时间相关任务的模型"><a href="#预测事件活动与时间相关任务的模型" class="headerlink" title="预测事件活动与时间相关任务的模型"></a>预测事件活动与时间相关任务的模型</h4><ol><li>利用自然语言处理中的GRU网络结构、双向循环网络结构、Word2vec技术以及Attention机制进行预测【将本文研 究 的预测流程下一时刻活动与时间、预测流程后续时刻事件活动与时间（  剩余周期时间）两个预测任务利用一个预测模型实现】</li></ol><h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
    
    
    <summary type="html">【可選】文章描述</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/categories/plan/graduate-student/"/>
    
    <category term="summarize" scheme="https://merlynr.github.io/categories/plan/graduate-student/summarize/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/categories/plan/graduate-student/summarize/paper/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/categories/plan/graduate-student/summarize/paper/machine-learning/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/plan/graduate-student/summarize/paper/machine-learning/algorithm/"/>
    
    <category term="process mining" scheme="https://merlynr.github.io/categories/plan/graduate-student/summarize/paper/machine-learning/algorithm/process-mining/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>2021-6-22[daily]</title>
    <link href="https://merlynr.github.io/2021/06/21/2021-6-22[daily]/"/>
    <id>https://merlynr.github.io/2021/06/21/2021-6-22[daily]/</id>
    <published>2021-06-20T16:00:00.000Z</published>
    <updated>2021-06-21T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NEED-TODO"><a href="#NEED-TODO" class="headerlink" title="NEED TODO"></a>NEED TODO</h2><ul><li><input disabled="" type="checkbox"> 阅读论文SpinalFlow: An Architecture and Dataflow Tailored for Spiking Neural Networks</li><li><input disabled="" type="checkbox"> 制作相关演讲PPT</li><li><input disabled="" type="checkbox"> 学习三个吴恩达机器学习视频</li><li><input disabled="" type="checkbox"> 学习一个中等，一个简单算法</li><li><input disabled="" type="checkbox"> 学习Java相关进阶</li></ul><h2 id="Plan"><a href="#Plan" class="headerlink" title="Plan"></a>Plan</h2><ol><li>10.20-11.20，阅读论文</li><li>13.40-15.30，阅读论文</li><li>15.40-16.20，看机器学习视频</li><li>16.30-17.20，学习算法</li><li>18.10-21.10，看论文</li><li>21.20-22.40，看Java进阶</li></ol><h2 id="Completion-Status"><a href="#Completion-Status" class="headerlink" title="Completion Status"></a>Completion Status</h2><h2 id="TODO-Tomorrow"><a href="#TODO-Tomorrow" class="headerlink" title="TODO Tomorrow"></a>TODO Tomorrow</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;NEED-TODO&quot;&gt;&lt;a href=&quot;#NEED-TODO&quot; class=&quot;headerlink&quot; title=&quot;NEED TODO&quot;&gt;&lt;/a&gt;NEED TODO&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; </summary>
      
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="daily" scheme="https://merlynr.github.io/tags/daily/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>2021-6-24[daily]</title>
    <link href="https://merlynr.github.io/2021/06/21/2021-6-24[daily]/"/>
    <id>https://merlynr.github.io/2021/06/21/2021-6-24[daily]/</id>
    <published>2021-06-20T16:00:00.000Z</published>
    <updated>2021-06-23T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NEED-TODO"><a href="#NEED-TODO" class="headerlink" title="NEED TODO"></a>NEED TODO</h2><ul><li><input disabled="" type="checkbox"> 阅读论文基于机器学习的业务流程系统的预测，总结其中技术点</li><li><input disabled="" type="checkbox"> 学习三个吴恩达机器学习视频</li><li><input disabled="" type="checkbox"> 学习一个中等，一个简单算法</li><li><input disabled="" type="checkbox"> 学习Java相关进阶</li></ul><h2 id="Plan"><a href="#Plan" class="headerlink" title="Plan"></a>Plan</h2><ul><li><input checked="" disabled="" type="checkbox"> <ol><li>10.20-11.20，阅读论文</li></ol></li><li><input disabled="" type="checkbox"> <ol start="2"><li>13.40-15.30，阅读论文</li></ol></li><li><input disabled="" type="checkbox"> <ol start="3"><li>15.40-16.20，看机器学习视频</li></ol></li><li><input disabled="" type="checkbox"> <ol start="4"><li>16.30-17.20，学习算法</li></ol></li><li><input disabled="" type="checkbox"> <ol start="5"><li>18.10-21.10，看论文</li></ol></li><li><input disabled="" type="checkbox"> <ol start="6"><li>21.20-22.40，看Java进阶</li></ol></li></ul><h2 id="Completion-Status"><a href="#Completion-Status" class="headerlink" title="Completion Status"></a>Completion Status</h2><h2 id="TODO-Tomorrow"><a href="#TODO-Tomorrow" class="headerlink" title="TODO Tomorrow"></a>TODO Tomorrow</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;NEED-TODO&quot;&gt;&lt;a href=&quot;#NEED-TODO&quot; class=&quot;headerlink&quot; title=&quot;NEED TODO&quot;&gt;&lt;/a&gt;NEED TODO&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; </summary>
      
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="daily" scheme="https://merlynr.github.io/tags/daily/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>555~四级</title>
    <link href="https://merlynr.github.io/2021/06/01/555~%E5%9B%9B%E7%BA%A7/"/>
    <id>https://merlynr.github.io/2021/06/01/555~%E5%9B%9B%E7%BA%A7/</id>
    <published>2021-05-31T16:00:00.000Z</published>
    <updated>2021-06-01T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h2><p><img src="https://gitee.com/merlynr/img-store/raw/master/202161/1622551964785.png" alt="作文要求"></p><ol><li>时间很紧，第一遍就需要写在纸上</li><li>手写，需要注意字体，建议衡水体</li><li>写的时候右侧也对齐，同时写85%，这样美观</li><li>背一些句型和一些常用词汇的替换词</li><li>切记不要跑题，介意通过直接更改题干来立题</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/202161/1622552071072.png" alt="衡水体"></p><h3 id="衔接词"><a href="#衔接词" class="headerlink" title="衔接词"></a>衔接词</h3><p>due to 因为<br>in spite of尽管，<br>thus因此，<br>on the contrary相反地<br>首先 in the first place, to begin with, first of all, for one thing<br>然后，而且 in addition, what’s more,moreover, besides, for another thing<br>最后 last but not the least<br>表举例 for instance<br>表对比 in contrast, on the contrary</p><h3 id="专业词汇"><a href="#专业词汇" class="headerlink" title="专业词汇"></a>专业词汇</h3><p><a href="https://www.bilibili.com/video/BV1MN411Z7Lu?spm_id_from=333.788.b_765f64657363.3">热点词</a></p><h3 id="例句"><a href="#例句" class="headerlink" title="例句"></a>例句</h3><ol><li>It is obvious that the cartoon is trying to tell us…</li><li>Currently, there is a growing tendency that people in mounting numbers are showing great enthusiasm for sth.</li><li>From my perspective/As for me, at no time should we ignore the importance of A.</li><li>“<strong><strong><strong>” is the opinion held by</strong></strong></strong> . This remark has been confirmed time and again by more and more people. “______”是______的观点，而且被越来越多的人反复证实。</li><li>The advantages of A are much greater than those of B.</li><li>A number of factors are accountable for this situation. 造成这种情况的因素有很多。</li></ol><h2 id="听力"><a href="#听力" class="headerlink" title="听力"></a>听力</h2><ol><li>利用一切时间，读题，找关键词</li><li>可以去预判一些常考的考点【因果、并列、转折、举例、男女对话即为换题的标志】</li><li>一般事请都会有波折，即往不好的方向发展</li><li>当没听清时，往主旨上蒙，越是详细越可能出错</li></ol><h2 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a>阅读</h2><p><font color="#00CED1">20mins</font></p><ol><li>原句<font color="#FF1493">重复出现，200%错</font>。正确的都是有改动的，即同意替换。</li><li>文章是<font color="#9400D3">按顺序出题</font>的。你要觉得不是，就是你做错了。</li><li>选项中意思完全相反的2个选项，<font color="#E9967A">其中之一是对的</font>。（要有这个意识）。</li><li>就一般而言，some people，表作者不认同的观点。<font color="#006400">few people，表作者的观点</font>。</li><li>用文章里<font color="#9400D3">举例的句子来作为选项</font>，直接排除。200%错。（要有能辨别这个选项是不是文章中例子的能力）。</li><li>某某人说的话，或者是带引号的，一定要高度<font color="#8B008B">重视</font>。尤其是在段落的后半部分。很有可能就是某个问题的同意替换。即题眼。</li><li>有的时候，一句话可以设2个问题。不过这种情况很少出现了，非常少。。</li><li>文章基本以5段为主（也有6段、7段的），要把握每段之间的关系。一般来说，一段一个题，只是一般来说喔。。</li><li>一篇文章总会有5、6+个长难句，且总会在这里设问题。所以，<font color="#FF8C00">长难句必须要拿下</font>！！</li><li><font color="#9932CC">每段的第一句很重要</font>。尤其总分结构的段。有的时候第一句话就是题眼。考研英语，总分结构或者总分总的段落很多。。</li><li>若文章首段以why为开头的，这里若设题的话，选项里有because的，往往就是正确选项。不过这种类型的题，很少见了。。</li><li>有时候每段的第一句话，仅仅是一个表述。而在第2或3句以后，会出现对比或者转折。一般来说，<font color="#00FFFF">转折后面的是作者的态度</font>。你要注意的是，作者对什么进行了转折。那个关键词你要找出来。</li><li>在应该出现答案的地方，没有答案。。接着往下读。答案可能会在下一段的开头部分。因为文章都是接着说的。要有连贯性。这和7选5的技巧有些相似。不过这种情况并不多见。。</li><li>一个长句看不懂，接着往下看，下一句可能是这个长句的解释说明。是的话，这的地方可能会出题。出的话，答案就在这附近。而实际情况是，文章在谈论某个问题或提出某个观点时，有时会再做进一步的解释说明。这种情况下，这里往往会设问题。不过，这种情况很少见了。。</li><li>有些句子仅仅是解释补充，或者是起过渡作用的。这样句子的特点是，句子比较短。注意，答案一般不会在这儿出现。选项中出现，肯定是<font color="#9932CC">干扰项</font>。你要知道的是，同意替换的句子，大都是长难句。一些作为过渡的句子，不可能是答案。在你读不懂的情况下，要有这个判断力。</li><li>正确选项都是原文中的个别几个词的<font color="#D2691E">同义替换</font>。阅读理解历年的所有真题，都是同意替换！！就看你能不能找得到。考研英语，考的就是这个！！那个关键词，就看你找没找得到，不管是什么类型的题。。</li><li>每一个问题，在原文中，都要有一个定位。然后精读，找出那个中心句或者关键词。要抓文章的中心主旨和各段落的大意，阅读理解考的就是这个“<font color="#057748">中心句</font>”。</li><li>选项中的几个单词，是该段中不同句子里的单词拼凑的，有时看上去很舒服，注意，干扰项。还有从不同的段落里的词拼凑到一起的，直接排除。总之，选项的单词是<font color="#A52A2A">拼凑的</font>，肯定错。</li><li>一定要注意文章中句子的宾语部分，尤其是长难句中主干的宾语。上面说了，考研英语大都是长难句里设题。你要知道的是，长难句里，最可能是出题的就是句子的主干部分！主干的主语、宾语是什么，一定要知道。<font color="#FF1493">正确选项</font>的题眼往往就在这儿。当然，还有一些起修饰、限定作用的词，一定要看仔细。小心陷阱。</li><li>若某个问题，是特别长的一个句子，一定要看清问的是什么，别打马虎眼。这是做题时需要留意的地方。</li><li>注意问题的主语是谁，它和原文题眼的主语原则上是一致的。主语不一致，一般来说，都是<font color="#9400D3">错的。</font></li><li>即第6条，某某人说的话，尤其特别长的句子，或者是带引号的。60%以上会出题。题眼就在这儿。这里又提了一遍，就是要引起你的<font color="#057748">重视</font>。</li><li>错误的选项，往往是就文章某一方面而说的，其特点是：所涉及的，仅仅是某一个小问题，或者很具体，<font color="#B22222">非常具体的一件实事</font>。200%错误选项。这是考研英语最经常遇到的干扰项。一定要会识别。</li><li>中国人出的题。多是总-分结构，或者总-分-总。所以每段开头结尾，都要注意。（这里指的是中间没有出现转折的段落）。整篇文章的<font color="#057748">开头结尾</font>。也要重视。</li><li>文章的结构，要么总-分或总-分-总，要么转折、对比，要么举例说明。就这么几个套路。</li><li>对选项中的“重点词”（即主语、宾语、修饰语）都要看清楚。有的时候，选项中，会对原文中本来正确的事做错误的修改，来作为干扰项。你要注意的是，选项句子的主语（与原文）是否一致、宾语是否符合原文意思，或者用一些牵强的修饰词，来做一些特殊的限定。要看清楚。这是干扰项的特点之一。</li><li>某人说过的话，有时并不是题眼，但可以从侧面或某个角度来反映作者的观点，也就是作者想表达的。<font color="#00FFFF">正确答案都是和这样的观点相一致的</font>。要把握关键词，有感情色彩的词。做题时，要有这个意识。</li><li>就某个词或者某个句子设问题，不用猜词。<font color="#057748">就一条，文章主旨</font>！ 不用去研究这个词什么意思，把握主旨即可。全文主旨和段落主旨（前者更重要）。</li><li>接着28条说，不管什么题型，上面说的还是其他别的题型。很绝对的说，反映主旨的肯定对，前提是你能确定它就是主旨。考研英语，一直到2011年，这一条还没变过。所以，文章读不太懂，但能把握作者想表达的意思即可。如2011年争议题37题。</li><li>注意中心句（即题眼）和前后句子之间的关系，是接着说的，还是转折关系。这里出题的话，要把握和<font color="#8A2BE2">前后句子之间的关系</font>。是并列关系的，可以从这些句子里找同义词。是转折关系的，就通过转折关系句子里的关键词的相反意思来判断。前提是在你读不懂的情况下。</li><li>凡是举例的，都是为了说明观点的。那么，这个观点（中心句），一般来说，会在举例之前就表达了。但有时候也在举例之后。总之，<font color="#bf242a">作者举例想说明的这个观点，你一定要找出来</font>。</li><li>排除2个选项以后，选出和文章主旨相关的选项即可。不知道主旨就把握关键词。</li><li>词汇题的正确答案，往往隐藏在原文的该处附近（就是那个<font color="#0000FF">同义替换词</font>），原文这附近的句子，是并列关系或者解释说明句的，就从这些句子的关键词的相近意思去把握。是转折关系的，就从关键词的相反意思去把握。总之，你要找的就是那个关键词。和30条一起理解吧。。</li><li>如果原文中出现“ A is B and C”。若某一问题，选项中出现了B没C，或者只出现C没B。<font color="#A52A2A">肯定错</font>，直接排除。可能你会问了，同时出现B and C 咋办？ 目前还没出现过这种情况。。注意，这里说的B和C，是单词或者短语。。这是干扰项的特点之一。 实际情况是，这个<font color="#B22222">句子不是题眼</font>。</li><li>接34题说，还一种情况是，若B和C是2个长句子，中间用分号隔开的。且这两个句子都是作者想表达的，选项中都出现了。。一般来说，选项中会对其中之一做错误的修改来作为干扰项。而另一个是对的。（如05年TEXT1 ，第一题。不过总体来说，这种题型非常非常少见。偶在这里想说的是34条。这样的干扰项，你要会识别。）</li><li>注意几个词，yet表转折，hardly表否定。while 有时是比较，有时也表转折。比较的时候，注意比较的对象，要弄清楚。转折的时候，你<font color="#0000FF">要知道作者对什么进行了转折</font>。</li><li>如果你对“关键词”比较蒙，或者你想问：我怎么知道哪个是关键词？解释一下，关键词就是句子中主干的宾语。尤其是一些你觉得比较重要的句子。这样的句子多数是长难句。一般来说，一个句子主干的主语，宾语，和其他的修饰部分，都是很重要的！！ 宾语是主语的宾语，所以，和主语是要对上号的，对不上不行。（也就是26条的主语是否一致）。至于修饰的部分，干扰项常常在这里做手脚，比如会有一些特殊的限定，千万要留意，别疏忽了。。</li><li>什么是中心句？即反应文章的主旨和每一段的中心意思的一句话。这句话是客观存在的。也就是作者的观点。中心句即题眼，选出正确答案，看的就是中心句。只有中心句才能选出正确答案。所以，中心句不知道在哪，或者读不懂，很难选出正确答案。中心句的具体位置，见下条。</li><li><font color="#7FFF00">很关键的一条</font>，抓住每段的中心意思，也就是中心句。每段至少一句，最多2句。 一般来说，总分结构的段落，中心句一般在段首。举例段一般在举例前后。转折段，中心句在出现转折的地方，或者后一句（一般来说在该段的第三行上下浮动）。再就是某某人说的话。要注意这句话和前后句的关系，是并列还是转折。然后来把握这句话的意思，把握不了就通过前后句是并列还是转折关系的关键词来把握。</li><li>每个问题，要还原到文章具体的某一段落。若此问题在某段的后半部分，且你没有太看懂，这段已经完事了。。要养成一个习惯。<font color="#1E90FF">接着看一下段的第一句话</font>。实在做不出来的话，就选那个和下一段第一句话的意思差不多的选项。只能这样了。。 （貌似是13条的重复）补充下，这只是小技巧，只起补充作用，有时候用不上。。</li><li>每段的第三行，一般来说，也是该段的第3句话（也可能是第2、4句话）。其特点是：句子很长，由两句或者两句以上组成，是个长难句。尤其是 that mean ，the notion is that 之类的，一定要重视。要把握句子的主干。作者想说的是什么（把握作者强调的是哪个句子）。看清楚哪句话是为了修饰哪句话的。这样的句子，若出题的话，句子的主干就是正确选项。起补充修饰作用的一定要看清楚。。每段最重要的三个地方：<font color="#0000FF">段首，段尾，和这儿</font>。再就是带引号的。中心句一般就在这几个地方。 其实也就这么几个地方。。别的地方一般都是过渡句。。</li><li>若是转折段的话，要注意转折的那个句子，一般都是在<font color="#0000FF">41条</font>的那个地方（即第三行上下浮动）。转折前后都要看，看对比的是什么。在看不懂的情况下，通过前面的，来翻译后面的（<font color="#00FFFF">反向翻译</font>），来找关键词。反之亦然。</li><li>最后一段，主要看段首和段尾。（最后一段是转折段的情况很少）。若是叙事段的话，叙事部分以外的，重点看。叙事部分尽量看懂。<font color="#725e82">非叙事部分非常重要</font>。一般段首若出现答案的话，段尾可能会作干扰项（见54条），但也不是绝对的（有时段首段尾都会有答案的提示）。段尾若出现答案的话，段首可能会很普通。 一定要把握哪一句话是重点，选项中有相近意思的不是片面的叙述，一般就是正确答案。要把握重点的句子提到的被说明对象（句子主干的宾语），也就是作者关注的。</li><li>选项中出现<font color="#7FFF00">ONLY </font>的，目前还没有对的。</li><li>说明原因的，且<font color="#057748">仅仅是说明原因</font>而已。目前没有对的。</li><li>中心句特别长的，2小句组成，选项中这2句都出现了，怎么排除？反映主旨的是对的。就是作者关注的对象！还一选项是对其进行具体的解释说明，或者补充，或是对主旨的一个具体现象的反应，或是对其造成的后果的叙述。这一选项一般会做错误的修改而作为<font color="#A52A2A">干扰项</font>（即使不做错误的修改也一样是干扰项）总之，这样的题，符合<font color="#1E90FF">28、29</font>条的就是对的。符合<font color="#B22222">23</font>条的，就是错的。</li><li>一定要注意，谁是用来修饰谁的。<font color="#B22222">起修饰作用</font>的词或句子，来做选项，一般是错的。<font color="#057748">被修饰的那部分</font>来作选项，一般是对的。</li><li>因果关系的题，很直接、很简单的因果关系，直接排除。间接的因果，反映主旨的，可能是对的。 总之，因果关系的题，把握主旨就可以了。文中提到的直接因果，如具体的事或是什么的。<font color="#8B0000">都是干扰项</font>。</li><li>48的补充，正确选项反应的，往往是实质的，根本的内容。选项反应的若是<font color="#bf242a">很具体</font>的某一表现，一般都是干扰项。</li><li>干扰项有时出现的生词（可能是你不认识的），是与文章主题无关的词，而非同意替换。（这就需要你的基本功了）</li><li>新趋势，有些题要懂文章才能做出来。读不懂很难选出来。而且，长难句明显增多。有时，它会让你崩溃到单词都认识，却不知道文章说的是什么。这时候什么技巧都不好使了。所以，一定要提高基本功。起码你要知道文章大概说的是啥，也就是谁和谁的关系。任何一篇文章的主旨，基本上都可以用“谁和谁的关系”来概括。</li><li>、通过首段或者前两段，来把握信息点。也就是作者想说的，<font color="#008B8B">是谁和谁的关系</font>？</li><li>接着上面说，一篇文章谈的是什么，或者说“谁与谁的关系”，一定要弄懂。这个具体的什么“关系”弄不懂的话，“谁与谁”一定要弄明白。比如，<font color="#00008B">一篇文章说的是A与B之间如何如何。若问题问你A，选项有B的，往往就是正确答案。若问你B，你就可以先把没有A的选项排除</font>。</li><li>最新趋势，最后一段，段尾很明显不是总结，而是以补充为主的句子。注意，这里可能会<font color="#FF1493">以干扰项的形式出现。</font></li><li>如上所说，中心句出现的地方无非就是段首、段中、段尾，或者带引号的句子。但是，这也是干扰项常常出现的地方。所以，你的基本功，对文章理解的程度，是你必须具备的能力。任何一门考试都有技巧，但是想拿理想的分数，光靠技巧是不现实的。</li><li>有的时候，你会遇到出现2到3个否定词的句子。否定再否定，或者否定否定再否定。遇到了，尤其是3重否定的，基本上<font color="#00008B">这里会设题</font>，这句话里的关键词一定要找出来。这个地方是要练的，到时候出现了，别蒙，别犯怵。。</li><li>再补充一条，<font color="#00008B">however 后面的句子一定要重视</font>。比如有一年的其中一篇的3个题，题眼都是however 后面的句子。 所以，这个词一定要敏感。</li></ol><p><strong><font color="#FF8C00">技巧done</font></strong></p><h2 id="段落匹配"><a href="#段落匹配" class="headerlink" title="段落匹配"></a>段落匹配</h2><p><font color="#9400D3">10mins</font></p><ol><li>看选项，勾关键词【能看懂的】<font color="#FF8C00">3~4mins</font></li><li>找对应的两个及以上的关键词</li></ol><p><strong><font color="#FF8C00">技巧done</font></strong></p><h2 id="选词填空"><a href="#选词填空" class="headerlink" title="选词填空"></a>选词填空</h2><p><font color="#9400D3">10mins</font></p><ol><li>先标词性在选词 </li></ol><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><ol><li>替换+尬写</li><li>看一下<a href="https://www.bilibili.com/video/BV1MN411Z7Lu?spm_id_from=333.788.b_765f64657363.3">热点词</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;写作&quot;&gt;&lt;a href=&quot;#写作&quot; class=&quot;headerlink&quot; title=&quot;写作&quot;&gt;&lt;/a&gt;写作&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://gitee.com/merlynr/img-store/raw/master/202161/162255</summary>
      
    
    
    
    <category term="exam" scheme="https://merlynr.github.io/categories/exam/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
    <category term="exam" scheme="https://merlynr.github.io/tags/exam/"/>
    
  </entry>
  
  <entry>
    <title>孤立森林（Isolation Forest）</title>
    <link href="https://merlynr.github.io/2021/05/31/%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97%EF%BC%88Isolation%20Forest%EF%BC%89/"/>
    <id>https://merlynr.github.io/2021/05/31/%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97%EF%BC%88Isolation%20Forest%EF%BC%89/</id>
    <published>2021-05-30T16:00:00.000Z</published>
    <updated>2021-05-30T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622426214147.png" alt="数据蛋糕"></p><p>假设我们用一个随机超平面来切割（split）数据空间（data space）, 切一次可以生成两个子空间（想象拿刀切蛋糕一分为二）。</p><p>之后我们再继续用一个随机超平面来切割每个子空间，循环下去，直到每子空间里面只有一个数据点为止。</p><blockquote><p><font color="#8B008B">满足的条件</font></p><ul><li>数据本身不可再分割</li><li>二叉树达到限定的最大深度</li></ul></blockquote><p>直观上来讲，我们可以发现那些密度很高的簇是可以被切很多次才会停止切割，但是那些密度很低的点很容易很早的就停到一个子空间里了。</p><p><font color="#6495ED">异常检测原理的理解：</font>由于异常值的数量较少且与大部分样本的疏离性，因此，异常值会被更早的孤立出来，也即异常值会距离iTree的根节点更近，而正常值则会距离根节点有更远的距离。</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>孤立森林算法主要针对的是<strong>连续型结构化</strong>数据中的异常点。</p><p><font color="#FF1493">理论前提</font></p><ul><li>异常数据占总样本量的比例很小</li><li>异常点的特征值与正常点的差异很大</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622426925434.png" alt="数据"></p><p>上图中，中心的白色空心点为正常点，即处于高密度群体中。四周的黑色实心点为异常点，散落在高密度区域以外的空间。</p><h3 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h3><p>孤立森林算法是基于 <strong>Ensemble</strong> 的异常检测方法，因此具有<font color="#7FFF00">线性的时间复杂度</font>。且精准度较高，在处理大数据时速度快，所以目前在工业界的应用范围比较广。常见的场景包括：网络安全中的攻击检测、金融交易欺诈检测、疾病侦测、噪声数据过滤（数据清洗）等。</p><blockquote><p><font color="#006400">知识补充</font>集成学习算法 (Ensemble Learning)<br>统机器学习算法 (例如：决策树，人工神经网络，支持向量机，朴素贝叶斯等) 的目标都是寻找一个最优分类器尽可能的将训练数据分开。集成学习 (Ensemble Learning) 算法的基本思想就是将多个分类器<font color="#8FBC8F">组合</font>，从而实现一个预测效果更好的<font color="#8A2BE2">集成分类器</font>。</p></blockquote><blockquote><p><font color="#FF00FF">知识补充：</font><br><a href="https://blog.zuishuailcq.xyz/2021/05/31/%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/">算法的性能指标 | 吾辈之人，自当自强不息！</a></p></blockquote><h2 id="孤立森林的创新点"><a href="#孤立森林的创新点" class="headerlink" title="孤立森林的创新点"></a>孤立森林的创新点</h2><ol><li><font color="#D2691E">Partial models</font>：在训练过程中，每棵孤立树都是随机选取部分样本</li><li><font color="#D2691E">No distance or density measures</font>：不同于 KMeans、DBSCAN 等算法，孤立森林不需要计算有关距离、密度的指标，可大幅度提升速度，减小系统开销</li><li><font color="#D2691E"> Linear time complexity</font>：因为基于 ensemble，所以有线性时间复杂度。通常树的数量越多，算法越稳定</li><li><font color="#D2691E">Handle extremely large data size</font>：由于每棵树都是独立生成的，因此可部署在大规模分布式系统上来加速运算</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;理解&quot;&gt;&lt;a href=&quot;#理解&quot; class=&quot;headerlink&quot; title=&quot;理解&quot;&gt;&lt;/a&gt;理解&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://gitee.com/merlynr/img-store/raw/master/2021531/16224</summary>
      
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="data mining" scheme="https://merlynr.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>拟合</title>
    <link href="https://merlynr.github.io/2021/05/31/%E6%8B%9F%E5%90%88/"/>
    <id>https://merlynr.github.io/2021/05/31/%E6%8B%9F%E5%90%88/</id>
    <published>2021-05-30T16:00:00.000Z</published>
    <updated>2021-06-02T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/72038532#:~:text=%E5%AF%B9%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%88%96%E6%9C%BA%E5%99%A8,%E7%A7%B0%E4%B8%BA%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE%E3%80%82">欠拟合、过拟合及如何防止过拟合 - 知乎</a></p><h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p>对于深度学习或机器学习模型而言，我们不仅要求它对训练数据集有很好的拟合（训练误差），同时也希望它可以对未知数据集（测试集）有很好的拟合结果（泛化能力），所产生的测试误差被称为泛化误差。度量泛化能力的好坏，最直观的表现就是模型的过拟合（overfitting）和欠拟合（underfitting）。过拟合和欠拟合是用于描述模型在训练过程中的两种状态。一般来说，训练过程会是如下所示的一个曲线图。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622461280602.png" alt="训练过程"></p><p>训练刚开始的时候，模型还在学习过程中，处于欠拟合区域。随着训练的进行，训练误差和测试误差都下降。在到达一个临界点之后，训练集的误差下降，测试集的误差上升了，这个时候就进入了过拟合区域——由于训练出来的网络过度拟合了训练集，对训练集以外的数据却不有效。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622461630220.png" alt="拟合"></p><h2 id="欠拟合"><a href="#欠拟合" class="headerlink" title="欠拟合"></a>欠拟合</h2><p><font color="#9932CC">欠拟合</font>是指模型不能在训练集上获得足够低的误差。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622461604985.png" alt="欠拟合"></p><p><strong>如何解决欠拟合？</strong></p><p>欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过<font color="#7FFF00">增加网络复杂度</font>或者在模型中<font color="#7FFF00">增加特征</font>，这些都是很好解决欠拟合的方法。</p><h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><p><font color="#9932CC">过拟合</font>是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，<font color="#ff7500">模型在训练集上表现很好，但在测试集上却表现很差。</font>模型对训练集”死记硬背”（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，泛化能力差。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622461615228.png" alt="过拟合"></p><p><font color="#FF1493">出现原因</font></p><ol><li><strong>训练数据集样本单一，样本不足。</strong> 如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。</li><li><strong>训练数据中噪声干扰过大。</strong> 噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。</li><li><strong>模型过于复杂</strong>。模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。</li></ol><p><strong><font color="#FF8C00">如何防止过拟合</font></strong><br>要想解决过拟合问题，就要显著减少测试误差而不过度增加训练误差，从而提高模型的泛化能力。我们可以使用正则化（Regularization）方法。</p><blockquote><p>正则化是指修改学习算法，使其降低泛化误差而非训练误差。</p></blockquote><h3 id="正则化方法"><a href="#正则化方法" class="headerlink" title="正则化方法"></a>正则化方法</h3><blockquote><p>常用的正则化方法根据具体的使用<font color="#D2691E">策略</font>不同可分为：<br>（1）直接提供正则化约束的参数正则化方法，如L1/L2正则化；<br>（2）通过工程上的技巧来实现更低泛化误差的方法，如提前终止(Early stopping)和Dropout；<br>（3）不直接提供约束的隐式正则化方法，如数据增强等。</p></blockquote><ol><li> 获取和使用更多的数据（数据集增强）——解决过拟合的<font color="#DC143C">根本性</font>方法</li></ol><p>让机器学习或深度学习模型泛化能力更好的办法就是使用更多的数据进行训练。但是，在实践中，我们拥有的数据量是有限的。解决这个问题的一种方法就是<font color="#7FFF00">创建“假数据”并添加到训练集中——数据集增强</font>。通过增加训练集的额外副本来增加训练集的大小，进而改进模型的泛化能力。</p><ol start="2"><li>采用合适的模型（控制模型的复杂度）</li></ol><p>过于复杂的模型会带来过拟合问题。对于模型的设计，目前公认的一个深度学习规律”deeper is better”。国内外各种大牛通过实验和竞赛发现，对于CNN来说，层数越多效果越好，但是也更容易产生过拟合，并且计算所耗费的时间也越长。</p><p>根据<font color="#E9967A">奥卡姆剃刀</font>法则：在同样能够解释已知观测现象的假设中，我们应该挑选“最简单”的那一个。对于模型的设计而言，我们应该选择简单、合适的模型解决复杂的问题。</p><ol start="3"><li>降低特征的数量</li></ol><p>对于一些特征工程而言，可以降低特征的数量——<font color="#006400">删除冗余特征</font>，人工选择保留哪些特征。这种方法也可以解决过拟合问题。</p><ol start="4"><li>L1 / L2 正则化</li></ol><p><a href="https://www.cnblogs.com/zingp/p/10375691.html#_label0">深入理解L1、L2正则化 - ZingpLiu - 博客园</a></p><ul><li>L1正则化</li></ul><p>L1正则化可以使得参数稀疏化，即得到的参数是一个稀疏矩阵，可以用于特征选择。</p><pre><code>    稀疏性，说白了就是模型的很多参数是0。通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，很多参数是0，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，即使去掉对模型也没有什么影响，此时我们就可以只</code></pre><p>在原始的损失函数后面加上一个L1正则化项，即<strong>全部权重 $w$ 的绝对值的和，再乘以λ/n</strong>。则损失函数变为：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mfrac><mi>&#x3BB;</mi><mi>n</mi></mfrac><munder><mo>&#x2211;</mo><mi>i</mi></munder><mfenced close="|" open="|"><msub><mi>w</mi><mi>i</mi></msub></mfenced></math></p><p>对应的梯度（导数）：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mo>&#x2202;</mo><mi>C</mi></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>&#x2202;</mo><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac><mo>+</mo><mfrac><mi>&#x3BB;</mi><mi>n</mi></mfrac><mo>sgn</mo><mo>(</mo><mi>w</mi><mo>)</mo></math></p><p>其中 <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>sgn</mo><mo>(</mo><mi>w</mi><mo>)</mo></math> 只是简单地取 $w1$ 各个元素地正负号。</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>sgn</mo><mo>(</mo><mi>w</mi><mo>)</mo><mo>=</mo><mfenced close="" open="{"><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>,</mo><mi>w</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>,</mo><mi>w</mi><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mo>-</mo><mn>1</mn><mo>,</mo><mi>w</mi><mo>&lt;</mo><mn>0</mn></mtd></mtr></mtable></mfenced></math></p><p>梯度下降时权重 $w$ 更新变为：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi><mo>&#x2192;</mo><msup><mi>w</mi><mo>‘</mo></msup><mo>=</mo><mi>w</mi><mo>-</mo><mfrac><mrow><mi>&#x3B7;</mi><mi>&#x3BB;</mi></mrow><mi>n</mi></mfrac><mo>sgn</mo><mo>(</mo><mi>w</mi><mo>)</mo><mo>-</mo><mi>&#x3B7;</mi><mfrac><mrow><mo>&#x2202;</mo><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac></math></p><p>当 $w=0$ 时，|w|是不可导的。所以我们仅仅能依照原始的未经正则化的方法去更新 $w$  。<br>当 $w&gt;0$  时，sgn( $w$  )&gt;0, 则梯度下降时更新后的 $w$  变小。<br>当 $w&lt;0$  时，sgn( $w$  )&gt;0, 则梯度下降时更新后的 $w$  变大。换句换说，L1正则化使得权重 $w$ 往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。</p><p>这也就是<font color="#6495ED">L1正则化会产生更稀疏（sparse）的解</font>的原因。此处稀疏性指的是最优值中的一些参数为0。<font color="#1E90FF">L1正则化的稀疏性质已经被广泛地应用于特征选择</font>机制，从可用的特征子集中选择出有意义的特征。</p><ul><li>L2 正则化</li></ul><p>L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合。</p><p>拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是抗扰动能力强。</p><p>L2正则化通常被称为<strong>权重衰减</strong>（weight decay），就是在原始的损失函数后面再加上一个L2正则化项，即<strong>全部权重</strong> $w$  的平方和，再乘以λ/2n。则损失函数变为：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mfrac><mi>&#x3BB;</mi><mrow><mn>2</mn><mi>n</mi></mrow></mfrac><mo>&#xB7;</mo><mo>&#x2211;</mo><msubsup><mi>w</mi><mi>i</mi><mn>2</mn></msubsup></math></p><p>对应的梯度（导数）：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mo>&#x2202;</mo><mi>C</mi></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>&#x2202;</mo><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac><mo>+</mo><mfrac><mi>&#x3BB;</mi><mi>n</mi></mfrac><mi>w</mi></math></p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mo>&#x2202;</mo><mi>C</mi></mrow><mrow><mo>&#x2202;</mo><mi>b</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>&#x2202;</mo><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mo>&#x2202;</mo><mi>b</mi></mrow></mfrac></math></p><p>能够发现L2正则化项对偏置 b 的更新没有影响，可是对于权重 $w$  的更新有影响：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi><mo>&#x2192;</mo><mi>w</mi><mo>-</mo><mi>&#x3B7;</mi><mfrac><mrow><mo>&#x2202;</mo><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac><mo>-</mo><mfrac><mrow><mi>&#x3B7;</mi><mi>&#x3BB;</mi></mrow><mi>n</mi></mfrac><mi>w</mi></math><br><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>=</mo><mfenced><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mi>&#x3B7;</mi><mi>&#x3BB;</mi></mrow><mi>n</mi></mfrac></mrow></mfenced><mi>w</mi><mo>-</mo><mi>&#x3B7;</mi><mfrac><mrow><mo>&#x2202;</mo><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mo>&#x2202;</mo><mi>w</mi></mrow></mfrac></math><br>这里的<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3B7;</mi></math>、 $n$ 、<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3BB;</mi></math>都是大于0的， 所以 <math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mo>-</mo><mfrac><mrow><mi>&#x3B7;</mi><mi>&#x3BB;</mi></mrow><mi>n</mi></mfrac></math>小于1。因此在梯度下降过程中，权重 $w$ 将逐渐减小，趋向于0但不等于0。这也就是<strong>权重衰减</strong>（weight decay）的由来。</p><p>L2正则化起到使得权重参数 $w$ 变小的效果，为什么能防止过拟合呢？因为更小的权重参数  意味着模型的复杂度更低，对训练数据的拟合刚刚好，不会过分拟合训练数据，从而提高模型的泛化能力。</p><ol start="5"><li>Dropout</li></ol><p>  Dropout是在训练网络时用的一种技巧（trike），相当于在隐藏单元增加了噪声。<strong>Dropout 指的是在训练过程中每次按一定的概率（比如50%）随机地“删除”一部分隐藏单元（神经元）</strong>。所谓的“删除”不是真正意义上的删除，其实就是将该部分神经元的激活函数设为0（激活函数的输出为0），让这些神经元不计算而已。</p><p>  <img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622466578527.png" alt="Dropout"></p><p><font color="#006400">  <strong>Dropout为什么有助于防止过拟合呢？</strong></font></p><p>（a）在训练过程中会产生不同的训练模型，不同的训练模型也会产生不同的的计算结果。随着训练的不断进行，计算结果会在一个范围内波动，但是均值却不会有很大变化，因此可以把最终的训练结果看作是不同模型的平均输出。<br>（b）它消除或者减弱了神经元节点间的联合，降低了网络对单个神经元的依赖，从而增强了泛化能力。</p><blockquote><p><font color="#00008B">理解</font><br>通过加入噪声，在训练模型时，扩展模型的接受范围，避免过拟合</p></blockquote><ol start="6"><li>Early stopping（提前终止）</li></ol><p>对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）。Early stopping是一种迭代次数截断的方法来防止过拟合的方法，<font color="#7FFF00">即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合</font>。</p><p>为了获得性能良好的神经网络，训练过程中可能会经过很多次epoch（遍历整个数据集的次数，一次为一个epoch）。如果epoch数量太少，网络有可能发生欠拟合；如果epoch数量太多，则有可能发生过拟合。Early stopping旨在解决epoch数量需要手动设置的问题。具体做法：每个epoch（或每N个epoch）结束后，在验证集上获取测试结果，随着epoch的增加，如果在验证集上发现测试误差上升，则停止训练，将停止之后的权重作为网络的最终参数。</p><p><font color="#E9967A">为什么能防止过拟合？</font></p><p>当还未在神经网络运行太多迭代过程的时候，w参数[误差]接近于0，因为随机初始化w值的时候，它的值是较小的随机值。当你开始迭代过程，w的值会变得越来越大。到后面时，w的值已经变得十分大了。所以early stopping要做的就是在中间点停止迭代过程。我们将会得到一个中等大小的w参数，会得到与L2正则化相似的结果，选择了w参数较小的神经网络。</p><p><font color="#A52A2A">Early Stopping缺点</font><br><strong>没有采取不同的方式来解决优化损失函数和过拟合这两个问题</strong>，而是用一种方法同时解决两个问题 ，结果就是要考虑的东西变得更复杂。之所以不能独立地处理，因为如果你停止了优化损失函数，你可能会发现损失函数的值不够小，同时你又不希望过拟合。</p>]]></content>
    
    
    <summary type="html">机器学习</summary>
    
    
    
    <category term="machine learning" scheme="https://merlynr.github.io/categories/machine-learning/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="data mining" scheme="https://merlynr.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>算法的性能指标</title>
    <link href="https://merlynr.github.io/2021/05/31/%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/"/>
    <id>https://merlynr.github.io/2021/05/31/%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/</id>
    <published>2021-05-30T16:00:00.000Z</published>
    <updated>2021-05-30T16:00:00.000Z</updated>
    
    
    <summary type="html">=</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>二分类之类别不平衡</title>
    <link href="https://merlynr.github.io/2021/05/31/%E4%BA%8C%E5%88%86%E7%B1%BB%E4%B9%8B%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1/"/>
    <id>https://merlynr.github.io/2021/05/31/%E4%BA%8C%E5%88%86%E7%B1%BB%E4%B9%8B%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1/</id>
    <published>2021-05-30T16:00:00.000Z</published>
    <updated>2021-06-08T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/massquantity/p/8550875.html">机器学习之类别不平衡问题 (1) —— 各种评估指标 - massquantity - 博客园</a><br><a href="https://www.zhihu.com/question/269698662">欠采样（undersampling）和过采样（oversampling）会对模型带来怎样的影响？</a><br><a href="https://www.cnblogs.com/inchbyinch/p/12642760.html">详解类别不平衡问题 - 天地辽阔 - 博客园</a></p><h2 id="类别不平衡-class-imbalance"><a href="#类别不平衡-class-imbalance" class="headerlink" title="类别不平衡(class-imbalance)"></a>类别不平衡(class-imbalance)</h2><blockquote><p><font color="#D2691E"> 惯例</font><br>在二分类问题中，一般将数目少的类别视为正例，数目多的类别视为负例</p></blockquote><p><font color="#228B22">也叫数据倾斜，数据不平衡指分类任务中不同类别的训练样例数目差别很大的情况。</font></p><h2 id="各种评估指标"><a href="#各种评估指标" class="headerlink" title="各种评估指标"></a>各种评估指标</h2><p><a href="https://www.cnblogs.com/massquantity/p/8550875.html">机器学习之类别不平衡问题 (1) —— 各种评估指标 - massquantity - 博客园</a></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202167/1623024781731.png" alt="混淆矩阵图"></p><ul><li>True Positive(真正例，TP)：实例为正例，预测为正例</li><li>False Negative (假负例，FN)：实际为正例，预测为负例。</li><li>True Negative (真负例，TN)：实际为负例，预测为负例。</li><li>False Positive (假正例，FP)：实际为负例，预测为正例。</li></ul><ol><li>Precision (查准率) = <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math> ，Precision衡量的是<font color="#7FFF00">所有被预测为正例的样本中有多少是真正例</font>。<font color="#A52A2A">但Precision并没有表现有多少正例是被错判为了负例(即FN)</font>，举个极端的例子，分类器只将一个样本判为正例，其他所有都判为负例，这种情况下Precision为100%，但其实遗漏了很多正例，所以Precision常和下面的Recall (TPR) 相结合。</li><li>True Positive Rate (TPR，真正例率) = <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math> ，又称__Recall__(查全率,召回率)，Sensitivity(灵敏性)。Recall (TPR)衡量的是所有的正例中有多少是被<font color="#008B8B">正确分类</font>了，也可以看作是为了<font color="#057748">避免假负例(FN)的发生</font>，<font color="#0000FF">即将真正例分类到真正中而不是通过假负来判断的</font>，因为TPR高意味着FN低。Recall的问题和Precision正相反，没有表现出有多少负例被错判为正例(即FP)，若将所有样本全划为正例，则Recall为100%，但这样也没多大用。</li><li>True Negative Rate (TNR，真负例率) = <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math> ，又称Specificity(特异性)。Specificity衡量的是<font color="#006400">所有的负例中有多少是被正确分类</font>了，由于<font color="#1E90FF">类别不平衡问题中通常关注正例能否正确被识别，Specificity高则FP低，意味着很少将正例错判为负例，即该分类器对正例的判别具有“特异性”，在预测为正例的样本中很少有负例混入</font>。</li><li>False Positive Rate (FPR，假正例率) = <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></math> = 1− $TNR$ , 由混淆矩阵可以看出该指标的<font color="#D2691E">着眼点</font>在于负例，意为有多少负例被错判成了正例。在ROC曲线中分别以TPR和FPR作为纵、横轴作图，显示出一种正例与负例之间的“<font color="#9400D3">博弈</font>”，在下篇文章中详解。</li></ol><p>F1 score = <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mn>2</mn><mrow><mfrac><mn>1</mn><mtext>&#xA0;recall&#xA0;</mtext></mfrac><mo>+</mo><mfrac><mn>1</mn><mtext>&#xA0;precision&#xA0;</mtext></mfrac></mrow></mfrac><mo>=</mo><mfrac><mrow><mn>2</mn><mo>&#xD7;</mo><mtext>&#xA0;precision&#xA0;</mtext><mo>&#xD7;</mo><mtext>&#xA0;recall&#xA0;</mtext></mrow><mrow><mtext>&#xA0;precision&#xA0;</mtext><mo>+</mo><mtext>&#xA0;recall&#xA0;</mtext></mrow></mfrac></math></p><p>F1分数（F1-Score），又称为平衡F分数（BalancedScore），是一个综合指标,它被定义为精确率和召回率的调和平均数 (harmonic mean),数值上一般接近于二者中的<font color="#1E90FF">较小值</font>，因此如果F1 score比较高的话，意味着Precision和Recall都较高。</p><blockquote><p><font color="#7FFF00"> 知识补充</font><br>调和平均数（harmonic mean）又称倒数平均数，是总体各统计变量倒数的算术平均数的倒数。调和平均数是平均数的一种。<br>算数平均数中，重要性取决于绝对值大的一方（强），而在调和平均数中，<font color="#057748">重要性</font>取决于<font color="#8B0000">绝对值小的一方</font>（弱）。</p></blockquote><p>FP和FN还有个还有个与之相关的概念，那就是统计假设检验中的<font color="#483D8B">第一类错误</font> (Type I error)和<font color="#483D8B">第二类错误 (Type II error)</font> 。由于我们比较关心正例，所以将负例视为零假设，正例视为备选假设，则第一类错误为错误地拒绝零假设 (负例)，选择备选假设，则为FP；第二类错误为错误地接受零假设，则为FN。</p><blockquote><p><font color="#006400">知识补充</font><br>零假设的内容一般是希望证明其错误的假设。</p></blockquote><hr><p>上面介绍的这些指标都没有考虑检索结果的先后顺序，而像搜索问题中我们通常希望第一个结果是与查询最相关的，第二个则是次相关的，以此类推，因而有时候不仅要预测准确，<font color="#6495ED">对于相关性的顺序也非常看重</font>。所以最后介绍两个广泛应用的<font color="#9400D3">排序指标</font>。</p><p>Mean Average Precision (MAP，平均准确率均值)，对于<font color="#B8860B">单个</font>信息需求，返回结果中在每篇相关文档上 Precision 的平均值被称为 Average Precision (AP)，然后对<font color="#D2691E">所有</font>查询取平均得到 MAP。<br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>AP</mi><mo>=</mo><mfrac><mrow><msubsup><mo>&#x2211;</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi>P</mi><mo>(</mo><mi>k</mi><mo>)</mo><mo>&#xD7;</mo><mo>rel</mo><mo>(</mo><mi>k</mi><mo>)</mo></mrow><mi>M</mi></mfrac></math><br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>MAP</mi><mo>=</mo><munderover><mo>&#x2211;</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mfrac><msub><mi>AP</mi><mi>q</mi></msub><mi>Q</mi></mfrac></math><br>其中 P(k) 为前 k 个结果的 Precision，又可写为P@k。 rel(k) 表示第 k 个结果是否为相关文档，相关为1不相关为0，M 表示所有相关文档的数量，n 表示所有文档数量。如果只关心<font color="#00008B">前 K 个查询的情况</font>，则是下式：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>AP</mi><mo>@</mo><mi>K</mi><mo>=</mo><mfrac><mrow><msubsup><mo>&#x2211;</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mi>P</mi><mo>(</mo><mi>k</mi><mo>)</mo><mo>&#xD7;</mo><mo>rel</mo><mo>(</mo><mi>k</mi><mo>)</mo></mrow><msub><mi>M</mi><mi>K</mi></msub></mfrac></math><br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>MAP</mi><mo>@</mo><mi>K</mi><mo>=</mo><munderover><mo>&#x2211;</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mfrac><mrow><msub><mi>AP</mi><mi>q</mi></msub><mo>@</mo><mi>K</mi></mrow><mi>Q</mi></mfrac></math><br>这里的 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>M</mi><mi>K</mi></msub></math> 为前 K 个结果中相关文档的数量。</p><p>对于单个信息需求来说，Average Precision 是<font color="#bf242a"> PR 曲线</font>下面积的近似值，因此 MAP 可粗略地认为是某个查询集合对应的多条 PR 曲线下面积的平均值。</p><p><strong>Normalized Discounted Cumulative Gain</strong> (NDCG，归一化折扣累计增益) 。如果说 <font color="#0000FF">MAP 是基于 0/1 二值描述相关性</font>，那么 <font color="#9932CC">NDCG 则是可将相关性分为多个等级的指标</font>。<br>对于信息检索和推荐之类的问题，每一个返回的结果都被赋予一个相关性分数 rel，则 NDCG 中的 CG 表示前 k 个结果的分数之和，即累计增益 ：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>CG</mi><mi>k</mi></msub><mo>=</mo><munderover><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>rel</mi><mi>i</mi></msub></math></p><p>CG 没有考虑推荐的次序，所以在此基础上引入对结果顺序的考虑，即<font color="#DC143C">相关性高的结果</font>若排在后面则会受更多的惩罚，于是就有了 DCG (discounted CG)，折扣累积增益。公式如下：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>DCG</mi><mi>k</mi></msub><mo>=</mo><munderover><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mfrac><mrow><msup><mn>2</mn><msub><mi>rel</mi><mi>i</mi></msub></msup><mo>-</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></mfrac></math></p><p>i 表示一个结果在结果集中的顺序，如果该结果 rel 很高，但排在后面，意味着分母 log2(i+1) 会变大，则相应的总体 DCG 会变小 (注意这里的 log 是以 2 为底的)。</p><p>对于不同的查询，往往会返回不同的结果集，而不同结果集之间因为大小不同难以直接用 DCG 进行比较，所以需要进行<font color="#006400">归一化</font>，这其实和机器学习中不同特征因量纲不同要进行归一化差不多意思。这个归一化后的指标就是 NDCG ：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>NDCG</mi><mi>k</mi></msub><mo>=</mo><mfrac><msub><mi>DCG</mi><mi>k</mi></msub><msub><mi>IDCG</mi><mi>k</mi></msub></mfrac></math><br>其中 IDCG 表示 Ideal DCG， 指<font color="#006400">某个查询所能返回的最好结果集</font>，IDCG 的值也是结果集中最大的。将所有结果按相关性大小排序，计算出的 DCG 即为前 k 个结果的 IDCG：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>IDCG</mi><mi>k</mi></msub><mo>=</mo><munderover><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo>|</mo><mi>R</mi><mi>E</mi><mi>L</mi><mo>|</mo></mrow></munderover><mfrac><mrow><msup><mn>2</mn><msub><mi>rel</mi><mi>i</mi></msub></msup><mo>-</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></mfrac></math><br>其中 |REL| 表示按相关性顺序排列的结果集。因此 DCG 的值介于 (0, IDCG] ，故 NDCG 的值介于(0,1]，这样就起到了归一化的效果。不同查询或用户的 NDCG 平均起来可以用以评估一个搜索引擎或推荐系统的整体效果。</p><p>NDCG 的缺点是<font color="#483D8B">需要预先指定每一个返回结果的相关性</font>，这个超参数需要人为指定。</p><h2 id="常用的评估方法"><a href="#常用的评估方法" class="headerlink" title="常用的评估方法"></a>常用的评估方法</h2><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>ROC曲线常用于二分类问题中的模型比较，主要表现为一种<font color="#0000FF">真正例率 (TPR) </font>和<font color="#0000FF">假正例率 (FPR) </font>的权衡。</p><p><strong><font color="#ff7500">概述：</font></strong> 是在不同的分类阈值 (threshold) 设定下分别以TPR和FPR为纵、横轴作图。由ROC曲线的两个指标，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mi>P</mi></mfrac><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mi>N</mi></mfrac><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow></mfrac></math> 可以看出，当一个样本被分类器判为正例，若其本身是正例，则TPR增加；若其本身是负例，则FPR增加，因此ROC曲线可以看作是随着阈值的不断移动，所有样本中正例与负例之间的“对抗”。曲线越靠近左上角，意味着<font color="#FF1493">越多的正例优先于负例，模型的整体表现也就越好</font>。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202167/1623072321656.png" alt="ROC曲线"></p><p> <strong><font color="#008B8B">AUC (Area Under the Curve)</font></strong></p><p> <img src="https://gitee.com/merlynr/img-store/raw/master/202167/1623073223653.png" alt="ROC space"></p><p>先看一下ROC曲线中的随机线，图中[0,0]到[1,1]的虚线即为随机线，该线上所有的点都<font color="#00FFFF">表示该阈值下TPR=FPR</font><br>根据定义，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mi>P</mi></mfrac></math>，表示所有正例中被预测为正例的概率；<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mi>N</mi></mfrac></math>，表示所有负例中被被预测为正例的概率。<font color="#B8860B">若二者相等，意味着无论一个样本本身是正例还是负例，分类器预测其为正例的概率是一样的，这等同于随机猜测</font>（注意这里的“随机”不是像抛硬币那样50%正面50%反面的那种随机）。</p><p>上图中B点就是一个随机点，无论是样本数量和类别如何变化，始终将75%的样本分为正例。</p><p><font color="#B8860B">ROC曲线围成的面积 (即AUC)可以解读为</font>：从所有正例中随机选取一个样本A，再从所有负例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。可以看到位于随机线上方的点(如图中的A点)被认为好于随机猜测。在这样的点上TPR总大于FPR，意为正例被判为正例的概率大于负例被判为正例的概率。<br>从另一个角度看，由于画ROC曲线时都是先将所有样本按分类器的<font color="#1E90FF">预测概率</font>排序，<font color="#B22222">所以AUC反映的是分类器对样本的排序能力</font>，依照上面的例子就是A排在B前面的概率。<font color="#008B8B">AUC越大，自然排序能力越好</font>，即分类器将越多的正例排在负例之前。</p><p><font color="#8B0000">ROC曲线的绘制方法</font>：假设有P个正例，N个反例，首先拿到分类器对于每个样本预测为正例的概率，根据概率对所有样本进行<font color="#006400">逆序排列</font>，然后将<font color="#0000FF">分类阈值设为最大</font>，即把所有样本均预测为反例，此时图上的点为 (0,0)。然后将分类阈值依次设为每个样本的预测概率，即依次将每个样本划分为正例，如果该样本为真正例，则TP+1，即<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mi>P</mi><mi>R</mi><mo>+</mo><mfrac><mn>1</mn><mi>P</mi></mfrac></math>; 如果该样本为负例，则FP+1，即<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mi>P</mi><mi>R</mi><mo>+</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></math>。最后的到所有样本点的TPR和FPR值，用线段相连。</p><blockquote><p><a href="https://github.com/massquantity/Class-Imbalance/tree/master/">massquantity/Class-Imbalance: 《机器学习之类别不平衡问题》文章代码</a></p></blockquote><h4 id="ROC的优点"><a href="#ROC的优点" class="headerlink" title="ROC的优点"></a>ROC的优点</h4><p><img src="https://gitee.com/merlynr/img-store/raw/master/202168/1623131643148.png" alt="混淆矩阵图"></p><ol><li><p>兼顾正例和负例的权衡。因为TPR聚焦于正例，FPR聚焦于与负例，使其成为一个比较均衡的评估方法。</p></li><li><p>ROC曲线选用的两个指标，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mi>P</mi></mfrac><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></math>，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mi>N</mi></mfrac><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow></mfrac></math>，都不依赖于具体的类别分布。</p><p> 注意TPR用到的TP和FN同属<font color="#FF1493">P</font>列，FPR用到的FP和TN同属<font color="#1E90FF">N</font>列，<font color="#7FFF00">所以即使P或N的整体数量发生了改变，也不会影响到另一列</font>。也就是说，即使正例与负例的比例发生了很大变化，ROC曲线也不会产生大的变化，而像Precision使用的TP和FP就分属两列，则易受类别分布改变的影响。</p></li></ol><p><a href="https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf">参考文献</a>中举了个例子，负例增加了10倍，ROC曲线没有改变，而PR曲线则变了很多。作者认为这是ROC曲线的优点，即具有<font color="#0000FF">鲁棒性</font>，在类别分布发生明显改变的情况下依然能客观地识别出较好的分类器。</p><blockquote><p><font color="#006400">代码验证</font><br><a href="https://www.cnblogs.com/massquantity/p/8592091.html">相关资料</a></p></blockquote><h4 id="ROC的缺点"><a href="#ROC的缺点" class="headerlink" title="ROC的缺点"></a>ROC的缺点</h4><ol><li>上文提到ROC曲线的优点是不会随着类别分布的改变而改变，但这在某种程度上也是其缺点。因为负例N增加了很多，而曲线却没变，这等于产生了大量FP。像信息检索中如果主要关心正例的预测准确性的话，这就不可接受了。</li><li>在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分乐观的效果估计。ROC曲线的横轴采用FPR，根据<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>FPR</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mi>N</mi></mfrac><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow></mfrac></math>，当负例N的数量远超正例P时，FP的大幅增长只能换来FPR的微小改变。<font color="#B22222">结果是虽然大量负例被错判成正例，在ROC曲线上却无法直观地看出来。</font>（当然也可以只分析ROC曲线左边一小段）<br>举个例子，假设一个数据集有正例20，负例10000，开始时有20个负例被错判，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mn>20</mn><mrow><mn>20</mn><mo>+</mo><mn>9980</mn></mrow></mfrac><mo>=</mo><mn>0</mn><mo>.</mo><mn>002</mn></math>，接着又有20个负例错判，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mi>P</mi><msub><mi>R</mi><mn>2</mn></msub><mo>=</mo><mfrac><mn>40</mn><mrow><mn>40</mn><mo>+</mo><mn>9960</mn></mrow></mfrac><mo>=</mo><mn>0</mn><mo>.</mo><mn>004</mn></math>，在ROC曲线上这个变化是很细微的。而与此同时Precision则从原来的0.5下降到了0.33，在PR曲线上将会是一个大幅下降。</li></ol><h3 id="PR-Precision-Recall-曲线"><a href="#PR-Precision-Recall-曲线" class="headerlink" title="PR(Precision Recall)曲线"></a>PR(Precision Recall)曲线</h3><p>PR曲线展示的是Precision vs Recall的曲线，PR曲线与ROC曲线的相同点是都采用了TPR (Recall)，都可以用AUC来衡量分类器的效果。不同点是ROC曲线使用了FPR，而PR曲线使用了Precision，因此<font color="#8A2BE2">PR曲线的两个指标都聚焦于正例</font>。<font color="#8A2BE2">类别不平衡问题中由于主要关心正例</font>，所以在此情况下PR曲线被广泛认为<font color="#FF8C00">优于</font>ROC曲线。</p><p>PR曲线的绘制与ROC曲线类似，PR曲线的AUC面积计算公式为：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><munder><mo>&#x2211;</mo><mi>n</mi></munder><mo>(</mo><msub><mi>R</mi><mi>n</mi></msub><mo>-</mo><msub><mi>R</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>)</mo><msub><mi>P</mi><mi>n</mi></msub></math></p><blockquote><p><a href="https://github.com/massquantity/Class-Imbalance/tree/master/">massquantity/Class-Imbalance: 《机器学习之类别不平衡问题》文章代码</a></p></blockquote><p><strong><font color="#FF00FF">使用场景</font></strong></p><ol><li>ROC曲线由于<font color="#1E90FF">兼顾</font>正例与负例，所以适用于评估分类器的<font color="#B22222">整体性</font>能，相比而言PR曲线完全聚焦于<font color="#FF00FF">正例</font>。</li><li>如果有多份数据且存在不同的类别分布，比如信用卡欺诈问题中每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合，因为<font color="#1E90FF">类别分布改变</font>可能使得PR曲线发生变化时好时坏，这种时候难以进行模型比较；反之，如果想<font color="#FF00FF">测试不同类别分布下对分类器的性能</font>的影响，则PR曲线比较适合。</li><li>如果想要评估在相同的类别分布下正例的预测情况，则宜选PR曲线。</li><li>类别不平衡问题中，ROC曲线通常会给出一个乐观的效果估计，所以大部分时候还是PR曲线更好。</li><li>最后可以根据具体的应用，在曲线上找到最优的点，得到相对应的precision，recall，f1 score等指标，去调整模型的阈值，从而得到一个符合具体应用的模型。</li></ol><h2 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h2><blockquote><p><font color="#FF1493">前提：</font>章节二三主要谈的是类别不平衡的评估指标，因此我们可以选择选择具体的类别不平衡问题的方法。</p></blockquote><p>采样方法大致可分为<font color="#00CED1">过采样 (oversampling)</font> 和<font color="#2F4F4F">欠采样 (undersampling) </font>，虽然过采样和降采样主题思想简单，但这些年来研究出了很多变种，本篇挑一些来具体阐述。见下思维导图：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202168/1623137294136.png" alt="采样方法"></p><h3 id="过采样"><a href="#过采样" class="headerlink" title="过采样"></a>过采样</h3><ol><li>随机过采样</li></ol><p>随机过采样顾名思义就是从样本少的类别中随机抽样，再将抽样得来的样本添加到数据集中。然而这种方法如今已经不大使用了，因为重复采样往往会导致<font color="#1E90FF">严重的过拟合</font>，因而现在的主流过采样方法是通过某种方式人工合成一些少数类样本，从而达到类别平衡的目的，而这其中的鼻祖就是SMOTE。</p><ol start="2"><li>SMOTE</li></ol><p>SMOTE (synthetic minority oversampling technique) 的思想概括起来就是在<font color="#00FFFF">少数类</font>样本之间进行插值来产生额外的样本。具体地，对于一个少数类样本<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">X</mi><mi>i</mi></msub></math>使用K近邻法(k值需要提前指定)，求出离<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">X</mi><mi>i</mi></msub></math>距离最近的k个少数类样本，其中距离定义为样本之间n维特征空间的欧氏距离。然后从k个近邻点中随机选取一个，使用下列公式生成新样本：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">x</mi><mtext>new&#xA0;</mtext></msub><mo>=</mo><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub><mo>+</mo><mfenced><mrow><msub><mover><mi mathvariant="bold">x</mi><mo>^</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub></mrow></mfenced><mo>&#xD7;</mo><mi>&#x3B4;</mi></math><br>其中 <math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi mathvariant="bold">X</mi><mo>^</mo></mover></math> 为选出的k近邻点，δ∈[0,1]是一个随机数。下图就是一个SMOTE生成样本的例子，使用的是3-近邻，可以看出SMOTE生成的样本一般就在<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub></math>和<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mover><mi mathvariant="bold">x</mi><mo>^</mo></mover><mi>i</mi></msub></math>相连的直线上：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623220386942.png" alt="SMOTE生成的样本"></p><p>SMOTE会随机选取少数类样本用以合成新样本，而不考虑周边样本的情况，这样容易带来两个<font color="#FF1493">问题</font>：</p><ol><li>如果选取的少数类样本周围也都是少数类样本，则新合成的样本不会提供太多有用信息。这就像支持向量机中远离margin的点对决策边界影响不大。</li><li>如果选取的少数类样本周围都是多数类样本，这类的样本可能是噪音，则新合成的样本会与周围的多数类样本产生大部分重叠，致使分类困难。</li></ol><p>总的来说我们希望新合成的少数类样本能处于两个类别的边界附近，这样往往能提供足够的信息用以分类。而这就是下面的 <strong>Border-line SMOTE</strong> 算法要做的事情。</p><blockquote><p><font color="#bf242a">知识补充</font><a href="https://blog.csdn.net/lemonaha/article/details/53410465#31-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95">k近邻法–统计学习方法总结_lemonaha的博客-CSDN博客</a><br> k近邻法（k-nearest neighbor,<font color="#0000FF"> k-NN</font>）是一种基本分类与回归方法。这里只讨论分类问题中的k近邻法。k近邻法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方法进行预测。因此，k近邻法不具有显式的学习过程。k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。**<font color="#1E90FF">k值的选择、距离度量及分类决策规则是k近邻法的三个基本要素</font>**。</p></blockquote><ol start="3"><li>Border-line SMOTE</li></ol><p>这个算法会先将所有的少数类样本分成三类，如下图所示：</p><ul><li>“noise” ： 所有的k近邻个样本都属于多数类</li><li>“danger” ： 超过一半的k近邻样本属于<font color="#0000FF">多</font>数类</li><li>“safe”： 超过一半的k近邻样本属于<font color="#0000FF">少</font>数类</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623220611894.png" alt="Border-line SMOTE"></p><p>  <font color="#ff7500">Border-line SMOTE</font>算法只会从处于”<em>danger</em>“状态的样本中随机选择，然后用SMOTE算法产生新的样本。处于”danger“状态的样本代表靠近”边界“附近的少数类样本，而处于边界附近的样本往往更<font color="#B8860B">容易被误分类</font>。因而 Border-line SMOTE 只对那些靠近”边界“的少数类样本进行人工合成样本，而 SMOTE 则对所有少数类样本一视同仁。</p><p>Border-line SMOTE 分为两种: Borderline-1 SMOTE 和 Borderline-2 SMOTE。 Borderline-1 SMOTE 在合成样本时,是式中的<math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi mathvariant="bold">x</mi><mo>^</mo></mover></math>是一个<font color="#1E90FF">少数类样本</font>，而 Borderline-2 SMOTE 中的<math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi mathvariant="bold">x</mi><mo>^</mo></mover></math>则是k近邻中的<font color="#FF1493">任意</font>一个样本。</p><ol start="4"><li>ADASYN</li></ol><p><font color="#8B008B">ADASYN</font>名为自适应合成抽样(adaptive synthetic sampling)，其最大的特点是<font color="#006400">采用某种机制自动决定每个少数类样本需要产生多少合成样本</font>，而不是像SMOTE那样对每个少数类样本合成同数量的样本。具体流程如下：</p><ol><li><p>首先计算需要合成的样本总量：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi><mo>=</mo><mfenced><mrow><msub><mi>S</mi><mrow><mi>m</mi><mi>a</mi><mi>j</mi></mrow></msub><mo>-</mo><msub><mi>S</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced><mo>&#xD7;</mo><mi>&#x3B2;</mi></math><br>其中<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>S</mi><mrow><mi>m</mi><mi>a</mi><mi>j</mi></mrow></msub></math>为多数类样本数量，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>S</mi><mi>min</mi></msub></math>为少数类样本数量，β∈[0,1]为系数。G即为总共想要<font color="#8A2BE2">合成的少数类样本数量</font>，如果β=1则是合成后各类别数目相等。</p></li><li><p>对于每个少类别样本xi，找出其K近邻个点，并计算：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x393;</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>&#x394;</mi><mi>i</mi></msub><mo>/</mo><mi>K</mi></mrow><mi>Z</mi></mfrac></math><br>其中Δi为K近邻个点中多数类样本的数量，Z为规范化因子以确保 Γ 构成一个分布。这样若一个少数类样本<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub></math>的周围多数类样本越多，则其 Γi 也就越高。</p></li><li><p>最后对每个少类别样本<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub></math>计算需要合成的样本数量<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">g</mi><mi>i</mi></msub></math>，再用SMOTE算法合成新样本：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>g</mi><mi>i</mi></msub><mo>=</mo><msub><mi>&#x393;</mi><mi>i</mi></msub><mo>&#xD7;</mo><mi>G</mi></math><br>可以看到ADASYN利用分布Γ来自动决定每个少数类样本所需要合成的样本数量，这等于是给每个少数类样本施加了一个权重，周围的多数类样本越多则权重越高。ADASYN的缺点是<font color="#A52A2A">易受离群点的影响</font>，如果一个少数类样本的K近邻都是多数类样本，则其权重会变得相当大，进而会在其周围生成较多的样本。</p></li></ol><p>下面利用sklearn中的 <em>make_classification</em> 构造了一个不平衡数据集，各类别比例为{0:54, 1:946}。原始数据，SMOTE，Borderline-1 SMOTE，Borderline-2 SMOTE和ADASYN的比较见下图，<font color="#0000FF">左侧为过采样后的决策边界</font>，<font color="#8B008B">右侧为过采样后的样本分布情况</font>，<font color="#B8860B">可以看到过采样后原来少数类的决策边界都扩大了，导致更多的多数类样本被划为少数类了</font>：</p><blockquote><p><font color="#0000FF">知识补充</font><br>决策边界顾名思义就是需要分类的数据中，区分不同类别的边界。</p></blockquote><pre><code>    原始数据</code></pre><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623222264899.png" alt="原始数据"><br>        SMOTE<br><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623222317451.png" alt="SMOTE过采样"><br>        Borderline-1 SMOTE<br><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623222382482.png" alt="Borderline-1 SMOTE"><br>        Borderline-2 SMOTE<br><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623222404250.png" alt="Borderline-2 SMOTE"><br>        ADASYN<br><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623222425227.png" alt="ADASYN"></p><p>从上图我们也可以比较几种过采样方法各自的特点。用 <code>SMOTE</code> 合成的样本分布比较平均，而<code>Border-line SMOTE</code>合成的样本则集中在类别边界处。<code>ADASYN</code>的特性是一个少数类样本周围多数类样本越多，则算法会为其生成越多的样本，从图中也可以看到生成的样本大都来自于原来与多数类比较靠近的那些少数类样本。</p><h3 id="欠采样"><a href="#欠采样" class="headerlink" title="欠采样"></a>欠采样</h3><ol><li><p>随机欠采样</p><p> 随机欠采样的思想同样比较简单，就是从多数类样本中随机选取一些剔除掉。这种方法的缺点是<font color="#0000FF">被剔除的样本可能包含着一些重要信息</font>，致使学习出来的模型效果不好。</p></li><li><p>EasyEnsemble 和 BalanceCascade<br>  EasyEnsemble和BalanceCascade采用集成学习机制来<font color="#8A2BE2">处理传统随机欠采样中的信息丢失</font>问题。</p></li></ol><ul><li>EasyEnsemble将多数类样本随机<font color="#00FFFF">划分成n个子集</font>，每个子集的数量等于少数类样本的数量，这相当于欠采样。接着将每个子集与少数类样本结合起来分别训练一个模型，最后将n个模型集成，这样虽然每个子集的样本少于总体样本，但集成后总信息量并不减少。</li><li>如果说EasyEnsemble是基于无监督的方式从多数类样本中生成子集进行欠采样，那么BalanceCascade则是采用了<font color="#7FFFD4">有监督</font>结合Boosting的方式。在第n轮训练中，将从多数类样本中抽样得来的子集与少数类样本结合起来训练一个基学习器H，训练完后多数类中能被H正确分类的样本会被剔除。在接下来的第n+1轮中，从被剔除后的多数类样本中产生子集用于与少数类样本结合起来训练，最后将不同的基学习器集成起来。BalanceCascade的有监督表现在每一轮的基学习器起到了在多数类中选择样本的作用，而其Boosting<font color="#bf242a">特点则体现在每一轮丢弃被正确分类的样本，进而后续基学习器会更注重那些之前分类错误的样本。</font></li></ul><blockquote><p><font color="#0000FF">知识补充</font>基学习器<br><a href="https://www.biaodianfu.com/boosting.html">机器学习算法之Boosting – 标点符</a><br>同质集成中的个体学习器又称为基学习器（base learner），相应的学习算法也被称为基学习算法（base learning algorithm）。</p></blockquote><ol start="3"><li>NearMiss</li></ol><p><font color="#725e82"><strong>NearMiss</strong></font>本质上是一种<font color="#BDB76B">原型选择</font>(prototype selection)方法，即从多数类样本中选取最具代表性的样本用于训练，主要是为了缓解随机欠采样中的信息丢失问题。NearMiss采用一些<font color="#A52A2A">启发式的规则</font>来选择样本，根据规则的不同可分为3类：</p><ul><li>NearMiss-1：选择到最近的K个少数类样本平均距离最近的多数类样本</li><li>NearMiss-2：选择到最远的K个少数类样本平均距离最近的多数类样本</li><li>NearMiss-3：对于每个少数类样本选择K个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围</li></ul><p>NearMiss-1和NearMiss-2的计算<font color="#0000FF">开销很大</font>，因为需要计算每个多类别样本的K近邻点。另外，NearMiss-1易受离群点的影响，如下面第二幅图中合理的情况是处于边界附近的多数类样本会被选中，然而由于右下方一些少数类离群点的存在，其附近的多数类样本就被选择了。相比之下NearMiss-2和NearMiss-3不易产生这方面的问题。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623224933174.png" alt="图一Oniginal data"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623225034300.png" alt="图二Resampling using Nearmiss-1"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623225086130.png" alt="图三Resampling using Nearmiss-2"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623225124104.png" alt="图四Resampling using Nearmiss-3"></p><ol start="4"><li>数据清洗方法 (data cleaning tichniques)</li></ol><p>这类方法主要<font color="#8A2BE2">通过某种规则来清洗重叠的数据</font>，从而达到欠采样的目的，而这些规则往往也是启发性的，下面进行简要阐述：</p><ul><li><p><font color="#ff7500">Tomek Link</font>：Tomek Link表示<font color="#7FFFD4">不同类别</font>之间距离最近的一对样本，即<font color="#bf242a">这两个样本互为最近邻且分属不同类别</font>。这样如果两个样本形成了一个Tomek Link，则要么其中一个是噪音，要么两个样本都在边界附近。这样通过移除Tomek Link就能“清洗掉”类间重叠样本，使得互为最近邻的样本皆属于同一类别，从而能更好地进行分类。</p><pre><code>  下图一上为原始数据，图二上为SMOTE后的数据，图三虚线标识出Tomek Link，图四为移除Tomek Link后的数据集，可以看到不同类别之间样本重叠减少了很多。</code></pre></li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623226151018.png" alt="图一"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623226162443.png" alt="图二"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623226175192.png" alt="图三"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623226186988.png" alt="图四"></p><ul><li><font color="#ff7500"> Edited Nearest Neighbours(ENN)</font>：对于属于多数类的一个样本，如果其K个近邻点有超过一半都不属于多数类，则这个样本会被剔除。这个方法的另一个变种是所有的K个近邻点都不属于多数类，则这个样本会被剔除。、</li></ul><p>最后，数据清洗技术<font color="#0000FF">最大的缺点</font>是无法控制欠采样的数量。由于都在某种程度上采用了K近邻法，而事实上大部分多数类样本周围也都是多数类，因而能剔除的多数类样本比较有限。</p><h3 id="过采样和欠采样结合"><a href="#过采样和欠采样结合" class="headerlink" title="过采样和欠采样结合"></a>过采样和欠采样结合</h3><p>上文中提到SMOTE算法的缺点是生成的少数类样本容易与周围的多数类样本产生重叠难以分类，而数据清洗技术恰好可以处理掉重叠样本，所以可以将二者结合起来形成一个pipeline，先过采样再进行数据清洗。主要的方法是 <code>SMOTE + ENN</code> 和 <code>SMOTE + Tomek</code> ，其中 <code>SMOTE + ENN</code> 通常能清除更多的重叠样本，如下图：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623227628385.png" alt="Resampling using Original"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623227661187.png" alt="Resampling using SMOTE"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623227708226.png" alt="Resampling using SMOTE + ENN"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202169/1623227766137.png" alt="Resampling using SMOTE + TOMEK"></p><hr><p><strong><font color="#DC143C">★ 采样方法的效果</font></strong></p><p><a href="https://www.cnblogs.com/massquantity/p/9382710.html">机器学习之类别不平衡问题 (3) —— 采样方法 - massquantity - 博客园</a></p><h2 id="省心的方法"><a href="#省心的方法" class="headerlink" title="省心的方法"></a>省心的方法</h2><h3 id="主动收集数据"><a href="#主动收集数据" class="headerlink" title="主动收集数据"></a>主动收集数据</h3><p>针对少量样本数据，可以尽可能去扩大这些少量样本的数据集，或者尽可能去增加他们特有的特征来丰富数据的多样性（尽量转化成情况1）。譬如，如果是一个情感分析项目，在分析数据比例时发现负样本（消极情感）的样本数量较少，那么我们可以尽可能在网站中搜集更多的负样本数量，或者花钱去买，毕竟数据少了会带来很多潜在的问题。</p><h3 id="将任务转换成异常检测问题、"><a href="#将任务转换成异常检测问题、" class="headerlink" title="将任务转换成异常检测问题、"></a>将任务转换成异常检测问题、</h3><p>如果少数类样本太少，少数类的结构可能并不能被少数类样本的分布很好地表示，那么用平衡数据或调整算法的方法不一定有效。如果这些少数类样本在特征空间中再分布的比较散，情况会更加糟糕。这时候不如将其转换为无监督的异常检测算法，不用过多的去考虑将数据转换为平衡问题来解决。</p><h3 id="调整权重"><a href="#调整权重" class="headerlink" title="调整权重"></a>调整权重</h3><p>可以简单的设置损失函数的权重，让模型增加对多数类的惩罚，更多的关注少数类。在python的scikit-learn中我们可以使用class_weight参数来设置权重。</p><p>另外，调整权重方法也适合于这种情况：不同类型的错误所造成的后果不同。例如在医疗诊断中，错误地把健康人诊断为患者可能会带来进一步检查的麻烦，但是错误地把患者诊断为健康人，则可能会丧失了拯救生命的最佳时机；再如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故；在信用卡盗用检查中，将正常使用误认为是盗用，可能会使用户体验不佳，但是将盗用误认为是正常使用，会使用户承受巨大的损失。为了权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”（unequal cost）。</p><h3 id="阈值调整（threshold-moving）"><a href="#阈值调整（threshold-moving）" class="headerlink" title="阈值调整（threshold moving）"></a>阈值调整（threshold moving）</h3><p>直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将原本默认为0.5的阈值调整到 <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mo>|</mo><mi>P</mi><mo>|</mo></mrow><mrow><mo>(</mo><mo>|</mo><mi>P</mi><mo>|</mo><mo>+</mo><mo>|</mo><mi>N</mi><mo>|</mo><mo>)</mo></mrow></mfrac></math>即可。（大部分是负样本，因此分类器倾向于给出较低的分数）</p><h2 id="类别不平横影响模型的输出"><a href="#类别不平横影响模型的输出" class="headerlink" title="类别不平横影响模型的输出"></a>类别不平横影响模型的输出</h2><p>许多模型的输出是基于阈值的，大部分模型的默认阈值为输出值的中位数。比如逻辑回归的输出范围为[0,1]，当某个样本的输出大于0.5就会被划分为正例，反之为反例。在数据的类别不平衡时，采用默认的分类阈值可能会导致输出全部为反例，产生虚假的高准确度，导致分类失败。</p>]]></content>
    
    
    <summary type="html">目中出现了二分类数据不平横问题，研究总结下对于类别不平横问题的处理经验</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="data mining" scheme="https://merlynr.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>kernal</title>
    <link href="https://merlynr.github.io/2021/05/30/kernal/"/>
    <id>https://merlynr.github.io/2021/05/30/kernal/</id>
    <published>2021-05-29T16:00:00.000Z</published>
    <updated>2021-05-29T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p><font color="#6495ED">核方法</font>是一类把低维空间的非线性可分问题，转化为高维空间的线性可分问题的方法。</p><blockquote><p><font color="#A9A9A9">理论基础:</font>核方法的理论基础是Cover’s theorem，指的是<font color="#FF8C00">对于非线性可分的训练集，可以大概率通过将其非线性映射到一个高维空间来转化成线性可分的训练集。</font></p></blockquote><p><font color="#9400D3">核函数</font>是映射关系 的内积，映射函数本身仅仅是一种映射关系，并没有增加维度的特性，不过可以利用核函数的特性，构造可以增加维度的核函数。</p><p>设 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">X</mi></math>是输入空间（即 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>i</mi></msub><mo>&#x2208;</mo><mi mathvariant="script">X</mi></math> ， <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">X</mi></math>  是 <math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi mathvariant="normal">&#x211D;</mi><mi>n</mi></msup></math> 的子集或离散集合 ），又设<math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">H</mi></math>  为特征空间（<math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">H</mi></math> 是希尔伯特空间），如果存在一个从 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">X</mi></math> 到 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">H</mi></math> 的映射</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3D5;</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>:</mo><mi mathvariant="script">X</mi><mo>&#x2192;</mo><mi mathvariant="script">H</mi></math></p><p>使得对所有 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi><mo>,</mo><mi>z</mi><mo>&#x2208;</mo><mi mathvariant="script">X</mi></math>,函数<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo>)</mo></math>满足条件</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo>)</mo><mo>=</mo><mo>&#x27E8;</mo><mi>&#x3D5;</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>,</mo><mi>&#x3D5;</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>&#x27E9;</mo></math></p><p>则称 $K$ 为核函数。其中 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3D5;</mi><mo>(</mo><mi>x</mi><mo>)</mo></math> 为映射函数， <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&#x27E8;</mo><mo>&#xB7;</mo><mo>,</mo><mo>&#xB7;</mo><mo>&#x27E9;</mo></math>为内积。</p><p>即核函数输入两个向量，它返回的值<font color="#FF1493">等于</font>这两个向量分别作 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3D5;</mi></math> 映射然后点积【内积】的结果。</p><p><font color="#008B8B">核技巧</font>是一种利用核函数直接计算 <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&#x27E8;</mo><mi>&#x3D5;</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>,</mo><mi>&#x3D5;</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>&#x27E9;</mo></math> ，以避开分别计算<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3D5;</mi><mo>(</mo><mi>x</mi><mo>)</mo></math>  和<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3D5;</mi><mo>(</mo><mi>z</mi><mo>)</mo></math>  ，从而加速核方法计算的技巧。</p><blockquote><p><font color="#FF1493">注意</font><br>得益于<font color="#FF8C00">SVM对偶问题</font>的表现形式，核技巧可以应用于SVM。<br><font color="#7FFF00">TODO  </font>没有了解<br>核函数的选择是SVM的<font color="#B8860B">最大变数</font>，如果核函数选择不适，那么  将不能将输入空间映射到线性可分的特征空间。</p></blockquote><h2 id="判断核函数"><a href="#判断核函数" class="headerlink" title="判断核函数"></a>判断核函数</h2><p><font color="#bf242a">不知道 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3D5;</mi></math> 的情况下，如何判断某个 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math> 是不是核函数？</font></p><p><strong>答案:</strong> 是 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math> 是核函数当且仅当对任意数据 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>D</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>&#x2026;</mo><mo>,</mo><msub><mi>x</mi><mi>m</mi></msub></math> ，核矩阵(kernal matrix,gram matrix)总是半正定的</p><blockquote><p><font color="#368AF8">知识补充：</font><strong>实对称矩阵</strong><br>如果有n阶矩阵A，其矩阵的元素都为实数，且矩阵A的转置等于其本身（aij=aji），(i,j为元素的脚标），则称A为实对称矩阵。</p></blockquote><blockquote><p><font color="#6495ED"><a href="https://zhuanlan.zhihu.com/p/44860862">知识补充</a>：</font><font color="#8B0000">「正定矩阵」(positive definite)</font>和<font color="#8B0000">「半正定矩阵」(positive semi-definite)</font><br><strong>正定矩阵：</strong> 给定一个大小为 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>&#xD7;</mo><mi>n</mi></math> 的实对称矩阵<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math>  ，若对于任意长度为 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math> 的<font color="#A66766">非零向量</font> <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">x</mi></math>，有 <math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi mathvariant="bold-italic">x</mi><mi>T</mi></msup><mi>A</mi><mi mathvariant="bold-italic">x</mi><mo>&gt;</mo><mn>0</mn></math> 恒成立，则矩阵 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math>  是一个正定矩阵。<br><strong>半正定矩阵：</strong> 给定一个大小为 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>&#xD7;</mo><mi>n</mi></math> 的实对称矩阵<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math>  ，若对于任意长度为 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math> 的<font color="#A66766">向量</font> <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">x</mi></math>，有 <math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi mathvariant="bold-italic">x</mi><mi>T</mi></msup><mi>A</mi><mi mathvariant="bold-italic">x</mi><mo>&gt;</mo><mn>0</mn></math> 恒成立，则矩阵 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math>  是一个正定矩阵。<br><font color="#FF00FF">半正定矩阵包括了正定矩阵，核矩阵与协方差矩阵都要半正定</font></p></blockquote><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622369097125.png" alt="核矩阵"></p><h2 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h2><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622366953315.png" alt="常用核函数"></p><h2 id="栗子"><a href="#栗子" class="headerlink" title="栗子"></a>栗子</h2><p>举一个<a href="https://zhuanlan.zhihu.com/p/95362628">栗子</a><br>下面这张图位于第一、二象限内。我们关注红色的门，以及“北京四合院”这几个字下面的紫色的字母。我们把红色的门上的点看成是“+”数据，紫色字母上的点看成是“-”数据，它们的横、纵坐标是两个特征。显然，在这个二维空间内，“+”“-”两类数据不是线性可分的。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622369536767.png" alt="二维"></p><p>我们现在考虑核函数<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mfenced><mrow><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub></mrow></mfenced><mo>=</mo><mo>&lt;</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><msup><mo>&gt;</mo><mn>2</mn></msup></math>，即“内积平方”。<br>这里面<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>v</mi><mn>1</mn></msub><mo>=</mo><mfenced><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub></mrow></mfenced><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo>=</mo><mfenced><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub></mrow></mfenced></math>是二维空间中的两个点。</p><p>这个核函数对应着一个二维空间到三维空间的映射，它的表达式是：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo><mo>=</mo><mfenced><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>,</mo><msqrt><mn>2</mn></msqrt><mi>x</mi><mi>y</mi><mo>,</mo><msup><mi>y</mi><mn>2</mn></msup></mrow></mfenced></math><br>可以验证，<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622369667776.png" alt="核函数"></p><p>在P这个映射下，原来二维空间中的图在三维空间中的像是这个样子：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622369693244.png" alt="三维"></p><p><font color="#D2691E">注意</font>到绿色的平面可以完美地分割红色和紫色，也就是说，两类数据在三维空间中变成线性可分的了。</p><p>而三维中的这个判决边界，再映射回二维空间中是这样的：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622369730773.png" alt="再二维"></p><p>这是一条双曲线，它不是线性的。</p><p><font color="#A52A2A">通过高维映射使得特征线性可分，换种思路就是当两个特征值无法将数据分开时，就将两个特征值进行点交，形成第三个特征，这个时候就有三个特征值，然后构成三位空间，进行分类</font></p>]]></content>
    
    
    <summary type="html">核函数相关的笔记</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="data mining" scheme="https://merlynr.github.io/tags/data-mining/"/>
    
    <category term="kernal method" scheme="https://merlynr.github.io/tags/kernal-method/"/>
    
    <category term="kernal trick" scheme="https://merlynr.github.io/tags/kernal-trick/"/>
    
    <category term="kernal function" scheme="https://merlynr.github.io/tags/kernal-function/"/>
    
  </entry>
  
  <entry>
    <title>协方差矩阵</title>
    <link href="https://merlynr.github.io/2021/05/26/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5/"/>
    <id>https://merlynr.github.io/2021/05/26/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5/</id>
    <published>2021-05-25T16:00:00.000Z</published>
    <updated>2021-05-26T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="方差和协方差的定义"><a href="#方差和协方差的定义" class="headerlink" title="方差和协方差的定义"></a>方差和协方差的定义</h2><p><font color="#184471"><strong>方差</strong>：</font>用来度量单个随机变量的离散程度</p><p>$$\sigma_{x}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}$$</p><p><font color="#D3B2F7">为什么样本方差的分母是n-1？</font></p><pre><code>最简单的原因，是因为因为均值已经用了n个数的平均来做估计在求方差时，只有(n-1)个数和均值信息是不相关的。而你的第ｎ个数已经可以由前(n-1)个数和均值　来唯一确定，实际上没有信息量。所以在计算方差时，只除以(n-1)。</code></pre><p><font color="#0C5F6C"><strong>协方差</strong>：</font>一般用来刻画两个随机变量的相似程度</p><p>$$\sigma(x, y)=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)$$</p><p>在公式中，符号 $\bar{x}, \bar{y}$ 分别表示两个随机变量所对应的观测样本均值，据此，我们发现：方差 $\sigma_{x}^{2}$ 可视作随机变量 x 关于其自身的协方差 $\sigma(x, x)$ .</p><h2 id="从方差-协方差到协方差矩阵"><a href="#从方差-协方差到协方差矩阵" class="headerlink" title="从方差/协方差到协方差矩阵"></a>从方差/协方差到协方差矩阵</h2><p>根据方差的定义，给定 $d$ 个随机变量 $x_{k},k=1,2,\ldots,d$ ，则这些随机变量的方差为<br>$$\sigma\left(x_{k},x_{k}\right)=\frac{1}{n-1}\sum_{i=1}^{n}\left(x_{k i}-\bar{x}_{k}\right)^{2},k=1,2,\ldots,d$$</p><p> $x_{k i}$ 表示随机变量 $x_{k}$ 中的第 $i$ 个观测样本，$n$  表示样本量，每个随机变量所对应的观测样本数量均为 $n$ 。<br> 对于这些随机变量，我们还可以根据协方差的定义，求出<strong>两两之间的协方差</strong>，即<br> <img src="https://gitee.com/merlynr/img-store/raw/master/2021527/1622097407978.png"></p><p> 因此，协方差矩阵为 $$\Sigma=\left[\begin{array}{ccc}\sigma\left(x_{1}, x_{1}\right)&amp;\cdots&amp;\sigma\left(x_{1},x_{d}\right)\\vdots&amp;\ddots&amp;\vdots\\sigma\left(x_{d},x_{1}\right)&amp;\cdots&amp;\sigma\left(x_{d},x_{d}\right)\end{array}\right]\in\mathbb{R}^{d\times d}$$<br>其中，对角线上的元素为各个随机变量的方差，非对角线上的元素为两两随机变量之间的协方差，根据协方差的定义，我们可以认定：矩阵 $\Sigma$ 为<font color="#AB8E35">对称矩阵</font>(symmetric matrix)，其大小为 $d$ x $d$ 。</p><h2 id="多元正态分布与线性变换"><a href="#多元正态分布与线性变换" class="headerlink" title="多元正态分布与线性变换"></a>多元正态分布与线性变换</h2><blockquote><p><font color="#EFED2E">多元正态分布</font>—n维的多元正态分布，也称为多元高斯分布</p></blockquote><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021526/1622033779880.png" alt="多元正态分布图"></p><p>假设一个向量 $x$ 服从均值向量为 $\boldsymbol{\mu}$ 、协方差矩阵为 $\Sigma$ 的多元正态分布(multi-variate Gaussian distribution)【第二章】，则 $$p(\boldsymbol{x})=|2 \pi \Sigma|^{-1 / 2} \exp \left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T} \Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right)$$</p><blockquote><p><font color="#DE8937">联立理解：</font>多元正态分布<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021526/1622035698486.png" alt="多元正态分布"></p></blockquote><p>令该分布的均值向量为 $\boldsymbol{\mu}=\mathbf{0}$ ，由于指数项外面的系数 $|2 \pi \Sigma|^{-1 / 2}$ 通常作为常数，故可将多元正态分布简化为 $$p(\boldsymbol{x}) \propto \exp \left(-\frac{1}{2} \boldsymbol{x}^{T} \Sigma^{-1} \boldsymbol{x}\right)$$<br>再令 $\boldsymbol{x}=(y, z)^{T}$ ，包含两个随机变量 $y$ 和 $z$ ，则协方差矩阵可写成如下形式： $$ \Sigma=\left[\begin{array}{ll}\sigma(y, y) &amp; \sigma(y, z) \ \sigma(z, y) &amp; \sigma(z, z)\end{array}\right] \in \mathbb{R}^{2 \times 2} $$<br>用<font color="#006EFF">单位矩阵</font>(identity matrix) $I$ 作为<font color="#183D66">协方差矩阵</font>，随机变量 $y$ 和 $z$ 的方差均为1，则生成如干个随机数如图所示。 </p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021526/1622038800402.png" alt="图1二元正态分布"></p><blockquote><p><font color="#DE8937">知识补充：</font>单位矩阵<br> 单位矩阵是个方阵，从左上角到右下角的对角线（称为主对角线）上的元素均为1。除此以外全都为0。<strong>任何矩阵与单位矩阵相乘都等于本身</strong><br><img src="https://gitee.com/merlynr/img-store/raw/master/2021526/1622039427482.png" alt="单位矩阵"></p></blockquote><p>在生成的若干个随机数中，每个点的似然为 $$ \mathcal{L}(\boldsymbol{x}) \propto \exp \left(-\frac{1}{2} \boldsymbol{x}^{T} \boldsymbol{x}\right) $$</p><blockquote><p><font color="">知识补充：</font>线性变换<br><a href="https://www.bilibili.com/video/av6043439">视频教学</a><br>线性性质一：直线在变换后仍然保持为直线，不能弯曲；线性性质二：原点是固定不变的</p></blockquote><p>对图[二元正态分布]中的所有点考虑一个线性变换(linear transformation)：$\boldsymbol{t}=A \boldsymbol{x}$  ，我们能够得到图</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021526/1622039676816.png" alt="图2 经过线性变换的二元正态分布，先将图1的纵坐标压缩0.5倍，再将所有点逆时针旋转30°得到"></p><p>在线性变换中，矩阵 $A$ 被称为<strong>变换矩阵</strong>(transformation matrix)，为了将图1中的点经过线性变换得到我们想要的图2，其实我们需要构造两个矩阵：</p><ul><li><strong>尺度矩阵</strong>(scaling matrix)： $$S=\left[\begin{array}{cc}s_{y}&amp;0\0&amp;s_{z}\end{array}\right]$$</li><li><strong>旋转矩阵</strong>(rotation matrix)： $$R=\left[\begin{array}{cc}\cos(\theta)&amp;-\sin(\theta)\\sin(\theta)&amp;\cos(\theta)\end{array}\right]$$<br>其中， $\theta$ 为顺时针旋转的度数。</li></ul><blockquote><p><font color="#8591A6">补充知识：</font>变换矩阵、尺度矩阵和旋转矩阵三者的关系式<br>$A=R S$</p></blockquote><p>在这个例子中，尺度矩阵为 $S=\left[\begin{array}{l l}1&amp;0\0&amp;\frac{1}{2}\end{array}\right]$ ，旋转矩阵为 $R=\left[\begin{array}{c c}\cos\left(-\frac{\pi}{6}\right)&amp;-\sin\left(-\frac{\pi}{6}\right)\\sin\left(-\frac{\pi}{6}\right)&amp;\cos\left(-\frac{\pi}{6}\right)\end{array}\right]=\left[\begin{array}{c c}\frac{\sqrt{3}}{2}&amp;\frac{1}{2}\-\frac{1}{2}&amp;\frac{\sqrt{3}}{2}\end{array}\right]$ ，故变换矩阵为 $A=R S=\left[\begin{array}{cc}\frac{\sqrt{3}}{2}&amp;\frac{1}{4}\-\frac{1}{2}&amp;\frac{\sqrt{3}}{4}\end{array}\right]$</p><p>另外，需要考虑的是，经过了线性变换，$t$  的分布是什么样子呢？</p><p>将 $\boldsymbol{x}=A^{-1} \boldsymbol{t}$ 带入前面给出的似然 $\mathcal{L}(\boldsymbol{x})$ ，有 $\mathcal{L}(\boldsymbol{t}) \propto \exp \left(-\frac{1}{2}\left(A^{-1} \boldsymbol{t}\right)^{T}\left(A^{-1} \boldsymbol{t}\right)\right)$<br>$=\exp \left(-\frac{1}{2} \boldsymbol{t}^{T}\left(A A^{T}\right)^{-1} \boldsymbol{t}\right)$</p><p>由此可以得到，多元正态分布的协方差矩阵为 $$\Sigma=A A^{T}=\left[\begin{array}{cc}\frac{\sqrt{3}}{2} &amp; \frac{1}{4} \ -\frac{1}{2} &amp; \frac{\sqrt{3}}{4}\end{array}\right]\left[\begin{array}{cc}\frac{\sqrt{3}}{2} &amp; -\frac{1}{2} \ \frac{1}{4} &amp; \frac{\sqrt{3}}{4}\end{array}\right]=\left[\begin{array}{cc}\frac{13}{16} &amp; -\frac{3 \sqrt{3}}{16} \ -\frac{3 \sqrt{3}}{16} &amp; \frac{7}{16}\end{array}\right]$$</p><h2 id="协方差矩阵的特征值分解"><a href="#协方差矩阵的特征值分解" class="headerlink" title="协方差矩阵的特征值分解"></a>协方差矩阵的特征值分解</h2><blockquote><p>回到我们已经学过的线性代数内容，对于任意对称矩阵 $\Sigma$ ，存在一个特征值分解(eigenvalue decomposition, EVD)： $$\Sigma=U \Lambda U^{T}$$ 其中, $U$ 的每一列都是相互正交的特征向量，且是单位向量，满足 $U^{T}U=I$ ， $\Lambda$ 对角线上的元素是从大到小排列的特征值，非对角线上的元素均为0。</p></blockquote><p>当然，这条公式在这里也可以很容易地写成如下形式： $$\Sigma=\left(U \Lambda^{1 / 2}\right)\left(U \Lambda^{1 / 2}\right)^{T}=A A^{T}$$<br>其中，$A=U \Lambda^{1 / 2}$  ，因此，通俗地说，<font color="#226771">任意一个协方差矩阵都可以视为线性变换的结果。</font><br>在上面的例子中，<strong>特征向量构成的矩阵</strong>为 $$U=R=\left[\begin{array}{cc}\cos (\theta) &amp; -\sin (\theta) \ \sin (\theta) &amp; \cos (\theta)\end{array}\right]=\left[\begin{array}{cc}\frac{\sqrt{3}}{2} &amp; \frac{1}{2} \ -\frac{1}{2} &amp; \frac{\sqrt{3}}{2}\end{array}\right]$$<br><strong>特征值构成的矩阵</strong>为<br>$$\Lambda=S S^{T}=\left[\begin{array}{cc}s_{y}^{2} &amp; 0 \ 0 &amp; s_{z}^{2}\end{array}\right]=\left[\begin{array}{ll}1 &amp; 0 \ 0 &amp; \frac{1}{4}\end{array}\right]$$<br>到这里，我们发现：多元正态分布的概率密度是由<font color="#BD5A5D">协方差矩阵的特征向量控制旋转(rotation)</font>，<font color="">特征值控制尺度(scale)</font>，除了协方差矩阵，<font color="#810006">均值向量会控制概率密度的位置</font>，在图1和图2中，均值向量为 $0$ ，因此，概率密度的中心位于坐标原点。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;方差和协方差的定义&quot;&gt;&lt;a href=&quot;#方差和协方差的定义&quot; class=&quot;headerlink&quot; title=&quot;方差和协方差的定义&quot;&gt;&lt;/a&gt;方差和协方差的定义&lt;/h2&gt;&lt;p&gt;&lt;font color=&quot;#184471&quot;&gt;&lt;strong&gt;方差&lt;/strong&gt;：</summary>
      
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>挖掘业务流程，结合机器学习进行业务预测分析</title>
    <link href="https://merlynr.github.io/2021/05/25/%E6%8C%96%E6%8E%98%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%EF%BC%8C%E7%BB%93%E5%90%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%BF%9B%E8%A1%8C%E4%B8%9A%E5%8A%A1%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/"/>
    <id>https://merlynr.github.io/2021/05/25/%E6%8C%96%E6%8E%98%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%EF%BC%8C%E7%BB%93%E5%90%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%BF%9B%E8%A1%8C%E4%B8%9A%E5%8A%A1%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/</id>
    <published>2021-05-24T16:00:00.000Z</published>
    <updated>2021-05-14T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基于机器学习的流程异常预测行为"><a href="#基于机器学习的流程异常预测行为" class="headerlink" title="基于机器学习的流程异常预测行为"></a>基于机器学习的流程异常预测行为</h2><p><img src="./attachments/%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%B5%81%E7%A8%8B%E5%BC%82%E5%B8%B8%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95_%E9%AD%8F%E6%87%BF.pdf" alt="基于机器学习的流程异常预测方法_魏懿"></p><h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><blockquote><p>通过挖掘流程执行的<font color="#D2691E">日志记录</font> 和<font color="#6495ED">活动执行时间信息 </font>，基于机器学习方法的异常检测方法，实现实时预测业务流程中的超 期 异 常 和 流 程 行 为 异 常。</p></blockquote><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><blockquote><p>异常(预期的，完全意外的)</p></blockquote><ol><li>流程超期、资源不可用、活动执行失败等和完全意外的异常</li></ol><blockquote><p>现有的流程异常检测方法</p></blockquote><ul><li>主动 的 设 置 时 间 检 查点、动态检查，或 被动地基于异常发生后捕捉异常、处理异常的机制<ol><li>主动设置时间检查点的方法有两个弊端，第一个设置点的位置无法精确判断，第二个是系统状态是动态的，受生产环境等诸多条件影响，所以主动i设置会造成很多新的问题</li><li>被动处理超期异常的方法，失去了对业务流程管理的主动性，从而将导致工作流期望的目标延迟或付出更大的开销。【即失去对于流程预测的主动性】</li></ol></li></ul><h3 id="目前国内外研究动态"><a href="#目前国内外研究动态" class="headerlink" title="目前国内外研究动态"></a>目前国内外研究动态</h3><h4 id="基于时间边界的时间异常检测"><a href="#基于时间边界的时间异常检测" class="headerlink" title="基于时间边界的时间异常检测"></a>基于时间边界的时间异常检测</h4><ol><li>基于时间边界的时间异常检测–Eder</li></ol><p><font size=1>the fifth and sixth document of this paper </font></p><pre><code>首先要明确每个任务节点执行时间的上下边界， 基于这两个时限， 计算起始节点到当前节点的最佳（ 最短） 执行时间和最坏（ 最长） 执行时间。当流程执行时， 如果当前时间在区间内， 则判断为没有时间异常</code></pre><ol start="2"><li>基于关键路径</li></ol><p><font size=1>the seventh document of this paper </font></p><pre><code>在工作流执行前，会根据模型先找出关键路径， 并在流程执行时检查最佳完成时间与最终时限， 如果最佳完成时间大于最终时限， 则预测为异常</code></pre><h4 id="时间统计模型建立"><a href="#时间统计模型建立" class="headerlink" title="时间统计模型建立"></a>时间统计模型建立</h4><ol><li>执行时间建模方法</li></ol><p><font size=1>the eighth document of this paper </font></p><pre><code>该方法利用历史日志生成涵盖所有活动持续时间直方图来表示当前节点和末端节点之间的剩余执行时间的概率，用于捕获有关工作流执行的时间信息，定义计算工作流执行时间的必要操作</code></pre><ol start="2"><li>综合时间模型和流程步骤分析</li></ol><p><font size=1>the ninth document of this paper </font></p><pre><code>综合运用时间统计模型和通过多个步骤分析方法生成运行时间概率分布、计算异常概率、与阈值比较的方法，提出一种基于运行的异常预测算法来预测工作流中的时间异常，该算法分为即设计时段和运行时段两个阶段，在设计时段，生成该模型所有可能产生的运行轨迹，并计算它们的预计执行时间的概率分布；在运行时段，通过分析计算流程超时的可能性与预设的阈值做比较来判断是否预测为异常    </code></pre><ol start="3"><li>结合积极语义模型</li></ol><p><font size=1>the tenth document of this paper </font></p><pre><code>采用积极语义模型来捕捉各种工作流情形下的 语 义 特 征，并 且 检 测 和 处 理 异 常    </code></pre><ol start="4"><li>提出受启发与传染病模型的时间延迟传播模型</li></ol><p><font size=1>the eleventh document of this paper </font></p><pre><code>着眼于并行云工作流中的时间延迟，提出受启发与传染病模型的时间延迟传播模型，预测使云工作流中达到一定完成率的最大时间异常数目</code></pre><h3 id="离群点检测的算法"><a href="#离群点检测的算法" class="headerlink" title="离群点检测的算法"></a>离群点检测的算法</h3><p><a href="https://blog.zuishuailcq.xyz/2021/05/25/%E7%A6%BB%E7%BE%A4%E7%82%B9%E6%A3%80%E6%B5%8B/">离群点检测 | 吾辈之人，自当自强不息！</a></p><p><a href="https://blog.zuishuailcq.xyz/2021/05/31/%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97%EF%BC%88Isolation%20Forest%EF%BC%89/">孤立森林（Isolation Forest） | 吾辈之人，自当自强不息！</a></p><h2 id="本文"><a href="#本文" class="headerlink" title="本文"></a>本文</h2><h3 id="idea"><a href="#idea" class="headerlink" title="idea"></a>idea</h3><blockquote><p>提出一种基于活动执行时间和比例关系的方法，通过学习历史流程执行日志中活动时间信息，根据正在执行的待预测流程的日志及状态，预测其是否为异常流程以及异常的类型。并且，本文提出通过计算活动执行时间之间的比例关系作为流程特征加入机器学习算法，运用机器学习中监督学习的分类器以预测流程是否会发生超期异常（流程执行总时间超过预设最终期限），同时使用非监督学习的离群点检测算法根据历史数据中活动执行时间比例关系判定流程行为异常。结合两种算法的结果对流程异常预测做出进一步的分类和分析。</p></blockquote><h3 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h3><ol><li>预处理历史和正在执行的流程数据，获得流程中活动执行时间序列以及计算活动执行时间<font color="#8B0000">比例关系</font></li><li>使用监督学习的分类器，预测并标记超期【流程执行<br>总时间超过预设最终期限】异常流程为<math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">I</mi></math>类异常流程</li><li> 用无监督学习检测离群点算法【 <font color="#9400D3">活动执行时间之间比例关系(单个活动占总体)</font>为特征值】，找出历史数据中的异常流程并标记为<math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">I</mi></math><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">I</mi></math>类异常流程</li><li> 通过集成业务流程异常预测方法将待预测流程分为正常流程或者不同种类的异常流程</li></ol><h3 id="结构概述"><a href="#结构概述" class="headerlink" title="结构概述"></a>结构概述</h3><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622429405396.png" alt="集成业务流程异常预测方法结构图"></p><blockquote><p><font color="#8FBC8F">知识补充：</font>弱监督<br>不完全监督（Incomplete supervision）：训练数据中只有一部分数据被给了标签，有一些数据是没有标签的。<br>不确切监督（Inexact supervision）：训练数据只给出了粗粒度标签。可理解为只给了大类的标签，详细属性没有给标签<br>不精确监督（Inaccurate supervision）：给出的标签不总是正确的</p></blockquote><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622441386940.png" alt="预测结果异常分类韦恩图"></p><ol><li>第一类通过弱监督学习方法可以标记出大部分的异常流程，但是系统的运行情况很容易受环境资源影响，很多时候由于等待时间过长，被误标为异常流程，但是依旧属于正常流程</li><li>在实际业务流程中， 活动的执行时间之间并非独立分布， 而是具有隐含的相关关系， 由于多种因素的影响， 造成了活动时间相应的变化。比如工作负荷加倍使得某些活动花费了较长时间， 导致流程总时间较长， 有超期异常的风险。但是从活动执行时间比例关系来看， 流程时间可能被近乎等比例放大， 完全是合情合理的， 并不应该被记为最终期限异常的流程。在活动时间比例上， 正常执行的流程活动时间比例关系是相似的， 而行为异常的流程活动时间比例关系容易出现离群点。因此计算流程中活动时间的比例关系， 并将其作为特征加入算法是有必要的。</li></ol><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h4><blockquote><p><font color="#1E90FF">数据集</font><br>将历史业务流程日志及正在执行流程中的活动执行时间信息作为初始的数据集</p></blockquote><p><font color="#7FFF00">TODO</font><br> 通过目前较为成熟的流程挖掘算法和软件， 如ProM Tools、Disco， 流程模型模拟业务流程获取数据可以简化结构、 缩减活动数量。</p><p><font color="#ff7500">数据初始化：</font><br>待预测流程的活动数量：n<br>多条与待预测执行流程路径一致的历史流程数量：q<br>待预测流程执行时间集：T<br>其中一条流程的活动执行时间序列：<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>T</mi><mi>k</mi></msub><mo>(</mo><mi>k</mi><mo>&#x2208;</mo><mo>[</mo><mn>1</mn><mo>,</mo><mi>q</mi><mo>]</mo><mo>)</mo></math><br>序列中单个活动的执行时间：<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>t</mi><mi>i</mi></msub><mo>(</mo><mi>i</mi><mo>&#x2208;</mo><mo>[</mo><mn>1</mn><mo>,</mo><mi>n</mi><mo>]</mo><mo>)</mo></math></p><p>待预测流程执行时间集：<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mo>=</mo><mfenced close="]" open="["><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msub><mi>T</mi><mi>k</mi></msub><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msub><mi>T</mi><mi>q</mi></msub></mrow></mfenced><mo>,</mo><mi>k</mi><mo>&#x2208;</mo><mo>[</mo><mn>1</mn><mo>,</mo><mi>q</mi><mo>]</mo></math><br>序列k的流程时间：<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>T</mi><mi>k</mi></msub><mo>=</mo><mfenced close="]" open="["><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub></mrow></mfenced><mo>,</mo><mi>i</mi><mo>&#x2208;</mo><mo>[</mo><mn>1</mn><mo>,</mo><mi>n</mi><mo>]</mo></math></p><h4 id="获得时间比例"><a href="#获得时间比例" class="headerlink" title="获得时间比例"></a>获得时间比例</h4><p><font color="#1E90FF">对长度为 $n$ 的活动执行时间序列<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>T</mi><mi>k</mi></msub></math>，求出长度为 $n-1$ 的时间比例序<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>R</mi><mi>k</mi></msub></math>，记比例数据集为 $R$ </font></p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi><mo>=</mo><mfenced close="]" open="["><mrow><msub><mi>R</mi><mn>1</mn></msub><mo>,</mo><msub><mi>R</mi><mn>2</mn></msub><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msub><mi>R</mi><mi>k</mi></msub><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msub><mi>R</mi><mi>q</mi></msub></mrow></mfenced><mo>,</mo><mi>k</mi><mo>&#x2208;</mo><mo>[</mo><mn>1</mn><mo>,</mo><mi>q</mi><mo>]</mo></math><br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>R</mi><mi>k</mi></msub><mo>=</mo><mfenced close="]" open="["><mrow><mfrac><msub><mi>t</mi><mn>1</mn></msub><msub><mi>t</mi><mn>2</mn></msub></mfrac><mo>,</mo><mfrac><msub><mi>t</mi><mn>2</mn></msub><msub><mi>t</mi><mn>3</mn></msub></mfrac><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><mfrac><msub><mi>t</mi><mi>i</mi></msub><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mfrac><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><mfrac><msub><mi>t</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><msub><mi>t</mi><mi>n</mi></msub></mfrac></mrow></mfenced><mo>,</mo><mi>i</mi><mo>&#x2208;</mo><mo>[</mo><mn>1</mn><mo>,</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo>]</mo></math></p><h4 id="对异常流程进行标记"><a href="#对异常流程进行标记" class="headerlink" title="对异常流程进行标记"></a>对异常流程进行标记</h4><ul><li>通过历史流程计算出流程执行时间分布，可以给不同活动设定阈值，来标记超期异常流程</li><li>可以用执行时间拟合建立高斯分布，利用模型参数设立阈值以标记异常（如临界点 $threshold$ <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>=</mo><mi>&#x3BC;</mi><mo>+</mo><mn>2</mn><mo>&#xB7;</mo><mi>&#x3C3;</mi></math>），标记出的异常数据集为<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>t</mi><mi>c</mi><mi>v</mi></math>【time constrain violation 违规时间约束】</li></ul><h4 id="集成业务流程异常预测方法（EnsBPAP）"><a href="#集成业务流程异常预测方法（EnsBPAP）" class="headerlink" title="集成业务流程异常预测方法（EnsBPAP）"></a>集成业务流程异常预测方法（EnsBPAP）</h4><blockquote><p><font color="#9932CC">前提</font><br>将待预测流程的活动执行时间序列记为ｔ，将其时间比例序列记为ｒ， 同数据预处理中得到的历史流程的执行时间,比例数据集和标记出的异常数据集 $T$ , $R$ , $tcv$</p></blockquote><ol><li>将活动执行时间和比例的训练数据集和测试用例数据传入监督学习的分类算法中， 得到超期异常预测结果</li><li>将活动时间比例的训练集和测试用例传入无监督学习异常检测算法， 得到行为异常预测结果</li><li>用两个预测结果访问EnsBPAP分类结果矩阵， 并返回最终的分类结果</li></ol><p><font color="#0000FF">EnsBPAP(t,r,T,R,balance_data)伪代码</font></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622448127881.png" alt="输入输出"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021531/1622448326369.png" alt="步骤函数"></p><ol><li>标记超期异常流程</li><li>标记行为异常【时间比例异常】</li><li>制定EnsBPAP模型【位运算】</li><li>将异常流程通过EnsBPAP模型，获得符合模型的综合分类结果</li></ol><blockquote><p>class： 预测测试过程的综合分类结果</p></blockquote><h4 id="超期异常预测"><a href="#超期异常预测" class="headerlink" title="超期异常预测"></a>超期异常预测</h4><p><font color="#7FFF00">分类器基本模型</font></p><ul><li>逻辑回归算法–监督学习</li></ul><p><font color="#DC143C">数据存在问题</font></p><ul><li>异常点在整个数据集中的数量远小于正常点的数量【样本不均衡问题】，会导致分类器倾向于把预测样本分为多数类。</li></ul><p><font color="#00FFFF">处理样本不平衡问题</font>—<a href="https://blog.zuishuailcq.xyz/2021/05/31/%E4%BA%8C%E5%88%86%E7%B1%BB%E4%B9%8B%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1/">二分类之类别不平衡 | 吾辈之人，自当自强不息！</a></p><ul><li>使用过采样和欠采样结合的方法<font color="#0000FF">SMOTE + TOMEK algorithm</font></li></ul><ol><li>第１步， 将执行时间和比例数据Ｔ，Ｒ 合并成训练集Ｘ， 训练目标为tcv，ｔ，ｒ合并成测试样本ｘ； </li><li>第２步， 根据balance_data参数选择是否执行SMOTE＋Tomek算法均衡训练样本【balance_data平衡训练数据以进行时间约束违规预测】</li><li>第3步，对每个特征做归一化消除数据量级的影响【<font color="#FF00FF">归一化的目的就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响</font>。】</li><li>第4步，初始化算法模型，超参数空间，最佳参数</li><li>第5步，通过若干次迭代随机生成超参数、参考交叉验证法评估当前超参数下的性能、更新最佳超参</li><li>最后一步，使用最佳超参数拟合算法模型，预测测试样本模型类型，并返回</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021611/1623397346285.png" alt="TimeConstraitsViolationPrediction(t,r,T,R,tcv,balance_data)"></p><h4 id="行为异常检测"><a href="#行为异常检测" class="headerlink" title="行为异常检测"></a>行为异常检测</h4><p><font color="#8B0000">目标</font></p><ul><li>通过活动执行时间比例找出离群点， 以鉴别待预测流程是否为行为异常的流程。</li></ul><p><font color="#8B008B">算法模型</font></p><ul><li>孤立森林</li></ul><ol><li>第１步， 初始化算法模型</li><li>第２步， 拟合历史数据得到孤立森林模型</li><li>第３步， 预测测试样本并返回</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021611/1623397471182.png" alt="BehaviorAnimalyDetection(r,R)"></p><blockquote><p><font color="#006400">补充知识</font><br><a href="https://blog.csdn.net/huangfei711/article/details/78456165">如何通俗易懂地理解皮尔逊相关系数？_黄飞的博客专栏-CSDN博客_皮尔逊相关系数怎么看</a></p></blockquote><h2 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h2><ol><li>流程的预设期限<font color="#5F9EA0">如何</font> <font color="#696969">在哪</font> 还有 <font color="#BDB76B">设置的标准</font>没有提到</li></ol>]]></content>
    
    
    <summary type="html">挖掘业务流程，结合机器学习进行业务预测分析</summary>
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="RPA" scheme="https://merlynr.github.io/tags/RPA/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>离群点检测</title>
    <link href="https://merlynr.github.io/2021/05/25/%E7%A6%BB%E7%BE%A4%E7%82%B9%E6%A3%80%E6%B5%8B/"/>
    <id>https://merlynr.github.io/2021/05/25/%E7%A6%BB%E7%BE%A4%E7%82%B9%E6%A3%80%E6%B5%8B/</id>
    <published>2021-05-24T16:00:00.000Z</published>
    <updated>2021-05-27T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h3 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h3><p><strong>离群点检测</strong>（<font color=" #009688">异常检测</font>）是找出其行为不同于预期对象的过程，这种对象称为离群点或异常。</p><blockquote><p>离群点和噪声有区别，噪声是观测变量的随机误差和方差，而离群点的产生机制和其他数据的产生机制就有根本的区别,同一批数据产生方式可能不一样。</p></blockquote><p><strong>全局离群点</strong>：通过找到某种合适的偏离度量方式，将离群点检测划为不同的类别；全局离群点是情景离群点的特例，因为考虑整个数据集为一个情境。</p><p><strong>情境离群点</strong>：又称为条件离群点，即在特定条件下它可能是离群点，但是在其他条件下可能又是合理的点。比如夏天的28℃和冬天的28℃等。</p><p><strong>集体离群点</strong>：个体数据可能不是离群点，但是这些对象作为整体显著偏移整个数据集就成为了集体离群点。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021525/1621947763766.png" alt="黑色对象形成集体离群点"></p><h3 id="补充学习"><a href="#补充学习" class="headerlink" title="补充学习"></a>补充学习</h3><blockquote><p>有些模型的表现一直不错，建议优先考虑。对于大数据量和高纬度的数据集，Isolation Forest算法的表现比较好。小数据集上，简单算法KNN和MCD的表现不错。</p></blockquote><p> <font color="#F6C75A">聚类：</font>将物理或抽象对象的集合分成由类似的对象组成的多个类的过程被称为聚类。</p><p><font color="#D87E8D">簇：</font>把数据划分为不同类别，机器学习给这个类别定义一个新的名字—簇。</p><h2 id="离群点检测目前遇到的挑战"><a href="#离群点检测目前遇到的挑战" class="headerlink" title="离群点检测目前遇到的挑战"></a>离群点检测目前遇到的挑战</h2><ul><li>正常数据和离群点的有效建模本身就是个挑战,数据没有标签，无法分清正常数据还是异常数据；或者缺乏异常数据；</li><li>离群点检测高度依赖于应用类型使得不可能开发出通用的离群点检测方法，比如针对性的相似性、距离度量机制等；</li><li>数据质量实际上往往很差，噪声充斥在数据中，影响离群点和正常点之间的差别，缺失的数据也可能“掩盖”住离群点，影响检测到有效性；</li><li>检测离群点的方法需要可解释性；</li></ul><h2 id="离群点检测方法"><a href="#离群点检测方法" class="headerlink" title="离群点检测方法"></a>离群点检测方法</h2><h3 id="监督方法"><a href="#监督方法" class="headerlink" title="监督方法"></a>监督方法</h3><p> <strong>➀训练可识别离群点的分类器</strong></p><p><font color="#009688">困难：</font> 1 .两个类别（正常和离群）的数据量很不平衡，缺乏足够的离群点样本可能会限制所构建分类器的能力；<br>2. 许多应用中，捕获尽可能多的离群点（灵敏度和召回率）比把正常对象误当做离群点更重要。</p><blockquote><p>由于与其他样本相比离群点很稀少，所以离群点检测的监督方法必须注意如何训练和如何解释分类率。</p></blockquote><p><strong>➁One-class model，一分类模型</strong></p><pre><code>考虑到数据集严重不平衡的问题，构建一个仅描述正常类的分类器，不属于正常类的任何样本都被视为离群点。比如SVM决策边界以外的都可以视为离群点。</code></pre><h3 id="无监督方法"><a href="#无监督方法" class="headerlink" title="无监督方法"></a>无监督方法</h3><blockquote><p>正常对象在某种程度上是“聚类”的，正常对象之间具有高度的相似性，但是离群点将远离正常对象的组群。<font color="#72A1C3">但是遇到前文所述的集体离群点时，正常数据是发散的，而离群点反而是聚类的</font>,这种情形下更适合<font color="#A98A2F">监督方法</font>进行检测。无监督方法很容易误标记离群点导致许多真实的离群点逃脱检测。</p></blockquote><p><strong>对于传统的聚类方法，有以下几个问题：</strong></p><ul><li>不属于任何簇的对象可能是噪声，而不是离群点；</li><li>先找出簇再找出离群点的开销很大（离群点数量远少于正常对象）；</li></ul><h3 id="半监督方法"><a href="#半监督方法" class="headerlink" title="半监督方法"></a>半监督方法</h3><p>当有一些被标记的正常对象时，可以先使用它们，与邻近的无标记对象一起训练一个正常的对象模型，使用这个模型检测离群点；但是由于具有标记的数据只有少部分，意味着仅仅基于少量被标记的离群点而构建的离群点模型不大可能是有效的。</p><h3 id="统计方法"><a href="#统计方法" class="headerlink" title="统计方法"></a>统计方法</h3><blockquote><p>假定正常的数据对象由一个统计模型产生，不遵守该模型的数据是离群点。即正常对象出现在该随机模型的高概率区域中，而低概率区域中的对象是离群点</p></blockquote><h4 id="参数方法—壹"><a href="#参数方法—壹" class="headerlink" title="参数方法—壹"></a>参数方法—壹</h4><blockquote><p>基于正态分布的一元离群点检测（仅涉及一个属性或变量的数据）</p></blockquote><ol><li>假定数据由某个正态分布产生，由输入来学习正态分布的参数（μ ，σ）（最大似然估计），通过假设检验的方法，一般认定如果某点距离估计的分布均值超过3σ  ，就被认为是离群点。下面的文章中提到过利用盒图和四分位数据来划分离群点，其原理类似。</li><li>另一种离群点检测方法是Grubb检验（最大标准残差检验），对于数据集中的每个对象x，定义z分数(z-score)为：$z=\frac{|x-\bar{x}|}{s}$ , $\bar{x}$是输入数据的均值，s是标准差。<br>若 $z\geq\frac{N-1}{\sqrt{n}}\sqrt{\frac{t_{a/(2N),N-2}^{2}}{N-2+t_{a/(2N),N-2}^{2}}}$ ,x视为离群点。<br>其中 $t^{2}\alpha/(2N),N-2$ 是显著水平 $\alpha /(2N)$ 下的 $t-$ 分布的值，N是数据集中的对象数。</li></ol><h4 id="参数方法—贰"><a href="#参数方法—贰" class="headerlink" title="参数方法—贰"></a>参数方法—贰</h4><blockquote><p>多元离群点检测<br><font size=1>涉及两个或多个属性或变量的数据称为多元数据。核心思想是把多元离群点检测任务转换成一元离群点检测问题。</font></p></blockquote><ol><li><font color="#032953"><strong>马哈拉诺比斯距离检测多元离群点</strong></font></li></ol><p> 对一个多元数据集，设 $\bar{o}$ 为均值向量，对数据集中的对象 $O$ ，从  $O$ 到 $\bar{o}$ 的马哈拉诺比斯距离为： $$M D i s t(o, \bar{o})=(o-\bar{o})^{T} S^{-1}(o-\bar{o})$$ ,S是协方差矩阵。 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo>(</mo><mi>o</mi><mo>,</mo><mover><mi>O</mi><mo>&#xAF;</mo></mover><mo>)</mo></math>是一元变量，于是可以对它进行Grubb检验，如果<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo>(</mo><mi>o</mi><mo>,</mo><mover><mi>O</mi><mo>&#xAF;</mo></mover><mo>)</mo></math>设定为离群点的阈值，则 $o$ 是为离群点。</p><blockquote><p><font color="#DD7ADF">补充知识：</font><a href="https://blog.zuishuailcq.xyz/2021/05/26/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5/">协方差矩阵 | 吾辈之人，自当自强不息！</a><br><font color="#2C7D82">协方差矩阵：</font>计算样本不同维度之间的协方差<br><font color="#348A8A">协方差：</font>一般用来刻画两个随机变量的相似程度</p></blockquote><blockquote><p><font color="#F27611">补充知识：</font><strong>欧氏距离</strong>—–又称欧几里得距离<br>m维空间中两个点之间的真实距离<br>例如二维空间的公式：<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3C1;</mi><mo>=</mo><msqrt><msup><mfenced><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>-</mo><msub><mi>x</mi><mn>1</mn></msub></mrow></mfenced><mn>2</mn></msup><mo>+</mo><msup><mfenced><mrow><msub><mi>y</mi><mn>2</mn></msub><mo>-</mo><msub><mi>y</mi><mn>1</mn></msub></mrow></mfenced><mn>2</mn></msup></msqrt></math>，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3C1;</mi></math> 为点 <math xmlns="http://www.w3.org/1998/Math/MathML"><mfenced><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub></mrow></mfenced></math>与点 <math xmlns="http://www.w3.org/1998/Math/MathML"><mfenced><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub></mrow></mfenced></math>之间的欧氏距离</p></blockquote><blockquote><p><font color="#4D74F2">补充知识：</font><strong>马哈拉诺比斯距离</strong><br>表示数据的协方差距离，它是一种有效的计算两个未知样本集的相似度的方法。<br><strong>思路</strong>：</p><ul><li>将变量按照主成分进行旋转，消除维度间的相关性</li><li>对向量和分布进行标准化，让各个维度同为标准正态分布</li></ul></blockquote><ol start="2"><li><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>&#x3C7;</mi><mn>2</mn></msup></math><strong>统计量</strong>的多元离群点检测</li></ol><p>  正态分布的假定下，<math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>&#x3C7;</mi><mn>2</mn></msup></math>  统计量也可以用来捕获多元离群点，对象 $o$ ，<math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>&#x3C7;</mi><mn>2</mn></msup></math>  统计量是：<br>  <math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>&#x3C7;</mi><mn>2</mn></msup><mo>=</mo><munderover><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfrac><msup><mfenced><mrow><msub><mi>o</mi><mi>i</mi></msub><mo>-</mo><msub><mi>E</mi><mi>i</mi></msub></mrow></mfenced><mn>2</mn></msup><msub><mi>E</mi><mi>i</mi></msub></mfrac></math></p><blockquote><p><font color="#6B6B6B">统计量：</font><br>是样本测量的一种<em>属性</em>。类似计算样本的平均值。</p></blockquote><p><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>O</mi><mi>i</mi></msub></math>是$o$在第 $i$ 维上的值，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>E</mi><mi>i</mi></msub></math>是所有对象在第 $i$ 维上的均值，而n是是维度。如果对象的 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3C7;</mi><mn>2</mn></math>统计量很大，则对象是离群点。</p><ol start="3"><li>混合参数分布检测离群</li></ol><p>  当实际数据很复杂时，假定服从正态分布的话会不太合适，这种情况下假定数据是被混合参数分布产生的。</p><blockquote><p><font color="#1076FF">补充知识：</font><strong>混合分布</strong><br>在概率与统计中，如果我们有一个包含多个随机变量的随机变量集合，再基于该集合生成一个新的随机变量，则该随机变量的分布称为混合分布(mixture distribution)。<br><font color="red">TODO:</font>查阅了<a href="https://blog.csdn.net/tanghonghanhaoli/article/details/90543917">混合分布</a>的三个性质没有理解如何判定离群 </p></blockquote><h4 id="非参数方法—壹"><a href="#非参数方法—壹" class="headerlink" title="非参数方法—壹"></a>非参数方法—壹</h4><p><font color="#DD7ADF">构造直方图</font></p><p>为了构造一个好的直方图，用户必须指定直方图的类型和其他参数（箱数、等宽or等深）。最简单的方法是，如果该对象落入直方图的一个箱中，则该对象被看做正常的，否则被认为是离群点。也可以使用直方图赋予每个对象一个离群点得分，比如对象的离群点得分为该对象落入的箱的容积的倒数。</p><h4 id="非参数方法—贰"><a href="#非参数方法—贰" class="headerlink" title="非参数方法—贰"></a>非参数方法—贰</h4><p><font color="#DD7ADF"><a href="https://blog.csdn.net/pipisorry/article/details/53635895">核密度估计</a></font></p><blockquote><p> <font color="#B22222">补充知识：</font><br> <strong>向量的内积与外积</strong><br> 对于向量a和向量b：<br> <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi><mo>=</mo><mfenced close="]" open="["><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><mo>&#x2026;</mo><mo>&#xB7;</mo><msub><mi>a</mi><mi>n</mi></msub></mrow></mfenced></math><br> <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>b</mi><mo>=</mo><mfenced close="]" open="["><mrow><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mo>&#x2026;</mo><mo>,</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mfenced></math><br> <font color="#00FFFF">内积</font><br> <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi><mo>&#x2219;</mo><mi>b</mi><mo>=</mo><msub><mi>a</mi><mn>1</mn></msub><msub><mi>b</mi><mn>1</mn></msub><mo>+</mo><msub><mi>a</mi><mn>2</mn></msub><msub><mi>b</mi><mn>2</mn></msub><mo>+</mo><mo>&#x2026;</mo><mo>+</mo><msub><mi>a</mi><mi mathvariant="normal">n</mi></msub><msub><mi>b</mi><mi>n</mi></msub></math>，内积的几何意义是可以用来表征【信息在头脑中的呈现方式】或计算两个向量之间的夹角，以及在b向量在a向量方向上的投影。<br><font color="#FF8C00">外积</font><br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi><mo>&#xD7;</mo><mi>b</mi><mo>=</mo><mfenced close="|" open="|"><mtable columnalign="left"><mtr><mtd><mi>i</mi></mtd><mtd><mi>j</mi></mtd><mtd><mi>k</mi></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd><mtd><msub><mi>y</mi><mn>1</mn></msub></mtd><mtd><msub><mi>z</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>2</mn></msub></mtd><mtd><msub><mi>y</mi><mn>2</mn></msub></mtd><mtd><msub><mi>z</mi><mn>2</mn></msub></mtd></mtr></mtable></mfenced><mo>=</mo><mfenced><mrow><msub><mi>y</mi><mn>1</mn></msub><msub><mi>z</mi><mn>2</mn></msub><mo>-</mo><msub><mi>y</mi><mn>2</mn></msub><msub><mi>z</mi><mn>1</mn></msub></mrow></mfenced><mi>i</mi><mo>-</mo><mfenced><mrow><msub><mi>x</mi><mn>1</mn></msub><msub><mi>z</mi><mn>2</mn></msub><mo>-</mo><msub><mi>x</mi><mn>2</mn></msub><msub><mi>z</mi><mn>1</mn></msub></mrow></mfenced><mi>j</mi><mo>+</mo><mfenced><mrow><msub><mi>x</mi><mn>1</mn></msub><msub><mi>y</mi><mn>2</mn></msub><mo>-</mo><msub><mi>x</mi><mn>2</mn></msub><msub><mi>y</mi><mn>1</mn></msub></mrow></mfenced><mi>k</mi></math>，外积的结果是一个向量，更为熟知的叫法是法向量，该向量垂直于a和b向量构成的平面。</p></blockquote><p>把每个观测对象看作一个周围区域中的高概率密度指示子，一个点上的概率密度依赖于该点到观测对象的距离，使用核函数对样本点对其邻域的影响建模。核函数K()满足以下两个条件：</p><ol><li><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mo>&#x222B;</mo><mrow><mo>-</mo><mo>&#x221E;</mo></mrow><mo>&#x221E;</mo></msubsup><mi>K</mi><mo>(</mo><mi>u</mi><mo>)</mo><mi>d</mi><mi>u</mi><mo>=</mo><mn>1</mn></math></li><li>对于所有的 $u$ 值，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>(</mo><mo>-</mo><mi>u</mi><mo>)</mo><mo>=</mo><mi>K</mi><mo>(</mo><mi>u</mi><mo>)</mo></math></li></ol><p> 一个频繁使用的核函数是标准高斯函数：<br> <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mfenced><mfrac><mrow><mi>x</mi><mo>-</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mi>h</mi></mfrac></mfenced><mo>=</mo><mfrac><mn>1</mn><msqrt><mn>2</mn><mi>&#x3C0;</mi></msqrt></mfrac><msup><mi>e</mi><mrow><mo>-</mo><mfrac><msup><mfenced><mrow><mi>x</mi><mo>-</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mfenced><mn>2</mn></msup><mrow><mn>2</mn><msup><mi>h</mi><mn>2</mn></msup></mrow></mfrac></mrow></msup></math></p><blockquote><p><font color="#3B4B6E">补充知识：</font>高斯函数<br>一维形式<br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>a</mi><msup><mi>e</mi><mrow><mo>-</mo><mfrac><mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>b</mi><msup><mo>)</mo><mn>2</mn></msup></mrow><mrow><mn>2</mn><msup><mi>c</mi><mn>2</mn></msup></mrow></mfrac></mrow></msup></math><br>a是曲线尖峰的高度，b是尖峰中心的坐标，c称为标准方差<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622339515069.png" alt="高斯函数"></p><p><font color="#DD7ADF">二维高斯核函数</font>常用于高斯模糊Gaussian Blur，在数学领域，主要是用于解决热力方程和扩散方程，以及定义Weiertrass Transform<br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo><mo>=</mo><mi>A</mi><mi>exp</mi><mfenced><mrow><mo>-</mo><mfenced><mrow><mfrac><msup><mfenced><mrow><mi>x</mi><mo>-</mo><msub><mi>x</mi><mi>o</mi></msub></mrow></mfenced><mn>2</mn></msup><mrow><mn>2</mn><msubsup><mi>&#x3C3;</mi><mi>x</mi><mn>2</mn></msubsup></mrow></mfrac><mo>+</mo><mfrac><msup><mfenced><mrow><mi>y</mi><mo>-</mo><msub><mi>y</mi><mi>o</mi></msub></mrow></mfenced><mn>2</mn></msup><mrow><mn>2</mn><msubsup><mi>&#x3C3;</mi><mi>y</mi><mn>2</mn></msubsup></mrow></mfrac></mrow></mfenced></mrow></mfenced></math><br>A是幅值，x。y。是中心点坐标，σx σy是方差，图示如下，A = 1, xo = 0, yo = 0, σx = σy = 1<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021530/1622340032183.png" alt="二维高斯函数"></p></blockquote><p>设 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>&#x2026;</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></math> 是随机变量 $f$ 的独立同分布样本，那么概率密度函数的核函数近似为：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mover><mi>f</mi><mo>^</mo></mover><mi>h</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><mi>h</mi></mrow></mfrac><munderover><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>K</mi><mfenced><mfrac><mrow><mi>x</mi><mo>-</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mi>h</mi></mfrac></mfenced></math>,K()是核函数，h是带宽,充当光滑参数</p><p>对于对象 $o$ ， <math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi>f</mi><mo>^</mo></mover><mo>(</mo><mi>o</mi><mo>)</mo></math> 给出该对象被随机过程中产生的估计概率。如果 <math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi>f</mi><mo>^</mo></mover><mo>(</mo><mi>o</mi><mo>)</mo></math> 过小，$o$  可能是离群点。</p><h3 id="基于邻近性的方法"><a href="#基于邻近性的方法" class="headerlink" title="基于邻近性的方法"></a>基于邻近性的方法</h3><p>假定一个对象是离群点，如果在特征空间中的最近邻也远离它，即该对象与它的最近邻之间的邻近性显著地偏离数据集中其他对象与它们的近邻之间的邻近性。</p><p>基于邻近性的方法的有效性高度依赖与所使用的邻近性度量，主要有<strong>基于距离</strong>和<strong>基于密度</strong>的离群点检测方法。</p><p><font color="#7FFF00">通俗理解，</font>离群点与近邻点的近邻距离明显大于其它对象与其的近邻的距离。即离群点周边环境明显和其它对象不一样。</p><h4 id="基于距离的离群点检测"><a href="#基于距离的离群点检测" class="headerlink" title="基于距离的离群点检测"></a>基于距离的离群点检测</h4><p>对象给定半径的邻域，如果它的邻域内没有足够多的其他点，则该点被认为是离群点。</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mfenced close="||" open="||"><mfenced close="}" open="{"><mrow><msup><mi>o</mi><mo>‘</mo></msup><mo>&#x2223;</mo><mo>dist</mo><mfenced><mrow><mi>o</mi><mo>,</mo><msup><mi>o</mi><mo>‘</mo></msup></mrow></mfenced><mo>&#x2264;</mo><mi>r</mi></mrow></mfenced></mfenced></mrow><mrow><mo>&#x2016;</mo><mi>D</mi><mo>&#x2016;</mo></mrow></mfrac><mo>&#x2264;</mo><mi>&#x3C0;</mi></math></p><p><font color="#228B22">r是距离阈值</font>，<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3C0;</mi></math>  是分数阈值，对象 $o$ 如果满足上面的式子则是一个  离群点。</p><h4 id="基于密度的离群点检测"><a href="#基于密度的离群点检测" class="headerlink" title="基于密度的离群点检测"></a>基于密度的离群点检测</h4><p>基于距离的检测方法从全局考虑数据集，所找到的离群点都是<strong>全局离群点</strong>，但实际上数据结构更复杂，对象<font color="#8FBC8F">可能</font>关于其局部邻域，而<font color="#8FBC8F">不是</font>整个数据分布而视为离群点。</p><p>基于密度的离群点检测方法基本假定为：<strong>非离群点对象周围的密度与其邻域周围的密度类似，而离群点对象周围的密度显著不同于其邻域周围的密度。</strong></p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo><mo>=</mo><mfenced close="}" open="{"><mrow><msup><mi>o</mi><mo>‘</mo></msup><mo>&#x2223;</mo><msup><mi>o</mi><mo>‘</mo></msup><mo>&#x2208;</mo><mi>D</mi><mo>,</mo><mo>dist</mo><mfenced><mrow><mi>o</mi><mo>,</mo><msup><mi>o</mi><mo>‘</mo></msup></mrow></mfenced><mo>&#x2264;</mo><msub><mo>dist</mo><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo></mrow></mfenced></math></p><p>D为数据集，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mo>dist</mo><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo></math>  是对象o第k个近邻的对象之间的距离，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo></math>  是所有在<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mo>dist</mo><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo></math> 之内的对象集。可以使用 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo></math> 中的对象到o的平均距离作为局部密度的度量，但是为了避免比如有非常近的近邻使得距离度量统计产生波动，需要加上光滑效果：<br>$reachdist$<math xmlns="http://www.w3.org/1998/Math/MathML"><mmultiscripts><mfenced><mrow><mi>o</mi><mo>&#x2190;</mo><msup><mi>o</mi><mo>‘</mo></msup></mrow></mfenced><mprescripts/><mi>k</mi><none/></mmultiscripts><mo>=</mo><mi>max</mi><mfenced close="}" open="{"><mrow><msub><mo>dist</mo><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo><mo>,</mo><mo>dist</mo><mfenced><mrow><mi>o</mi><mo>,</mo><msup><mi>o</mi><mo>‘</mo></msup></mrow></mfenced></mrow></mfenced></math><br>k是用户指定参数，控制光滑效果。对象o的局部密度为：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>l</mi><mi>r</mi><msub><mi>d</mi><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo><mo>=</mo><mfrac><mfenced close="||" open="||"><mrow><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo></mrow></mfenced><mrow><msub><mo>&#x2211;</mo><mrow><msup><mi>o</mi><mo>‘</mo></msup><mo>&#x2208;</mo><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo></mrow></msub><msub><mtext>&#xA0;reachdist&#xA0;</mtext><mi>k</mi></msub><mfenced><mrow><msup><mi>o</mi><mo>‘</mo></msup><mo>&#x2190;</mo><mi>o</mi></mrow></mfenced></mrow></mfrac></math><br>o的局部离群点因子为：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi><mi>O</mi><msub><mi>F</mi><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msub><mo>&#x2211;</mo><mrow><msup><mi>o</mi><mo>‘</mo></msup><mo>&#x2208;</mo><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo></mrow></msub><mfrac><mrow><msub><mo>lrd</mo><mi>k</mi></msub><mfenced><msup><mi>o</mi><mo>‘</mo></msup></mfenced></mrow><mrow><msub><mo>lrd</mo><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo></mrow></mfrac></mrow><mfenced close="||" open="||"><mrow><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>o</mi><mo>)</mo></mrow></mfenced></mfrac></math><br>局部离群点因子是o的可达密度与o的k-最近邻可达密度之比的平均值。对象o的局部可达密度越低，并且o的k-最近邻局部可达密度越高，LOF值越高。</p><p><font color="#B8860B">LOF 的思想：</font><br>通过比较每个点 p 和其邻域点的密度来判断该点是否为异常点，如果点 p 的密度越低，越可能被认定是异常点。至于这个密度，是通过点之间的距离来计算的，点之间距离越远，密度越低，距离越近，密度越高，而且这里的密度不是基于全局数据，而是基于局部数据。</p><h3 id="基于聚类的方法"><a href="#基于聚类的方法" class="headerlink" title="基于聚类的方法"></a>基于聚类的方法</h3><p>假定<font color="#556B2F">正常数据</font>对象属于大的、稠密的簇、而<font color="#556B2F">离群点</font>属于小或稀疏的簇，或者不属于任何簇。直截了当的采用聚类方法用于离群点检测开销会很大，不能很好地扩展到大数据集上。</p><ol><li>将离群点检测为不属于任何簇的对象</li><li>最近簇距离的离群点检测</li></ol><blockquote><p>假设o到最近的簇中心为 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>c</mi><mi>o</mi></msub></math> ,则o与 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>c</mi><mi>o</mi></msub></math> 之间的距离为 <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>dist</mo><mfenced><mrow><mi>o</mi><mo>,</mo><msub><mi>c</mi><mi>o</mi></msub></mrow></mfenced></math> ， <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>c</mi><mi>o</mi></msub></math> 与指派到 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>c</mi><mi>o</mi></msub></math> 这个簇中的对象之间的平均距离为 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>l</mi><msub><mi>c</mi><mi>o</mi></msub></msub></math> ，比率 <math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mrow><mo>dist</mo><mfenced><mrow><mi>o</mi><mo>,</mo><msub><mi>c</mi><mi>o</mi></msub></mrow></mfenced></mrow><msub><mi>l</mi><msub><mi>c</mi><mi>o</mi></msub></msub></mfrac></math> 度量 <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>dist</mo><mfenced><mrow><mi>o</mi><mo>,</mo><msub><mi>c</mi><mi>o</mi></msub></mrow></mfenced></math> 与平均值的差异程度。</p></blockquote><ol start="3"><li>识别小簇或稀疏簇</li></ol><blockquote><p>先是找出数据集中的簇，并把它们按照大小降序排列，假定大部分数据点都不是离群点。它使用一个参数<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3B1;</mi><mo>(</mo><mn>0</mn><mo>&#x2264;</mo><mi>&#x3B1;</mi><mo>&#x2264;</mo><mn>1</mn><mo>)</mo></math>  区别大小簇。任何至少包含数据集中 <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x3B1;</mi><mo>%</mo></math> 数据点的簇都被视为大簇，其余为小簇。然后对每个数据点赋予基于簇的局部离群点因子(CBLOF)。对于属于大簇的点，它的CBLOF是簇的大小与该点与簇的相似性的乘积。对于小簇的点，其CBLOF用小簇的大小和该点与最近的大簇的相似性乘积计算。<br>CBLOF代表点属于簇的概率，值越大，点与簇越相似。远离任何大簇的小簇被看作离群点组成，并且具有最低CBLOF值的点怀疑为离群点。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>LOF类的算法适用于局部区域空间问题，对于完整区域空间，KNN和Iforest更好。</li><li>KNN每次运行需要遍历所有数据，所以效率比较低，如果效率要求比较高，用聚类方法更好。</li><li>传统机器学习算法中Iforest、KNN和OCSVM表现较好，基于深度学习的算法准确率在论文中更好！</li><li>对于不同种类的数据，没有哪一种算法是最好的，HBOS算法在某些数据集上的表现非常好，且运算速度很快。</li><li>当数据特征数很多时，如400个特征，只有KNN表现还不错，Iforest表现也不好，因为特征选取的随机性，可能无法覆盖足够多的特征（不绝对）。</li><li>ABOD综合效果最差，尽量不要用。</li></ol>]]></content>
    
    
    <summary type="html">数据挖掘--离群点检测算法的学习</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="data mining" scheme="https://merlynr.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>2021-05-15【Week】</title>
    <link href="https://merlynr.github.io/2021/05/19/2021-05-15%E3%80%90Week%E3%80%91/"/>
    <id>https://merlynr.github.io/2021/05/19/2021-05-15%E3%80%90Week%E3%80%91/</id>
    <published>2021-05-18T16:00:00.000Z</published>
    <updated>2021-05-24T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="本周安排"><a href="#本周安排" class="headerlink" title="本周安排"></a>本周安排</h3><ul><li><input disabled="" type="checkbox"> 源码分析</li><li><input disabled="" type="checkbox"> 预开题准备</li><li><input disabled="" type="checkbox"> 重新整理并寻找合适的研究点</li></ul><h3 id="任务完成情况"><a href="#任务完成情况" class="headerlink" title="任务完成情况"></a>任务完成情况</h3><ol><li>源码分析已经完成，tagui的源码的难点主要是语言多，但是其中逻辑不是很难</li><li>这次预开题，我是比较认真准备的，当时讲的时候人比较多，可能比较着急哇，我主要为了解决RPA中可并发执行任务的功能，这块的难点就是资源分配的问题，所以涉及了大量资源分配的研究，目前国内外RPA这方面都比较淡化，没有很好的解决方式，所以我感觉还是可以的：）</li><li>现在在看老师给发的文档，大部分我也看过了，里面的点太笼统，面太大，当时和老师交流后，重新理解了一下，是研究机器学习在业务流程中处理文档，表格，但是目前就RPA里面来说，自动获取文件中信息处理的比较好，各个公司都没有在这块上投入更多资源</li></ol><h3 id="汇总文件补充"><a href="#汇总文件补充" class="headerlink" title="汇总文件补充"></a>汇总文件补充</h3><blockquote><p>平时学习时，总结都是单独的，所以这块就引用上周写的汇总了</p></blockquote><ol><li>tagui源码分析</li></ol><p><img src="./attachments/TagUI%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E5%8F%8A%E5%88%86%E6%9E%90.pdf" alt="TagUI源码阅读及分析"></p><ol start="2"><li>预开题PPT</li></ol><p><img src="./attachments/RPA%E5%8A%A8%E6%80%81%E6%84%9F%E7%9F%A5%E5%88%86%E9%85%8D%E4%B8%9A%E5%8A%A1%E8%B5%84%E6%BA%90%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%BA%94%E7%94%A8.pptx" alt="RPA动态感知分配业务资源的研究与应用"></p><ol start="3"><li>重新整理—未完成</li></ol><p><img src="./attachments/RPA+AI.pdf" alt="RPA+AI"></p>]]></content>
    
    
    <summary type="html">每周安排</summary>
    
    
    
    <category term="weekly report" scheme="https://merlynr.github.io/categories/weekly-report/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="report" scheme="https://merlynr.github.io/tags/report/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="weekly" scheme="https://merlynr.github.io/tags/weekly/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>RPA+AI</title>
    <link href="https://merlynr.github.io/2021/05/17/RPA+AI/"/>
    <id>https://merlynr.github.io/2021/05/17/RPA+AI/</id>
    <published>2021-05-16T16:00:00.000Z</published>
    <updated>2021-05-24T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h2><p><a href="https://developer.aliyun.com/group/rpa?spm=a2c6h.12873639.0.0.65b05d65mS6OlC#/?_k=agnoe0">阿里云RPA社区</a></p><p><a href="https://zhuanlan.zhihu.com/p/59034887?utm_oi=786717341600858112">阿里云RPA（机器人流程自动化）系列</a></p><p><a href="https://www.yuque.com/aliyun_rpa">阿里云RPA文档</a></p><p><a href="https://github.com/rpabotsworld/awesome-rpa">资源</a></p><h2 id="阅读记录"><a href="#阅读记录" class="headerlink" title="阅读记录"></a>阅读记录</h2><h3 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h3><h4 id="阿里云RPA系列"><a href="#阿里云RPA系列" class="headerlink" title="阿里云RPA系列"></a>阿里云RPA系列</h4><ol><li>文件信息的处理–提取和处理结构和半结构化数据</li><li>异常处理–宕机、流程回滚、中断后的流程接续等问题</li><li>业务流程从明确化变为高适配的—智能处理【制定与运行过程中】</li><li>各类场景–大量重复【基本配置通用性高】、可贴合各类场景【规则灵活，外附组件可灵活配置】</li></ol><h4 id="S公司智能财务机器人共享中心建设与实践"><a href="#S公司智能财务机器人共享中心建设与实践" class="headerlink" title="S公司智能财务机器人共享中心建设与实践"></a>S公司智能财务机器人共享中心建设与实践</h4><p><a href="https://m.hanspub.org/journal/paper/34237">URL</a></p><ol><li>分布式部署时，对于资源无法实时有效判断其是否有效可用</li><li>将robot集中到资源池中，供全公司使用，打破单元机器人的壁垒—无法有效的共享数据，但是安全受到了极大威胁</li></ol><h4 id="一种基于RPA机器人共享中心的自动审批的方法【专利】"><a href="#一种基于RPA机器人共享中心的自动审批的方法【专利】" class="headerlink" title="一种基于RPA机器人共享中心的自动审批的方法【专利】"></a>一种基于RPA机器人共享中心的自动审批的方法【专利】</h4><p><a href="http://www10.drugfuture.com/pdfview/generic/web/viewer.html?file=/cnpat/package/%E5%8F%91%E6%98%8E%E4%B8%93%E5%88%A9%E7%94%B3%E8%AF%B7%E8%AF%B4%E6%98%8E%E4%B9%A6CN201911335237.4.pdf">PDF</a></p><ol><li>没有实际创新点，就是为用户提供了访问权限，根据用户自己提交的内容，进行过滤分类，然后由robot进行访问对应的资源进行处理，专利只是讲了研究内容，具体算法和实践没有提到，所以它所涉及到的对机器人共享中心进行分级调度没有表现出来。</li></ol><h3 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h3><blockquote><p>阿里云版本迭代</p></blockquote><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021517/1621238642819.png" alt="阿里云版本迭代方案"></p><h3 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h3><ol><li>流程处理未知问题的智能化</li><li>中间通信安全</li><li>重新定义RPA，目前RPA，只是一个外接的控制工具，而不能替代人工</li><li>==* #F44336==挖掘业务流程，结合机器学习进行业务预测</li><li></li></ol><h2 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h2><ol><li>无数据比对，无证明方式，用数据证明有效改进</li></ol><p><a href="https://wap.cnki.net/touch/web/Dissertation/Article/10013-1019047248.nh.html">https://wap.cnki.net/touch/web/Dissertation/Article/10013-1019047248.nh.html</a></p><p><a href="https://www.touqikan.com/jsjj/660421.html">https://www.touqikan.com/jsjj/660421.html</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;相关文章&quot;&gt;&lt;a href=&quot;#相关文章&quot; class=&quot;headerlink&quot; title=&quot;相关文章&quot;&gt;&lt;/a&gt;相关文章&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://developer.aliyun.com/group/rpa?spm=a2c6h.1287</summary>
      
    
    
    
    <category term="RPA" scheme="https://merlynr.github.io/categories/RPA/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="RPA" scheme="https://merlynr.github.io/tags/RPA/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习--吴恩达</title>
    <link href="https://merlynr.github.io/2021/05/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0--%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    <id>https://merlynr.github.io/2021/05/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0--%E5%90%B4%E6%81%A9%E8%BE%BE/</id>
    <published>2021-05-16T16:00:00.000Z</published>
    <updated>2021-05-24T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>监督学习—因子和结果都给出，让机器学习判断<br>无监督学习–无规则学习 【聚类属于无监督】</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;监督学习—因子和结果都给出，让机器学习判断&lt;br&gt;无监督学习–无规则学习 【聚类属于无监督】&lt;/p&gt;
</summary>
      
    
    
    
    <category term="machine learning" scheme="https://merlynr.github.io/categories/machine-learning/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
  </entry>
  
</feed>
